{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"(Baseline) Average_Fusion.ipynb","provenance":[{"file_id":"1GEPelG8M4pwx_llUYLlmUO6jfjnUgMPl","timestamp":1618388090083},{"file_id":"1Px7sog6jBh8jhBFgrHAKeqt1maPE48jM","timestamp":1614749164976}],"collapsed_sections":[],"machine_shape":"hm"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU","widgets":{"application/vnd.jupyter.widget-state+json":{"0cae70d03c2a4d68917f5bbff7aed01b":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_e0b33e256d6044449e5230008aa73fd0","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_fff5b70cf3794c60913bdc2c5210cde2","IPY_MODEL_cc69bb4467de41a5af8a03ed8530259b"]}},"e0b33e256d6044449e5230008aa73fd0":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"fff5b70cf3794c60913bdc2c5210cde2":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_49f3590449e84752a830da2ff9a6acf1","_dom_classes":[],"description":"100%","_model_name":"FloatProgressModel","bar_style":"success","max":84916,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":84916,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_d994870160ce42d7ba0f4e5515bc59dd"}},"cc69bb4467de41a5af8a03ed8530259b":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_0ccadde7db684151af6b96d51a1464bb","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 84916/84916 [00:04&lt;00:00, 19469.43it/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_445d7365f5ec4934aeeb46733313fd2b"}},"49f3590449e84752a830da2ff9a6acf1":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"d994870160ce42d7ba0f4e5515bc59dd":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"0ccadde7db684151af6b96d51a1464bb":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"445d7365f5ec4934aeeb46733313fd2b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"4a79e8ddb1074514891f123bb78a2521":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_c7e68ca648fe4427a07ac62a3af0e4c5","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_0fcde4256baf4ab5be8b299cf456412d","IPY_MODEL_a29be7fd44ba4d4f865a8563183420da"]}},"c7e68ca648fe4427a07ac62a3af0e4c5":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"0fcde4256baf4ab5be8b299cf456412d":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_81c4ebd4de154760ab50400ef22d12e7","_dom_classes":[],"description":"100%","_model_name":"FloatProgressModel","bar_style":"success","max":84916,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":84916,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_d3a79a2a20224ce589968606e09e77c9"}},"a29be7fd44ba4d4f865a8563183420da":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_45115e8d41cd4840aadecfe1a476a83f","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 84916/84916 [00:01&lt;00:00, 67516.56it/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_d928ccd6a8ad40288577da347a6eb046"}},"81c4ebd4de154760ab50400ef22d12e7":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"d3a79a2a20224ce589968606e09e77c9":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"45115e8d41cd4840aadecfe1a476a83f":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"d928ccd6a8ad40288577da347a6eb046":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"aa0be85fdc614d018a9d1ba64b8e6254":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_f25b6155abd34796ab1a3325de8262cd","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_c2713fb931b848fe9a4e93dbe6567a50","IPY_MODEL_b9441a2fed2d46599671c30553d2427b"]}},"f25b6155abd34796ab1a3325de8262cd":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"c2713fb931b848fe9a4e93dbe6567a50":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_ad67e57cfd9546a3beb5a4d7e72a9034","_dom_classes":[],"description":"100%","_model_name":"FloatProgressModel","bar_style":"success","max":55116,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":55116,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_b3049adc2a2744c6b8be1a7098e0fc70"}},"b9441a2fed2d46599671c30553d2427b":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_46c56615834b445e8d62d834e43af82a","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 55116/55116 [00:01&lt;00:00, 34760.06it/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_207127a4c2e347ff8798b8f5c83100b6"}},"ad67e57cfd9546a3beb5a4d7e72a9034":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"b3049adc2a2744c6b8be1a7098e0fc70":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"46c56615834b445e8d62d834e43af82a":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"207127a4c2e347ff8798b8f5c83100b6":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}}}}},"cells":[{"cell_type":"markdown","metadata":{"id":"1fCPKaFCgw23"},"source":["# Multimodal Calssification on Rakuten France Dataset\n","# Multi Modal Addition Fusion"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"feRlWWySuDCt","executionInfo":{"status":"ok","timestamp":1620397625402,"user_tz":-120,"elapsed":1155,"user":{"displayName":"Sara Fattouh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjGrdvZo2x_8K8abskRubIgMK3SdtS4cG-aQE01nA=s64","userId":"09360594903764573322"}},"outputId":"5f2b1526-7f59-411b-ded3-7585d29d33d5"},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":88,"outputs":[{"output_type":"stream","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"BZouy12lm_Xv","executionInfo":{"status":"ok","timestamp":1620397626672,"user_tz":-120,"elapsed":2409,"user":{"displayName":"Sara Fattouh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjGrdvZo2x_8K8abskRubIgMK3SdtS4cG-aQE01nA=s64","userId":"09360594903764573322"}},"outputId":"3275e79b-c8f1-4db6-cdef-c5e7218fb043"},"source":["!ls"],"execution_count":89,"outputs":[{"output_type":"stream","text":["data\t   test_inputs_cam.pt\ttr_inputs_cam.pt   val_inputs_cam.pt\n","image\t   test_inputs_flau.pt\ttr_inputs_flau.pt  val_inputs_flau.pt\n","image.zip  test_masks_cam.pt\ttr_masks_cam.pt    val_masks_cam.pt\n","models\t   test_masks_flau.pt\ttr_masks_flau.pt   val_masks_flau.pt\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"uS6m5N5TKv_x","executionInfo":{"status":"ok","timestamp":1620397626675,"user_tz":-120,"elapsed":2401,"user":{"displayName":"Sara Fattouh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjGrdvZo2x_8K8abskRubIgMK3SdtS4cG-aQE01nA=s64","userId":"09360594903764573322"}}},"source":["# mv './image_1/image' '../image'"],"execution_count":90,"outputs":[]},{"cell_type":"code","metadata":{"id":"JixDkVh-LCd9","executionInfo":{"status":"ok","timestamp":1620397626676,"user_tz":-120,"elapsed":2393,"user":{"displayName":"Sara Fattouh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjGrdvZo2x_8K8abskRubIgMK3SdtS4cG-aQE01nA=s64","userId":"09360594903764573322"}}},"source":["# mv image Rakuten"],"execution_count":91,"outputs":[]},{"cell_type":"code","metadata":{"id":"BhBoZ1r7tBSd","executionInfo":{"status":"ok","timestamp":1620397626676,"user_tz":-120,"elapsed":2387,"user":{"displayName":"Sara Fattouh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjGrdvZo2x_8K8abskRubIgMK3SdtS4cG-aQE01nA=s64","userId":"09360594903764573322"}}},"source":["mkdir Rakuten"],"execution_count":92,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"rtpdRXBytE0X","executionInfo":{"status":"ok","timestamp":1620397626677,"user_tz":-120,"elapsed":2377,"user":{"displayName":"Sara Fattouh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjGrdvZo2x_8K8abskRubIgMK3SdtS4cG-aQE01nA=s64","userId":"09360594903764573322"}},"outputId":"5ea7b5c9-5fbb-4deb-f1db-557353ce0995"},"source":["cd './Rakuten'"],"execution_count":93,"outputs":[{"output_type":"stream","text":["/content/Rakuten/Rakuten\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"Im900y65tWcT","executionInfo":{"status":"ok","timestamp":1620397626887,"user_tz":-120,"elapsed":2577,"user":{"displayName":"Sara Fattouh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjGrdvZo2x_8K8abskRubIgMK3SdtS4cG-aQE01nA=s64","userId":"09360594903764573322"}}},"source":["mkdir models data "],"execution_count":94,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"id":"zqk1CENOwDRU","executionInfo":{"status":"ok","timestamp":1620397626894,"user_tz":-120,"elapsed":2571,"user":{"displayName":"Sara Fattouh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjGrdvZo2x_8K8abskRubIgMK3SdtS4cG-aQE01nA=s64","userId":"09360594903764573322"}},"outputId":"759411ad-e64d-44d0-a52c-4a3baaa3a246"},"source":["pwd"],"execution_count":95,"outputs":[{"output_type":"execute_result","data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'/content/Rakuten/Rakuten'"]},"metadata":{"tags":[]},"execution_count":95}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"sRtwmVtke5nf","executionInfo":{"status":"ok","timestamp":1620397627714,"user_tz":-120,"elapsed":3373,"user":{"displayName":"Sara Fattouh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjGrdvZo2x_8K8abskRubIgMK3SdtS4cG-aQE01nA=s64","userId":"09360594903764573322"}},"outputId":"4a3a8c2a-4cf7-41e5-ed18-fb5c183de7d9"},"source":["ls -la"],"execution_count":96,"outputs":[{"output_type":"stream","text":["total 16\n","drwxr-xr-x 4 root root 4096 May  7 14:27 \u001b[0m\u001b[01;34m.\u001b[0m/\n","drwxr-xr-x 6 root root 4096 May  7 14:27 \u001b[01;34m..\u001b[0m/\n","drwxr-xr-x 2 root root 4096 May  7 14:27 \u001b[01;34mdata\u001b[0m/\n","drwxr-xr-x 2 root root 4096 May  7 14:27 \u001b[01;34mmodels\u001b[0m/\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"rXbFKdl0wGVS","executionInfo":{"status":"ok","timestamp":1620397627715,"user_tz":-120,"elapsed":3363,"user":{"displayName":"Sara Fattouh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjGrdvZo2x_8K8abskRubIgMK3SdtS4cG-aQE01nA=s64","userId":"09360594903764573322"}}},"source":["# cp '/content/drive/My Drive/Rakuten/models/CamemBERT_best_model_baseline.pt' models"],"execution_count":97,"outputs":[]},{"cell_type":"code","metadata":{"id":"IwsYn7UatdVV","executionInfo":{"status":"ok","timestamp":1620397627715,"user_tz":-120,"elapsed":3356,"user":{"displayName":"Sara Fattouh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjGrdvZo2x_8K8abskRubIgMK3SdtS4cG-aQE01nA=s64","userId":"09360594903764573322"}}},"source":["# cp '/content/drive/My Drive/Rakuten/models/FlauBERT_best_model_baseline.pt' models\n"],"execution_count":98,"outputs":[]},{"cell_type":"code","metadata":{"id":"OrrpaqRTui48","executionInfo":{"status":"ok","timestamp":1620397627716,"user_tz":-120,"elapsed":3350,"user":{"displayName":"Sara Fattouh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjGrdvZo2x_8K8abskRubIgMK3SdtS4cG-aQE01nA=s64","userId":"09360594903764573322"}}},"source":["# cp '/content/drive/My Drive/Rakuten/models/RESNET_baseline_model.pt' models\n"],"execution_count":99,"outputs":[]},{"cell_type":"code","metadata":{"id":"4kTwB_kSusK2","executionInfo":{"status":"ok","timestamp":1620397627716,"user_tz":-120,"elapsed":3344,"user":{"displayName":"Sara Fattouh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjGrdvZo2x_8K8abskRubIgMK3SdtS4cG-aQE01nA=s64","userId":"09360594903764573322"}}},"source":["# !cp  '/content/drive/My Drive/Rakuten/image.zip' './'"],"execution_count":100,"outputs":[]},{"cell_type":"code","metadata":{"id":"JhdiQHVFvSj3","executionInfo":{"status":"ok","timestamp":1620397627717,"user_tz":-120,"elapsed":3330,"user":{"displayName":"Sara Fattouh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjGrdvZo2x_8K8abskRubIgMK3SdtS4cG-aQE01nA=s64","userId":"09360594903764573322"}}},"source":["# !unzip  ./image.zip "],"execution_count":101,"outputs":[]},{"cell_type":"code","metadata":{"id":"hgr5ai-NfUU5","executionInfo":{"status":"ok","timestamp":1620397627717,"user_tz":-120,"elapsed":3319,"user":{"displayName":"Sara Fattouh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjGrdvZo2x_8K8abskRubIgMK3SdtS4cG-aQE01nA=s64","userId":"09360594903764573322"}}},"source":["# pwd"],"execution_count":102,"outputs":[]},{"cell_type":"code","metadata":{"id":"v1kbpdFQwrir","executionInfo":{"status":"ok","timestamp":1620397631203,"user_tz":-120,"elapsed":6796,"user":{"displayName":"Sara Fattouh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjGrdvZo2x_8K8abskRubIgMK3SdtS4cG-aQE01nA=s64","userId":"09360594903764573322"}}},"source":[" !cp '/content/drive/My Drive/Rakuten/data/NewTest.csv' data \n"," !cp '/content/drive/My Drive/Rakuten/data/NewTraining.csv' data \n"," !cp '/content/drive/My Drive/Rakuten/data/catalog_english_taxonomy.tsv' data \n"," !cp '/content/drive/My Drive/Rakuten/data/Y_train.tsv' data \n"," !cp '/content/drive/My Drive/Rakuten/data/X_train.tsv' data"],"execution_count":103,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"LXpLoRd1h5xu"},"source":["# 1. Setup"]},{"cell_type":"markdown","metadata":{"id":"FIGO5jehh8-2"},"source":["# 1.1 Using Colab GPU for Training\n","\n","Since we’ll be training a large neural network it’s best to take advantage of the free GPUs and TPUs that Google offers (in this case we’ll attach a GPU), otherwise training will take a very long time.\n","\n","A GPU can be added by going to the menu and selecting:\n","\n","Edit 🡒 Notebook Settings 🡒 Hardware accelerator 🡒 (GPU)"]},{"cell_type":"code","metadata":{"id":"-hgdtmoPf2al","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1620397631204,"user_tz":-120,"elapsed":6790,"user":{"displayName":"Sara Fattouh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjGrdvZo2x_8K8abskRubIgMK3SdtS4cG-aQE01nA=s64","userId":"09360594903764573322"}},"outputId":"4710c8ff-8de6-44a3-b219-130148ffa50b"},"source":["import os, time, datetime\n","import numpy as np\n","import pandas as pd\n","from tqdm.notebook import tqdm\n","import random\n","import logging\n","tqdm.pandas()\n","import seaborn as sns\n","from sklearn.model_selection import train_test_split\n","from google.colab import drive\n","\n","#NN Packages\n","import torch\n","import torch.nn as nn\n","from torch.utils.data import TensorDataset, random_split,DataLoader, RandomSampler, SequentialSampler\n","\n","logger = logging.getLogger(__name__)"],"execution_count":104,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/tqdm/std.py:658: FutureWarning: The Panel class is removed from pandas. Accessing it from the top-level namespace will also be removed in the next version\n","  from pandas import Panel\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7-DN_gCoh_pG","executionInfo":{"status":"ok","timestamp":1620397631205,"user_tz":-120,"elapsed":6781,"user":{"displayName":"Sara Fattouh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjGrdvZo2x_8K8abskRubIgMK3SdtS4cG-aQE01nA=s64","userId":"09360594903764573322"}},"outputId":"569157ef-4cc5-408d-c2f4-0b4c1e701411"},"source":["if torch.cuda.is_available():    \n","\n","    # Tell PyTorch to use the GPU.    \n","    device = torch.device(\"cuda\")\n","\n","    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n","\n","    print('We will use the GPU:', torch.cuda.get_device_name(0))\n","\n","# If not...\n","else:\n","    print('No GPU available, using the CPU instead.')\n","    device = torch.device(\"cpu\")"],"execution_count":105,"outputs":[{"output_type":"stream","text":["There are 1 GPU(s) available.\n","We will use the GPU: Tesla P100-PCIE-16GB\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"8dnUWVTZM8Wc","executionInfo":{"status":"ok","timestamp":1620397631642,"user_tz":-120,"elapsed":7208,"user":{"displayName":"Sara Fattouh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjGrdvZo2x_8K8abskRubIgMK3SdtS4cG-aQE01nA=s64","userId":"09360594903764573322"}},"outputId":"a9ddd8d8-5adb-4dcd-a7ae-bc40d33ce047"},"source":["gpu_info = !nvidia-smi\n","gpu_info = '\\n'.join(gpu_info)\n","if gpu_info.find('failed') >= 0:\n","  print('Select the Runtime > \"Change runtime type\" menu to enable a GPU accelerator, ')\n","  print('and then re-execute this cell.')\n","else:\n","  print(gpu_info)"],"execution_count":106,"outputs":[{"output_type":"stream","text":["Fri May  7 14:27:09 2021       \n","+-----------------------------------------------------------------------------+\n","| NVIDIA-SMI 465.19.01    Driver Version: 460.32.03    CUDA Version: 11.2     |\n","|-------------------------------+----------------------+----------------------+\n","| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n","| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n","|                               |                      |               MIG M. |\n","|===============================+======================+======================|\n","|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n","| N/A   46C    P0    34W / 250W |   7839MiB / 16280MiB |      0%      Default |\n","|                               |                      |                  N/A |\n","+-------------------------------+----------------------+----------------------+\n","                                                                               \n","+-----------------------------------------------------------------------------+\n","| Processes:                                                                  |\n","|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n","|        ID   ID                                                   Usage      |\n","|=============================================================================|\n","+-----------------------------------------------------------------------------+\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"tibnb-j2mVPo"},"source":["# 1.2. Installing the Hugging Face Library - Image Pretrained Models\n","\n","Install the transformers package from **Hugging Face** which will give us a pytorch interface for working with BERT. This library contains interfaces for other pretrained language models.\n","\n","We’ve selected the pytorch interface because it strikes a nice balance between the high-level APIs (which are easy to use but don’t provide insight into how things work) and tensorflow code (which contains lots of details but often sidetracks us into lessons about tensorflow, when the purpose here is BERT!).\n","\n","At the moment, the Hugging Face library seems to be the most widely accepted and powerful pytorch interface for working with BERT. In addition to supporting a variety of different pre-trained transformer models, the library also includes pre-built modifications of these models suited to your specific task.\n","E.g \"BertForSequenceClassification\" that we will be using.\n","\n","The goal  of the **pretrainedmodels** is to:\n","\n","-  help to reproduce transfer learning setups\n","\n","-  access pretrained ConvNets with a unique interface/API inspired by torchvision."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"j_umLuXpmYwk","executionInfo":{"status":"ok","timestamp":1620397644414,"user_tz":-120,"elapsed":19969,"user":{"displayName":"Sara Fattouh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjGrdvZo2x_8K8abskRubIgMK3SdtS4cG-aQE01nA=s64","userId":"09360594903764573322"}},"outputId":"ea0ef9e5-c049-4020-a681-f85a3f6be3dc"},"source":["!pip install transformers\n","!pip install sentencepiece\n","!pip install pretrainedmodels"],"execution_count":107,"outputs":[{"output_type":"stream","text":["Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (4.5.1)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.41.1)\n","Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers) (0.0.45)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers) (20.9)\n","Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from transformers) (3.10.1)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.0.12)\n","Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n","Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.10.2)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.0.1)\n","Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n","Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n","Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers) (2.4.7)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.4.1)\n","Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.7.4.3)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2020.12.5)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n","Requirement already satisfied: sentencepiece in /usr/local/lib/python3.7/dist-packages (0.1.95)\n","Requirement already satisfied: pretrainedmodels in /usr/local/lib/python3.7/dist-packages (0.7.4)\n","Requirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (from pretrainedmodels) (1.8.1+cu101)\n","Requirement already satisfied: torchvision in /usr/local/lib/python3.7/dist-packages (from pretrainedmodels) (0.9.1+cu101)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from pretrainedmodels) (4.41.1)\n","Requirement already satisfied: munch in /usr/local/lib/python3.7/dist-packages (from pretrainedmodels) (2.5.0)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch->pretrainedmodels) (3.7.4.3)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torch->pretrainedmodels) (1.19.5)\n","Requirement already satisfied: pillow>=4.1.1 in /usr/local/lib/python3.7/dist-packages (from torchvision->pretrainedmodels) (7.1.2)\n","Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from munch->pretrainedmodels) (1.15.0)\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"LLbB2aTdigvz"},"source":["# 2. Dataset Loading and Preprocessing"]},{"cell_type":"markdown","metadata":{"id":"WDHFkRyDjxpP"},"source":["# 2.1 Dataset Loading"]},{"cell_type":"code","metadata":{"id":"wJ0jmI1xxQ7j","executionInfo":{"status":"ok","timestamp":1620397644418,"user_tz":-120,"elapsed":19960,"user":{"displayName":"Sara Fattouh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjGrdvZo2x_8K8abskRubIgMK3SdtS4cG-aQE01nA=s64","userId":"09360594903764573322"}}},"source":["text_data_path = '/content/Rakuten/data'\n","image_data_path = '/content/Rakuten/image' "],"execution_count":108,"outputs":[]},{"cell_type":"code","metadata":{"id":"9QzA9onxAnhv","executionInfo":{"status":"ok","timestamp":1620397644418,"user_tz":-120,"elapsed":19953,"user":{"displayName":"Sara Fattouh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjGrdvZo2x_8K8abskRubIgMK3SdtS4cG-aQE01nA=s64","userId":"09360594903764573322"}}},"source":["class SigirPreprocess():\n","\n","  def __init__(self, text_data_path):\n","    \n","        self.text_data_path = text_data_path\n","        self.train = None # Merged X_train and Y_train\n","        self.dict_code_to_id = {}\n","        self.dict_id_to_code = {}\n","        self.list_tags = {} #unique type code\n","        self.sentences = []\n","        self.labels = []\n","        self.text_col = None\n","        self.X_test = None\n","\n","  def prepare_data(self):\n","        \n","        # #loading the Merged, preprocessed text data and test data\n","        # train = pd.read_csv(self.text_data_path+\"/NewTraining.csv\")\n","        # # new_train =  train[train['Description'] != \" \"]\n","        # # new_train = new_train[new_train['Description'].notna()]\n","        # self.train = train\n","\n","        catalog_eng= pd.read_csv(text_data_path+\"/catalog_english_taxonomy.tsv\",sep=\"\\t\")\n","        X_train= pd.read_csv(text_data_path+\"/X_train.tsv\",sep=\"\\t\")\n","        Y_train= pd.read_csv(text_data_path+\"/Y_train.tsv\",sep=\"\\t\")\n","        \n","        self.list_tags = list(Y_train['Prdtypecode'].unique())\n","        for i,tag in enumerate(self.list_tags):\n","            self.dict_code_to_id[tag] = i \n","            self.dict_id_to_code[i]=tag\n","        print(self.dict_code_to_id)\n","            \n","        Y_train['labels']=Y_train['Prdtypecode'].map(self.dict_code_to_id)\n","        train=pd.merge(left=X_train,right=Y_train,\n","               how='left',left_on=['Integer_id','Image_id','Product_id'],\n","               right_on=['Integer_id','Image_id','Product_id'])\n","        prod_map=pd.Series(catalog_eng['Top level category'].values,\n","                           index=catalog_eng['Prdtypecode']).to_dict()\n","\n","        train['product'] = train['Prdtypecode'].map(prod_map)\n","        train['title_len']=train['Title'].progress_apply(lambda x : len(x.split()) if pd.notna(x) else 0)\n","        train['desc_len']=train['Description'].progress_apply(lambda x : len(x.split()) if pd.notna(x) else 0)\n","        train['title_desc_len']=train['title_len'] + train['desc_len']\n","        train.loc[train['Description'].isnull(), 'Description'] = \" \"\n","        train['title_desc'] = train['Title'] + \" \" + train['Description']\n","        \n","        self.train = train\n","\n","        \n","  def get_sentences(self, text_col, remove_null_rows=True):\n","\n","       #get values of a specific column\n","        self.text_col = text_col        \n","\n","        new_train = self.train.copy()  \n","        self.sentences = new_train[text_col].values\n","        self.labels = new_train['labels'].values\n","\n","\n","  def prepare_test(self, text_col):\n","    \n","        X_test = pd.read_csv(self.text_data_path + \"/NewTest.csv\")\n","        self.X_test = X_test\n","        X_test['title_desc'] = X_test['Title'] + \" \" + X_test['Description']\n","        self.test_sentences = X_test[text_col].values\n","        return self.test_sentences\n","        "],"execution_count":109,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"8triqwglkYZO"},"source":["# 2.2 Drop Records With No Description"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":186,"referenced_widgets":["0cae70d03c2a4d68917f5bbff7aed01b","e0b33e256d6044449e5230008aa73fd0","fff5b70cf3794c60913bdc2c5210cde2","cc69bb4467de41a5af8a03ed8530259b","49f3590449e84752a830da2ff9a6acf1","d994870160ce42d7ba0f4e5515bc59dd","0ccadde7db684151af6b96d51a1464bb","445d7365f5ec4934aeeb46733313fd2b","4a79e8ddb1074514891f123bb78a2521","c7e68ca648fe4427a07ac62a3af0e4c5","0fcde4256baf4ab5be8b299cf456412d","a29be7fd44ba4d4f865a8563183420da","81c4ebd4de154760ab50400ef22d12e7","d3a79a2a20224ce589968606e09e77c9","45115e8d41cd4840aadecfe1a476a83f","d928ccd6a8ad40288577da347a6eb046"]},"id":"Q_BYtGYwkSPk","executionInfo":{"status":"ok","timestamp":1620397646544,"user_tz":-120,"elapsed":22070,"user":{"displayName":"Sara Fattouh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjGrdvZo2x_8K8abskRubIgMK3SdtS4cG-aQE01nA=s64","userId":"09360594903764573322"}},"outputId":"ada62207-72f2-40b0-bd08-64089e534da0"},"source":["#Load train and test data (test for specific column)\n","\n","text_col = 'title_desc'\n","\n","max_len = 256\n","\n","num_classes = 27\n","\n","Preprocess = SigirPreprocess(text_data_path)\n","Preprocess.prepare_data()\n","train = Preprocess.train\n","print(\"Trian:  \", len(Preprocess.train))\n","\n","\n","Preprocess.get_sentences(text_col)\n","print(\"Labels: \", len(Preprocess.labels))\n","\n","# X_test = Preprocess.prepare_test(text_col)\n","# print(\"Test:   \", len(Preprocess.X_test))"],"execution_count":110,"outputs":[{"output_type":"stream","text":["{10: 0, 2280: 1, 50: 2, 1280: 3, 2705: 4, 2522: 5, 2582: 6, 1560: 7, 1281: 8, 1920: 9, 2403: 10, 1140: 11, 2583: 12, 1180: 13, 1300: 14, 2462: 15, 1160: 16, 2060: 17, 40: 18, 60: 19, 1320: 20, 1302: 21, 2220: 22, 2905: 23, 2585: 24, 1940: 25, 1301: 26}\n"],"name":"stdout"},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"0cae70d03c2a4d68917f5bbff7aed01b","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=0.0, max=84916.0), HTML(value='')))"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["\n"],"name":"stdout"},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"4a79e8ddb1074514891f123bb78a2521","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=0.0, max=84916.0), HTML(value='')))"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["\n","Trian:   84916\n","Labels:  84916\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"qguSL6pMHQve","executionInfo":{"status":"ok","timestamp":1620397646544,"user_tz":-120,"elapsed":22058,"user":{"displayName":"Sara Fattouh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjGrdvZo2x_8K8abskRubIgMK3SdtS4cG-aQE01nA=s64","userId":"09360594903764573322"}},"outputId":"29332a9e-5873-44de-c573-8ac3b3b204de"},"source":["print(Preprocess.train['Title'].isnull().sum())\n","\n","print(Preprocess.train['Description'].isnull().sum())\n","\n","print(Preprocess.train['Image_id'].isnull().sum())\n","\n","print(Preprocess.train['Product_id'].isnull().sum())\n","\n","print(Preprocess.train['Prdtypecode'].isnull().sum())\n","\n","print(Preprocess.train['labels'].isnull().sum())\n","\n","print(Preprocess.train['product'].isnull().sum()) #top level category\n","\n","print(Preprocess.train['title_desc'].isnull().sum())\n","\n","\n"],"execution_count":111,"outputs":[{"output_type":"stream","text":["0\n","0\n","0\n","0\n","0\n","0\n","0\n","0\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9gGj4NDbINdl","executionInfo":{"status":"ok","timestamp":1620397646943,"user_tz":-120,"elapsed":22443,"user":{"displayName":"Sara Fattouh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjGrdvZo2x_8K8abskRubIgMK3SdtS4cG-aQE01nA=s64","userId":"09360594903764573322"}},"outputId":"b59b3671-ad7f-43a6-fc8a-890b8116b154"},"source":["\n","Preprocess.train.isnull().values.any()"],"execution_count":112,"outputs":[{"output_type":"execute_result","data":{"text/plain":["False"]},"metadata":{"tags":[]},"execution_count":112}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"8797SZsJIVnP","executionInfo":{"status":"ok","timestamp":1620397646947,"user_tz":-120,"elapsed":22436,"user":{"displayName":"Sara Fattouh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjGrdvZo2x_8K8abskRubIgMK3SdtS4cG-aQE01nA=s64","userId":"09360594903764573322"}},"outputId":"6229841e-0b2d-45f0-a1e3-3ca71f911f2e"},"source":["Preprocess.train.isnull().sum().sum()"],"execution_count":113,"outputs":[{"output_type":"execute_result","data":{"text/plain":["0"]},"metadata":{"tags":[]},"execution_count":113}]},{"cell_type":"code","metadata":{"id":"SNdXR21N-nES","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1620397646948,"user_tz":-120,"elapsed":22427,"user":{"displayName":"Sara Fattouh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjGrdvZo2x_8K8abskRubIgMK3SdtS4cG-aQE01nA=s64","userId":"09360594903764573322"}},"outputId":"235a0122-f06f-4122-b145-4e20b3e7e4c8"},"source":["new_train =  Preprocess.train[Preprocess.train['Description'] != \" \"]\n","Preprocess.train = new_train\n","Preprocess.labels = new_train['labels'].values\n","\n","print(\"Numner of records without Null Description:  \",len(Preprocess.train))\n","print(\"Numner of corresponding Labels:  \",len(Preprocess.labels))"],"execution_count":114,"outputs":[{"output_type":"stream","text":["Numner of records without Null Description:   55116\n","Numner of corresponding Labels:   55116\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"jBCwmgodH5wL","executionInfo":{"status":"ok","timestamp":1620397646949,"user_tz":-120,"elapsed":22414,"user":{"displayName":"Sara Fattouh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjGrdvZo2x_8K8abskRubIgMK3SdtS4cG-aQE01nA=s64","userId":"09360594903764573322"}},"outputId":"b70ac046-aa19-4d28-9f31-f7e4b6d03eea"},"source":["Preprocess.train"],"execution_count":115,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Integer_id</th>\n","      <th>Title</th>\n","      <th>Description</th>\n","      <th>Image_id</th>\n","      <th>Product_id</th>\n","      <th>Prdtypecode</th>\n","      <th>labels</th>\n","      <th>product</th>\n","      <th>title_len</th>\n","      <th>desc_len</th>\n","      <th>title_desc_len</th>\n","      <th>title_desc</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>2</th>\n","      <td>2</td>\n","      <td>Grand Stylet Ergonomique Bleu Gamepad Nintendo...</td>\n","      <td>PILOT STYLE Touch Pen de marque Speedlink est ...</td>\n","      <td>938777978</td>\n","      <td>201115110</td>\n","      <td>50</td>\n","      <td>2</td>\n","      <td>Entertainment</td>\n","      <td>12</td>\n","      <td>109</td>\n","      <td>121</td>\n","      <td>Grand Stylet Ergonomique Bleu Gamepad Nintendo...</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>4</td>\n","      <td>La Guerre Des Tuques</td>\n","      <td>Luc a des id&amp;eacute;es de grandeur. Il veut or...</td>\n","      <td>1077757786</td>\n","      <td>278535884</td>\n","      <td>2705</td>\n","      <td>4</td>\n","      <td>Books</td>\n","      <td>4</td>\n","      <td>34</td>\n","      <td>38</td>\n","      <td>La Guerre Des Tuques Luc a des id&amp;eacute;es de...</td>\n","    </tr>\n","    <tr>\n","      <th>7</th>\n","      <td>7</td>\n","      <td>Conquérant Sept Cahier Couverture Polypro 240 ...</td>\n","      <td>CONQUERANT CLASSIQUE Cahier 240 x 320 mm seyès...</td>\n","      <td>999581347</td>\n","      <td>344240059</td>\n","      <td>2522</td>\n","      <td>5</td>\n","      <td>Books</td>\n","      <td>14</td>\n","      <td>18</td>\n","      <td>32</td>\n","      <td>Conquérant Sept Cahier Couverture Polypro 240 ...</td>\n","    </tr>\n","    <tr>\n","      <th>9</th>\n","      <td>9</td>\n","      <td>Tente Pliante V3s5-Pro Pvc Blanc - 3 X 4m50 - ...</td>\n","      <td>Tente pliante V3S5 Pro PVC 500 gr/m² - 3 x 4m5...</td>\n","      <td>1245644185</td>\n","      <td>3793572222</td>\n","      <td>2582</td>\n","      <td>6</td>\n","      <td>Household</td>\n","      <td>19</td>\n","      <td>293</td>\n","      <td>312</td>\n","      <td>Tente Pliante V3s5-Pro Pvc Blanc - 3 X 4m50 - ...</td>\n","    </tr>\n","    <tr>\n","      <th>10</th>\n","      <td>10</td>\n","      <td>Eames Inspired Sxw Chair - Pink - Black</td>\n","      <td>The timeless DSW seat can now be paired with m...</td>\n","      <td>1111840281</td>\n","      <td>1915836983</td>\n","      <td>1560</td>\n","      <td>7</td>\n","      <td>Household</td>\n","      <td>8</td>\n","      <td>94</td>\n","      <td>102</td>\n","      <td>Eames Inspired Sxw Chair - Pink - Black The ti...</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>84908</th>\n","      <td>84908</td>\n","      <td>Dimmerable Usb Led Lampe De Bureau Réglable Ch...</td>\n","      <td>Nom de la marque:oobest&lt;br /&gt;Ampoules incluses...</td>\n","      <td>1313620762</td>\n","      <td>4198481300</td>\n","      <td>2060</td>\n","      <td>17</td>\n","      <td>Household</td>\n","      <td>18</td>\n","      <td>37</td>\n","      <td>55</td>\n","      <td>Dimmerable Usb Led Lampe De Bureau Réglable Ch...</td>\n","    </tr>\n","    <tr>\n","      <th>84909</th>\n","      <td>84909</td>\n","      <td>espa - kit complet de nage à contre courant 39...</td>\n","      <td>espa espa - kit complet de nage à contre coura...</td>\n","      <td>1043841028</td>\n","      <td>853455937</td>\n","      <td>2583</td>\n","      <td>12</td>\n","      <td>Household</td>\n","      <td>17</td>\n","      <td>173</td>\n","      <td>190</td>\n","      <td>espa - kit complet de nage à contre courant 39...</td>\n","    </tr>\n","    <tr>\n","      <th>84910</th>\n","      <td>84910</td>\n","      <td>Vêtements Pour Animaux Mode Style Chiens Rayé ...</td>\n","      <td>le t - shirt rayé mode chiens  petits chiots v...</td>\n","      <td>1158527239</td>\n","      <td>2699568414</td>\n","      <td>2220</td>\n","      <td>22</td>\n","      <td>Household</td>\n","      <td>12</td>\n","      <td>168</td>\n","      <td>180</td>\n","      <td>Vêtements Pour Animaux Mode Style Chiens Rayé ...</td>\n","    </tr>\n","    <tr>\n","      <th>84912</th>\n","      <td>84912</td>\n","      <td>Kit piscine acier NEVADA déco pierre Ø 3.50m x...</td>\n","      <td>&lt;b&gt;Description complète :&lt;/b&gt;&lt;br /&gt;Kit piscine...</td>\n","      <td>1188462883</td>\n","      <td>3065095706</td>\n","      <td>2583</td>\n","      <td>12</td>\n","      <td>Household</td>\n","      <td>10</td>\n","      <td>190</td>\n","      <td>200</td>\n","      <td>Kit piscine acier NEVADA déco pierre Ø 3.50m x...</td>\n","    </tr>\n","    <tr>\n","      <th>84914</th>\n","      <td>84914</td>\n","      <td>Table Basse Bois De Récupération Massif Base B...</td>\n","      <td>&lt;p&gt;Cette table basse a un design unique et con...</td>\n","      <td>1267353403</td>\n","      <td>3942400296</td>\n","      <td>1560</td>\n","      <td>7</td>\n","      <td>Household</td>\n","      <td>9</td>\n","      <td>262</td>\n","      <td>271</td>\n","      <td>Table Basse Bois De Récupération Massif Base B...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>55116 rows × 12 columns</p>\n","</div>"],"text/plain":["       Integer_id  ...                                         title_desc\n","2               2  ...  Grand Stylet Ergonomique Bleu Gamepad Nintendo...\n","4               4  ...  La Guerre Des Tuques Luc a des id&eacute;es de...\n","7               7  ...  Conquérant Sept Cahier Couverture Polypro 240 ...\n","9               9  ...  Tente Pliante V3s5-Pro Pvc Blanc - 3 X 4m50 - ...\n","10             10  ...  Eames Inspired Sxw Chair - Pink - Black The ti...\n","...           ...  ...                                                ...\n","84908       84908  ...  Dimmerable Usb Led Lampe De Bureau Réglable Ch...\n","84909       84909  ...  espa - kit complet de nage à contre courant 39...\n","84910       84910  ...  Vêtements Pour Animaux Mode Style Chiens Rayé ...\n","84912       84912  ...  Kit piscine acier NEVADA déco pierre Ø 3.50m x...\n","84914       84914  ...  Table Basse Bois De Récupération Massif Base B...\n","\n","[55116 rows x 12 columns]"]},"metadata":{"tags":[]},"execution_count":115}]},{"cell_type":"markdown","metadata":{"id":"VAqoRpq9zMS4"},"source":["# View Tokenizer Input "]},{"cell_type":"code","metadata":{"id":"lhxfSqOKlR94","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1620397646950,"user_tz":-120,"elapsed":22401,"user":{"displayName":"Sara Fattouh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjGrdvZo2x_8K8abskRubIgMK3SdtS4cG-aQE01nA=s64","userId":"09360594903764573322"}},"outputId":"503a98f5-ae20-48ac-ba68-d4787abc4fa5"},"source":["Preprocess.get_sentences(text_col,True)\n","sentences = Preprocess.sentences\n","\n","labels = Preprocess.labels\n","print(sentences)\n"],"execution_count":116,"outputs":[{"output_type":"stream","text":["['Grand Stylet Ergonomique Bleu Gamepad Nintendo Wii U - Speedlink Pilot Style PILOT STYLE Touch Pen de marque Speedlink est 1 stylet ergonomique pour GamePad Nintendo Wii U.<br> Pour un confort optimal et une précision maximale sur le GamePad de la Wii U: ce grand stylet hautement ergonomique est non seulement parfaitement adapté à votre main mais aussi très élégant.<br> Il est livré avec un support qui se fixe sans adhésif à l\\'arrière du GamePad<br> <br> Caractéristiques:<br> Modèle: Speedlink PILOT STYLE Touch Pen<br> Couleur: Bleu<br> Ref. Fabricant: SL-3468-BE<br> Compatibilité: GamePad Nintendo Wii U<br> Forme particulièrement ergonomique excellente tenue en main<br> Pointe à revêtement longue durée conçue pour ne pas abîmer l\\'écran tactile<br> En bonus : Support inclu pour GamePad<br> <span class=\"vga_style2\"><b></b><br>'\n"," \"La Guerre Des Tuques Luc a des id&eacute;es de grandeur. Il veut organiser un jeu de guerre de boules de neige et s'arranger pour en &ecirc;tre le vainqueur incontest&eacute;. Mais Sophie s'en m&ecirc;le et chambarde tous ses plans...\"\n"," 'Conquérant Sept Cahier Couverture Polypro 240 X 320 Mm 96 Pages 90g Seyès Incolore CONQUERANT CLASSIQUE Cahier 240 x 320 mm seyès incolorecouverture en Polypro 96 pages agrafé papier de 90 g/m2(400006764)'\n"," ...\n"," 'Vêtements Pour Animaux Mode Style Chiens Rayé T-Shirt Costume Petit Chiot Rouge le t - shirt rayé mode chiens  petits chiots vêtements<ul><li>note: veuillez comparer le détail tailles avec toi avant d acheter.!!utiliser les mêmes</li><li>les vêtements de comparer avec la taille.</li><li>description:</li><li>100% brand new la qualité élevée</li><li>quantité: 1</li><li>matériel: coton</li><li>motif: le</li><li>votre chien est plus élégante cool</li><li>parfait pour la marche  jogging</li><li>attention: comme différents ordinateurs afficher les couleurs différemment  la couleur de la poste peut varier légèrement d images ci - dessus  merci pour votre compréhension.</li><li>toutes les dimensions sont mesurées à la main  il y a peut - être 2-3cm déviations  merci pour ta compréhension</li><li>taille des détails:</li><li>taille: s</li><li>cou: 24 cm / 9 h 45  </li><li>dos: 23cm / 9.06  </li><li>bust: 34cm / 13.39  </li><li>taille: m </li><li>cou: 28 / 11.02  </li><li>dos: isbn / 10 63  </li><li>bust: 40 cm / 15.75</li><li>taille: l</li><li>cou: 34cm / 13.39  </li><li>retour: 31cm / 12.20  </li><li>buste: 44 / 17.32 po</li><li>taille: xl</li><li>cou: 38 cm / 14.96  </li><li>: 37cm / 14.57 po</li><li>bust: 51cm / 20.08  </li><li>taille: xxl</li><li>cou: 42cm / 16.54 po</li><li>: 41cm / 16.14  </li><li>buste: 60 cm / 23.62 po</li><li>forfait comprend:</li><li>le t - shirt 1pcs pet</li><li></li></ul>'\n"," 'Kit piscine acier NEVADA déco pierre Ø 3.50m x 0.90m <b>Description complète :</b><br />Kit piscine hors-sol Toi PIEDRA GRIS ronde Ø 3.50m hauteur 0.90m. Parois acier liner 30/100eme uni bleu revêtement breveté exclusif imitant la pierre échelle profilés en PVC. Kit piscine complet.<br /><br /><b>Caractéristiques détaillées :</b><br />- Forme : Ronde<br />- Type : Kit piscine acier hors-sol<br />- Dimensions extérieures : Ø3.50 x 0.90m<br />- Surface installation : 3.60m x 3.60m<br />- Hauteur avec margelle : 0.90m<br />- Utilisation : Hors-sol<br />- Capacité : 8m3<br />- Kit complet : Oui<br />- Liner : Uni bleu 30/100e avec fixation overlap<br />- Largeur des margelles : Sans<br />- Structure : Parois acier anti-corrosion<br />- Epaisseur des parois : Acier 35/100eme<br />- Jambes de force : Sans jambes de forces apparentes<br />- Revêtement extérieur : Parois laquées avec décoration pierre &#34;Stone Effect&#34;<br />- Type de filtration : Filtration à cartouche<br />- Débit : 2m³ / heure<br />- Pompe : 46W<br />- Garantie structure : 2 ans<br />- Garantie liner : 2 ans<br />- Garantie filtration : 2 ans<br />- Echelle : Symétrique acier 2 marches<br />- Notice de montage : Oui<br />- Livraison : 1 palette<br />- Garantie : 2 ans<br />'\n"," 'Table Basse Bois De Récupération Massif Base Blanche 60x60x33cm <p>Cette table basse a un design unique et constituera un ajout intemporel à votre maison. Son dessus de table en bois massif est idéal pour ranger vos boissons panier de fruits ou objets décoratifs et sa base en acier solide ajoute à la robustesse de la table d&#39;appoint. La table basse est faite de bois de récupération massif provenant de solives de planchers et de poutres de soutien de vieux bâtiments en cours de démolition et peut être composée de différents types de bois comme le Sesham (bois de rose) le pin le teck le hêtre le chêne le cèdre le bois de manguier l&#39;acacia etc. Cela signifie que le bois de récupération conserve les caractéristiques de ces différents types de bois. Le bois récupéré est déjà vieilli patiné et séché de sorte qu&#39;il ne rétrécit pas ne se plie pas et n&#39;a pas besoin d&#39;une finition. Chaque étape du processus est réalisée avec le plus grand soin que ce soit le ponçage la peinture ou le laquage. Les belles fibres de bois rendent chaque meuble unique et légèrement différent du suivant. Les signes d&#39;usure et la structure fibreuse visible donnent à chaque pièce son histoire et un aspect unique. L&#39;article est déjà assemblé ; aucun assemblage n&#39;est requis. Remarque importante : les couleurs varient d&#39;un meuble à l&#39;autre rendant chacune de nos tables basses unique la livraison est aléatoire.</p> <ul><li>Couleur de base : Blanc</li><li>Matériau : dessus de table en bois massif de récupération &#43; base en acier</li><li>Dimensions : 60 x 33 cm (Diam. x H)</li><li>Produit poncé peint et laqué</li><li>Aucun assemblage requis</li></ul>']\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"XXlPVw5U1pos","executionInfo":{"status":"ok","timestamp":1620397646951,"user_tz":-120,"elapsed":22390,"user":{"displayName":"Sara Fattouh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjGrdvZo2x_8K8abskRubIgMK3SdtS4cG-aQE01nA=s64","userId":"09360594903764573322"}},"outputId":"ff96b35a-dff0-4636-fc65-b650b6214b7c"},"source":["print (type(sentences))\n","print()\n","print(\"Total number of sentences:{}, labels:{}\".format(len(sentences), len(labels)))"],"execution_count":117,"outputs":[{"output_type":"stream","text":["<class 'numpy.ndarray'>\n","\n","Total number of sentences:55116, labels:55116\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"oVemBMlCk4xW"},"source":["**Helper Function**"]},{"cell_type":"code","metadata":{"id":"WV-Xbdjxk3Ii","executionInfo":{"status":"ok","timestamp":1620397646951,"user_tz":-120,"elapsed":22379,"user":{"displayName":"Sara Fattouh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjGrdvZo2x_8K8abskRubIgMK3SdtS4cG-aQE01nA=s64","userId":"09360594903764573322"}}},"source":["def format_time(elapsed):\n","    '''\n","    Takes a time in seconds and returns a string hh:mm:ss\n","    '''\n","    # Round to the nearest second.\n","    elapsed_rounded = int(round((elapsed)))\n","    \n","    # Format as hh:mm:ss\n","    return str(datetime.timedelta(seconds=elapsed_rounded))"],"execution_count":118,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ZIbKzQwxlrZm"},"source":["# 3. Tokenization & Input Formatting\n"," Transform our dataset into the format that BERT can be trained on."]},{"cell_type":"markdown","metadata":{"id":"Ud5qFfdmlwsk"},"source":["# 3.1. BERT Tokenizer\n","To feed our text to BERT, it must be split into tokens, and then these tokens must be mapped to their index in the tokenizer vocabulary.\n","\n","The tokenization must be performed by the tokenizer included within BERT"]},{"cell_type":"code","metadata":{"id":"AbEg-aB2l61a","executionInfo":{"status":"ok","timestamp":1620397646952,"user_tz":-120,"elapsed":22372,"user":{"displayName":"Sara Fattouh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjGrdvZo2x_8K8abskRubIgMK3SdtS4cG-aQE01nA=s64","userId":"09360594903764573322"}}},"source":["from transformers import XLMForSequenceClassification\n","from transformers import FlaubertModel, FlaubertTokenizer,FlaubertForSequenceClassification,AdamW, FlaubertConfig \n","\n","from torch.nn import Dropout,Conv1d, Linear\n","from transformers.modeling_utils import SequenceSummary\n","\n","#from transformers.modeling_roberta import RobertaClassificationHead\n"],"execution_count":119,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"0l3oZ1Tims3q"},"source":["# 3.2. Required Formatting\n","We are required to give it a number of pieces of information\n","\n","We need to:\n","\n","Add special tokens to the start and end of each sentence.\n","Pad & truncate all sentences to a single constant length.\n","Explicitly differentiate real tokens from padding tokens with the “attention mask”."]},{"cell_type":"markdown","metadata":{"id":"fZvQKKcxmu4L"},"source":["# 3.3. Tokenize Dataset\n","We will use \"encode_plus\":\n","\n","returns a dictionary containing the encoded sequence or sequence pair and additional information: the mask for sequence classification and the overflowing elements if a max_length is specified."]},{"cell_type":"code","metadata":{"id":"lSBvH82Amx_3","executionInfo":{"status":"ok","timestamp":1620397646953,"user_tz":-120,"elapsed":22366,"user":{"displayName":"Sara Fattouh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjGrdvZo2x_8K8abskRubIgMK3SdtS4cG-aQE01nA=s64","userId":"09360594903764573322"}}},"source":["def prep_input(sentences,labels, max_len,tokenizer):\n","    input_ids = []\n","    attention_masks = []\n","\n","    # For every sentence...\n","    for sent in sentences:\n","        # `encode_plus` will:\n","        #   (1) Tokenize the sentence.\n","        #   (2) Prepend the `[CLS]` token to the start.\n","        #   (3) Append the `[SEP]` token to the end.\n","        #   (4) Map tokens to their IDs.\n","        #   (5) Pad or truncate the sentence to `max_length`\n","        #   (6) Create attention masks for [PAD] tokens.\n","        encoded_dict = tokenizer.encode_plus(\n","                            sent,                           # Sentence to encode.\n","                            add_special_tokens = True,      # Add '[CLS]' and '[SEP]'\n","                            max_length = max_len,           # Pad & truncate all sentences.\n","                            pad_to_max_length = True,\n","                            return_attention_mask = True,   # Construct attn. masks.\n","                            return_tensors = 'pt',     # Return pytorch tensors.\n","                       )\n","\n","        # Add the encoded sentence to the list.    \n","        input_ids.append(encoded_dict['input_ids'])       # IDs of the the vocabularies in the Model's dictionary\n","\n","        # And its attention mask (simply differentiates padding from non-padding).\n","        attention_masks.append(encoded_dict['attention_mask'])\n","\n","    # Convert the lists into tensors. \n","    input_ids = torch.cat(input_ids, dim=0)             # Concatenates the given sequence of seq tensors in the given dimension. \n","                                                        # All tensors must  have the same shape \n","    attention_masks = torch.cat(attention_masks, dim=0)\n","\n","    if labels is not None:\n","        labels = torch.tensor(labels)\n","        return input_ids, attention_masks, labels\n","    else:\n","        return input_ids, attention_masks"],"execution_count":120,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"KVKdFMMmP8aT"},"source":["# 3.4. Importing Tokenizers and Input Preparation\n","\n","- Now it is time to import both Camembert and FlauBERT tokenizers from  pretained package and prepare the input using them. \n","\n","- Calling prep_input() for each model will result in the corresponding:\n","     \n","\n","1.   **input ids**\n","2.   **attention maks**\n","3.   **labels**\n","\n"," "]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-BYzJChjw5AG","executionInfo":{"status":"ok","timestamp":1620397650077,"user_tz":-120,"elapsed":25482,"user":{"displayName":"Sara Fattouh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjGrdvZo2x_8K8abskRubIgMK3SdtS4cG-aQE01nA=s64","userId":"09360594903764573322"}},"outputId":"efba55c8-df4e-4ae4-a561-e4d6f180bcc5"},"source":["from transformers import CamembertConfig, CamembertTokenizer, CamembertModel, CamembertForSequenceClassification, AdamW\n","from transformers import FlaubertModel, FlaubertTokenizer,FlaubertForSequenceClassification,AdamW, FlaubertConfig \n","\n","print('Using Camembert')\n","tokenizer_cam = CamembertTokenizer.from_pretrained('camembert-base', do_lowercase=False)\n","print('Using Flaubert')\n","tokenizer_flau = FlaubertTokenizer.from_pretrained('flaubert/flaubert_base_cased', do_lowercase=False)"],"execution_count":121,"outputs":[{"output_type":"stream","text":["Using Camembert\n","Using Flaubert\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"vAvaWiVEwq54","executionInfo":{"status":"ok","timestamp":1620397716430,"user_tz":-120,"elapsed":91823,"user":{"displayName":"Sara Fattouh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjGrdvZo2x_8K8abskRubIgMK3SdtS4cG-aQE01nA=s64","userId":"09360594903764573322"}},"outputId":"f9edc8e6-17e7-4444-acf7-a5d1da8b38a9"},"source":["input_ids_cam, attention_masks_cam, labels_cam = prep_input (sentences, labels, max_len, tokenizer_cam)\n"],"execution_count":122,"outputs":[{"output_type":"stream","text":["Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"o-4ArTutx-Cx","executionInfo":{"status":"ok","timestamp":1620397915745,"user_tz":-120,"elapsed":291127,"user":{"displayName":"Sara Fattouh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjGrdvZo2x_8K8abskRubIgMK3SdtS4cG-aQE01nA=s64","userId":"09360594903764573322"}},"outputId":"7fd0db8a-d478-4e53-f0d3-65875459f00d"},"source":["input_ids_flau, attention_masks_flau, labels_flau  = prep_input(sentences,labels, max_len,tokenizer_flau)"],"execution_count":123,"outputs":[{"output_type":"stream","text":["Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"],"name":"stderr"}]},{"cell_type":"markdown","metadata":{"id":"7XrGAY_zRX--"},"source":["# 3.5. Training & Validation Split\n","Divide up our training randomly select **10%** as a validation set off of the training set.\n","\n","While splitting, we used the following parameters:\n","\n","\n","1.   **stratify**: \n","in this context, stratification means that the train_test_split method returns training and test subsets that have the same proportions of class labels as the input dataset.\n","2.   **random_state**: \n","simply sets a seed to the random generator, so that your train-test splits are always deterministic. If you don't set a seed, it is different each time."]},{"cell_type":"code","metadata":{"id":"nZQ0mB1NyeWa","executionInfo":{"status":"ok","timestamp":1620397915749,"user_tz":-120,"elapsed":291120,"user":{"displayName":"Sara Fattouh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjGrdvZo2x_8K8abskRubIgMK3SdtS4cG-aQE01nA=s64","userId":"09360594903764573322"}}},"source":["# val_size = 0.1\n","\n","# tr_inputs_cam, val_inputs_cam, _,_ = train_test_split (input_ids_cam, labels_cam, stratify = labels_cam,    \n","#                                                             random_state=2020, test_size = val_size)\n","\n","# tr_masks_cam, val_masks_cam, _,_ =   train_test_split (attention_masks_cam, labels, stratify = labels,        # labels: Preprocess.labels\n","#                                                             random_state=2020, test_size = val_size)\n","\n","# tr_inputs_flau, val_inputs_flau, _,_ = train_test_split (input_ids_flau, labels_flau, stratify=labels,\n","#                                                             random_state=2020, test_size = val_size)\n","\n","# tr_masks_flau, val_masks_flau, _,_   = train_test_split (attention_masks_flau, labels,stratify=labels_flau,  # labels: Preprocess.labels\n","#                                                             random_state=2020, test_size = val_size)"],"execution_count":124,"outputs":[]},{"cell_type":"code","metadata":{"id":"SfiE2yDIzuRO","executionInfo":{"status":"ok","timestamp":1620397915749,"user_tz":-120,"elapsed":291113,"user":{"displayName":"Sara Fattouh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjGrdvZo2x_8K8abskRubIgMK3SdtS4cG-aQE01nA=s64","userId":"09360594903764573322"}}},"source":["# torch.save(tr_inputs_cam, \"tr_inputs_cam.pt\")\n","# torch.save(val_inputs_cam, \"val_inputs_cam.pt\")\n","# torch.save(tr_masks_cam, \"tr_masks_cam.pt\")\n","# torch.save(val_masks_cam, \"val_masks_cam.pt\")\n","\n","# torch.save(tr_inputs_flau, \"tr_inputs_flau.pt\")\n","# torch.save(val_inputs_flau, \"val_inputs_flau.pt\")\n","# torch.save(tr_masks_flau, \"tr_masks_flau.pt\")\n","# torch.save(val_masks_flau, \"val_masks_flau.pt\")"],"execution_count":125,"outputs":[]},{"cell_type":"code","metadata":{"id":"mdkITHXh0Ahs","executionInfo":{"status":"ok","timestamp":1620397915750,"user_tz":-120,"elapsed":291107,"user":{"displayName":"Sara Fattouh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjGrdvZo2x_8K8abskRubIgMK3SdtS4cG-aQE01nA=s64","userId":"09360594903764573322"}}},"source":["# text_input='./'\n","\n","# tr_inputs_cam = torch.load(text_input + \"tr_inputs_cam.pt\")\n","# val_inputs_cam = torch.load(text_input +\"val_inputs_cam.pt\")\n","# tr_masks_cam = torch.load(text_input + \"tr_masks_cam.pt\")\n","# val_masks_cam = torch.load(text_input + \"val_masks_cam.pt\")\n","\n","# tr_inputs_flau = torch.load(text_input + \"tr_inputs_flau.pt\")\n","# val_inputs_flau = torch.load(text_input + \"val_inputs_flau.pt\")\n","# tr_masks_flau = torch.load(text_input + \"tr_masks_flau.pt\")\n","# val_masks_flau = torch.load(text_input + \"val_masks_flau.pt\")"],"execution_count":126,"outputs":[]},{"cell_type":"code","metadata":{"id":"4HFg5HwahAij","executionInfo":{"status":"ok","timestamp":1620397916098,"user_tz":-120,"elapsed":291449,"user":{"displayName":"Sara Fattouh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjGrdvZo2x_8K8abskRubIgMK3SdtS4cG-aQE01nA=s64","userId":"09360594903764573322"}}},"source":["tr_inputs, test_inputs_cam, tr_labels, test_labels_cam = train_test_split(input_ids_cam, labels_cam, stratify=labels_cam, random_state=2020,\n","                                                                test_size = 0.2)\n","\n","tr_inputs_cam, val_inputs_cam, train_labels, val_labels = train_test_split(tr_inputs, tr_labels, stratify=tr_labels, random_state=2020,\n","                                                                test_size = 0.15)\n","\n","tr_masks, test_masks_cam, tr_masks_labels, _ =   train_test_split(attention_masks_cam, labels, stratify=labels, random_state=2020,\n","                                                                 test_size=0.2)\n","\n","tr_masks_cam, val_masks_cam, u,v =   train_test_split(tr_masks, tr_masks_labels, stratify=tr_masks_labels, random_state=2020,\n","                                                                test_size=0.15 )"],"execution_count":127,"outputs":[]},{"cell_type":"code","metadata":{"id":"dGL3n3GUhhGI","executionInfo":{"status":"ok","timestamp":1620397916098,"user_tz":-120,"elapsed":291442,"user":{"displayName":"Sara Fattouh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjGrdvZo2x_8K8abskRubIgMK3SdtS4cG-aQE01nA=s64","userId":"09360594903764573322"}}},"source":["tr_inputs, test_inputs_flau, tr_labels, test_labels_flau = train_test_split(input_ids_cam, labels_cam, stratify=labels_cam, random_state=2020,\n","                                                                test_size = 0.2)\n","\n","tr_inputs_flau, val_inputs_flau, train_labels, val_labels = train_test_split(tr_inputs, tr_labels, stratify=tr_labels, random_state=2020,\n","                                                                test_size = 0.15)\n","\n","tr_masks, test_masks_flau, tr_masks_labels, _ =   train_test_split(attention_masks_cam, labels, stratify=labels, random_state=2020,\n","                                                                 test_size=0.2)\n","\n","tr_masks_flau, val_masks_flau, u,v =   train_test_split(tr_masks, tr_masks_labels, stratify=tr_masks_labels, random_state=2020,\n","                                                                test_size=0.15 )"],"execution_count":128,"outputs":[]},{"cell_type":"code","metadata":{"id":"y__2u2hzhObl","executionInfo":{"status":"ok","timestamp":1620397917361,"user_tz":-120,"elapsed":292698,"user":{"displayName":"Sara Fattouh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjGrdvZo2x_8K8abskRubIgMK3SdtS4cG-aQE01nA=s64","userId":"09360594903764573322"}}},"source":["torch.save(tr_inputs_cam, \"tr_inputs_cam.pt\")\n","torch.save(val_inputs_cam, \"val_inputs_cam.pt\")\n","torch.save(tr_masks_cam, \"tr_masks_cam.pt\")\n","torch.save(val_masks_cam, \"val_masks_cam.pt\")\n","torch.save(test_inputs_cam, \"test_inputs_cam.pt\")\n","torch.save(test_masks_cam, \"test_masks_cam.pt\")\n","\n","\n","torch.save(tr_inputs_flau, \"tr_inputs_flau.pt\")\n","torch.save(val_inputs_flau, \"val_inputs_flau.pt\")\n","torch.save(tr_masks_flau, \"tr_masks_flau.pt\")\n","torch.save(val_masks_flau, \"val_masks_flau.pt\")\n","torch.save(test_inputs_flau, \"test_inputs_flau.pt\")\n","torch.save(test_masks_flau, \"test_masks_flau.pt\")\n","\n"],"execution_count":129,"outputs":[]},{"cell_type":"code","metadata":{"id":"tB_LU5TRhUov","executionInfo":{"status":"ok","timestamp":1620397917364,"user_tz":-120,"elapsed":292694,"user":{"displayName":"Sara Fattouh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjGrdvZo2x_8K8abskRubIgMK3SdtS4cG-aQE01nA=s64","userId":"09360594903764573322"}}},"source":["text_input='./'\n","\n","tr_inputs_cam = torch.load(text_input + \"tr_inputs_cam.pt\")\n","val_inputs_cam = torch.load(text_input +\"val_inputs_cam.pt\")\n","tr_masks_cam = torch.load(text_input + \"tr_masks_cam.pt\")\n","val_masks_cam = torch.load(text_input + \"val_masks_cam.pt\")\n","input_ids_test_cam = torch.load(text_input + \"test_inputs_cam.pt\") \n","attention_masks_test_cam = torch.load(text_input + \"test_masks_cam.pt\") \n","\n","tr_inputs_flau = torch.load(text_input + \"tr_inputs_flau.pt\")\n","val_inputs_flau = torch.load(text_input + \"val_inputs_flau.pt\")\n","tr_masks_flau = torch.load(text_input + \"tr_masks_flau.pt\")\n","val_masks_flau = torch.load(text_input + \"val_masks_flau.pt\")\n","input_ids_test_flau = torch.load(text_input + \"test_inputs_flau.pt\")\n","attention_masks_test_flau = torch.load(text_input + \"test_masks_flau.pt\")"],"execution_count":130,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"i1O4EyDzCpsS"},"source":["# 4. Defining Models to be Fused\n","\n","Now, as our  data has been preprocessed, cleaned and text data was tokenzied , it is ready to be fed to the models. \n","- As a first step,  first we need to define and configure the models. \n"]},{"cell_type":"markdown","metadata":{"id":"rlj0Dz1FW7cM"},"source":["# 4.1. RESNet Model for Image Processing. \n","\n","In PyTorch, you always need to define a forward method for your neural network model. But you never have to call it explicitly.\n","Here we are defining our image processing class is subclass of nn.Module and is inheriting all methods. In the super class, nn.Module, there is a __call__ method which obtains the forward function from the subclass and calls it."]},{"cell_type":"markdown","metadata":{"id":"rnnJlj2Y3iii"},"source":["# 4.1.1.  Image Processing Model - RESNet50\n","\n","\n"]},{"cell_type":"code","metadata":{"id":"QWvMo_nV1XCb","executionInfo":{"status":"ok","timestamp":1620397917364,"user_tz":-120,"elapsed":292686,"user":{"displayName":"Sara Fattouh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjGrdvZo2x_8K8abskRubIgMK3SdtS4cG-aQE01nA=s64","userId":"09360594903764573322"}}},"source":["from torch.nn import functional as F\n","import torch.nn as nn\n","import pretrainedmodels\n","class SEResnext50_32x4d(nn.Module):\n","    def __init__(self, pretrained='imagenet'):\n","        super(SEResnext50_32x4d, self).__init__()\n","        \n","        self.base_model = pretrainedmodels.__dict__[\"se_resnext50_32x4d\"](pretrained=None)\n","        if pretrained is not None:\n","            self.base_model.load_state_dict(\n","                self.base_model.load_state_dict(torch.load(resnet_model_path))\n","                )\n","            \n","        self.l0 = nn.Linear(2048, 27)  # Applies a linear transformation to the incoming data\n","        # batch_size = 2048\n","    \n","    def forward(self, image):\n","        batch_size, _, _, _ = image.shape\n","\n","        # During the training you will get batches of images, \n","        # so your shape in the forward method will get an additional batch dimension at dim0: \n","        # [batch_size, channels, height, width].\n","        \n","        x = self.base_model.features(image) \n","\n","        #extracting feature vector from network after feature leaning \n","        #This is the flatten vector \n","\n","        x = F.adaptive_avg_pool2d(x, 1).reshape(batch_size, -1) \n","        #adaptive_avg_pool2d : Kernel size = (input_size+target_size-1) // target_size rounded up\n","        #Then the positions of where to apply the kernel are computed as rounded equidistant points between 0 and input_size - kernel_size\n","        \n","        out = self.l0(x)\n","\n","        return out"],"execution_count":131,"outputs":[]},{"cell_type":"code","metadata":{"id":"NaAYA3QY1cbO","executionInfo":{"status":"ok","timestamp":1620397917365,"user_tz":-120,"elapsed":292680,"user":{"displayName":"Sara Fattouh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjGrdvZo2x_8K8abskRubIgMK3SdtS4cG-aQE01nA=s64","userId":"09360594903764573322"}}},"source":["class Identity(nn.Module):\n","  \n","    def __init__(self):\n","        super(Identity, self).__init__()\n","        \n","    def forward(self, x):\n","        return x"],"execution_count":132,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"vr8ciZGg1Ee4"},"source":["# 4.1.2. Instaniating the Image Processing Network \n"," Now we create an instance from the SEResnext50_32x4d class that we defined and load the weights from a pretrained model, since the training is done previously. "]},{"cell_type":"code","metadata":{"id":"ZGGor-mS1zJe","executionInfo":{"status":"ok","timestamp":1620397917917,"user_tz":-120,"elapsed":293225,"user":{"displayName":"Sara Fattouh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjGrdvZo2x_8K8abskRubIgMK3SdtS4cG-aQE01nA=s64","userId":"09360594903764573322"}}},"source":["#data_path = '/content/drive/My Drive/Rakuten/'\n","\n","img_model = SEResnext50_32x4d(pretrained=None)\n","# img_model.load_state_dict(torch.load(os.path.join(data_path, 'models/RESNET_best_model.pt')))\n","\n","# img_model.cuda()"],"execution_count":133,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"P9gfXO9k21-o"},"source":["# 4.1.3. Prinitng Model's Params"]},{"cell_type":"code","metadata":{"id":"WOiwcopd8m8B","executionInfo":{"status":"ok","timestamp":1620397917917,"user_tz":-120,"elapsed":293218,"user":{"displayName":"Sara Fattouh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjGrdvZo2x_8K8abskRubIgMK3SdtS4cG-aQE01nA=s64","userId":"09360594903764573322"}}},"source":["img_model.l0 = Identity()"],"execution_count":134,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Jb7I6qBq2c0E","executionInfo":{"status":"ok","timestamp":1620397917918,"user_tz":-120,"elapsed":293211,"user":{"displayName":"Sara Fattouh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjGrdvZo2x_8K8abskRubIgMK3SdtS4cG-aQE01nA=s64","userId":"09360594903764573322"}},"outputId":"d2567534-544c-4d40-9669-6c5e4765f82d"},"source":["for param in img_model.parameters():\n","     print(type(param), param.size())"],"execution_count":135,"outputs":[{"output_type":"stream","text":["<class 'torch.nn.parameter.Parameter'> torch.Size([64, 3, 7, 7])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([64])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([64])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([128, 64, 1, 1])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([128])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([128])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([128, 4, 3, 3])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([128])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([128])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([256, 128, 1, 1])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([256])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([256])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([16, 256, 1, 1])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([16])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([256, 16, 1, 1])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([256])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([256, 64, 1, 1])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([256])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([256])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([128, 256, 1, 1])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([128])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([128])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([128, 4, 3, 3])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([128])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([128])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([256, 128, 1, 1])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([256])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([256])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([16, 256, 1, 1])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([16])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([256, 16, 1, 1])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([256])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([128, 256, 1, 1])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([128])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([128])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([128, 4, 3, 3])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([128])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([128])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([256, 128, 1, 1])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([256])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([256])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([16, 256, 1, 1])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([16])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([256, 16, 1, 1])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([256])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([256, 256, 1, 1])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([256])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([256])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([256, 8, 3, 3])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([256])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([256])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([512, 256, 1, 1])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([512])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([512])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([32, 512, 1, 1])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([32])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([512, 32, 1, 1])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([512])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([512, 256, 1, 1])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([512])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([512])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([256, 512, 1, 1])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([256])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([256])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([256, 8, 3, 3])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([256])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([256])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([512, 256, 1, 1])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([512])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([512])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([32, 512, 1, 1])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([32])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([512, 32, 1, 1])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([512])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([256, 512, 1, 1])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([256])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([256])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([256, 8, 3, 3])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([256])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([256])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([512, 256, 1, 1])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([512])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([512])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([32, 512, 1, 1])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([32])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([512, 32, 1, 1])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([512])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([256, 512, 1, 1])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([256])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([256])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([256, 8, 3, 3])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([256])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([256])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([512, 256, 1, 1])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([512])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([512])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([32, 512, 1, 1])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([32])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([512, 32, 1, 1])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([512])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([512, 512, 1, 1])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([512])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([512])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([512, 16, 3, 3])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([512])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([512])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([1024, 512, 1, 1])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([1024])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([1024])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([64, 1024, 1, 1])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([64])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([1024, 64, 1, 1])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([1024])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([1024, 512, 1, 1])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([1024])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([1024])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([512, 1024, 1, 1])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([512])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([512])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([512, 16, 3, 3])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([512])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([512])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([1024, 512, 1, 1])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([1024])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([1024])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([64, 1024, 1, 1])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([64])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([1024, 64, 1, 1])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([1024])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([512, 1024, 1, 1])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([512])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([512])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([512, 16, 3, 3])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([512])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([512])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([1024, 512, 1, 1])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([1024])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([1024])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([64, 1024, 1, 1])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([64])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([1024, 64, 1, 1])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([1024])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([512, 1024, 1, 1])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([512])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([512])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([512, 16, 3, 3])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([512])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([512])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([1024, 512, 1, 1])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([1024])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([1024])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([64, 1024, 1, 1])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([64])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([1024, 64, 1, 1])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([1024])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([512, 1024, 1, 1])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([512])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([512])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([512, 16, 3, 3])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([512])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([512])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([1024, 512, 1, 1])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([1024])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([1024])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([64, 1024, 1, 1])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([64])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([1024, 64, 1, 1])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([1024])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([512, 1024, 1, 1])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([512])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([512])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([512, 16, 3, 3])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([512])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([512])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([1024, 512, 1, 1])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([1024])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([1024])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([64, 1024, 1, 1])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([64])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([1024, 64, 1, 1])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([1024])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([1024, 1024, 1, 1])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([1024])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([1024])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([1024, 32, 3, 3])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([1024])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([1024])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([2048, 1024, 1, 1])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([2048])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([2048])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([128, 2048, 1, 1])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([128])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([2048, 128, 1, 1])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([2048])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([2048, 1024, 1, 1])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([2048])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([2048])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([1024, 2048, 1, 1])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([1024])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([1024])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([1024, 32, 3, 3])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([1024])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([1024])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([2048, 1024, 1, 1])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([2048])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([2048])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([128, 2048, 1, 1])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([128])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([2048, 128, 1, 1])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([2048])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([1024, 2048, 1, 1])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([1024])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([1024])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([1024, 32, 3, 3])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([1024])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([1024])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([2048, 1024, 1, 1])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([2048])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([2048])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([128, 2048, 1, 1])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([128])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([2048, 128, 1, 1])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([2048])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([1000, 2048])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([1000])\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"9iqaXH5A3Esl"},"source":["# 4.1.4. Model's Params Require No Grads\n","\n","These are just regular tensors, with one very special addition: we tell PyTorch that they require a gradient. This causes PyTorch to record all of the operations done on the tensor, so that it can calculate the gradient during back-propagation automatically!\n","As our model is already trained and weights are assigned, then there is no need to calculate the gradients so no need to send them to the optimizer."]},{"cell_type":"code","metadata":{"id":"c0XJcLgW8u09","executionInfo":{"status":"ok","timestamp":1620397917919,"user_tz":-120,"elapsed":293201,"user":{"displayName":"Sara Fattouh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjGrdvZo2x_8K8abskRubIgMK3SdtS4cG-aQE01nA=s64","userId":"09360594903764573322"}}},"source":["# params are iterators which contain model's parameters. Usually passed to the optimizer\n","for params in img_model.parameters():\n","      params.requires_grad = False"],"execution_count":136,"outputs":[]},{"cell_type":"code","metadata":{"id":"vaXULFfzkdB8","executionInfo":{"status":"ok","timestamp":1620397917920,"user_tz":-120,"elapsed":293195,"user":{"displayName":"Sara Fattouh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjGrdvZo2x_8K8abskRubIgMK3SdtS4cG-aQE01nA=s64","userId":"09360594903764573322"}}},"source":["img_model.out_proj = Identity()"],"execution_count":137,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"JWgGQqbU4nNR"},"source":["# Image Data Preparation"]},{"cell_type":"code","metadata":{"id":"4yNKxOJ2-QrA","executionInfo":{"status":"ok","timestamp":1620397917921,"user_tz":-120,"elapsed":293189,"user":{"displayName":"Sara Fattouh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjGrdvZo2x_8K8abskRubIgMK3SdtS4cG-aQE01nA=s64","userId":"09360594903764573322"}}},"source":["# # Data path\n","# text_data_path = os.path.join('/content/drive/My Drive/Rakuten')\n","# image_data_path = os.path.join('')\n"],"execution_count":138,"outputs":[]},{"cell_type":"code","metadata":{"id":"SM6bWAoKTsx7","executionInfo":{"status":"ok","timestamp":1620397917921,"user_tz":-120,"elapsed":293182,"user":{"displayName":"Sara Fattouh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjGrdvZo2x_8K8abskRubIgMK3SdtS4cG-aQE01nA=s64","userId":"09360594903764573322"}}},"source":["def get_img_path(img_id, prd_id, path):\n","    \n","    pattern = 'image'+'_'+str(img_id)+'_'+'product'+'_'+str(prd_id)+'.jpg'\n","    return path + pattern"],"execution_count":139,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"oYomnrt7wKWw","executionInfo":{"status":"ok","timestamp":1620397917922,"user_tz":-120,"elapsed":293176,"user":{"displayName":"Sara Fattouh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjGrdvZo2x_8K8abskRubIgMK3SdtS4cG-aQE01nA=s64","userId":"09360594903764573322"}},"outputId":"38e04056-af67-4640-fa21-40787f231d24"},"source":["print(len(Preprocess.train), len(train))"],"execution_count":140,"outputs":[{"output_type":"stream","text":["55116 84916\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"QzgyQu0aY17J"},"source":["# Obtaining & Splitting Images "]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":236,"referenced_widgets":["aa0be85fdc614d018a9d1ba64b8e6254","f25b6155abd34796ab1a3325de8262cd","c2713fb931b848fe9a4e93dbe6567a50","b9441a2fed2d46599671c30553d2427b","ad67e57cfd9546a3beb5a4d7e72a9034","b3049adc2a2744c6b8be1a7098e0fc70","46c56615834b445e8d62d834e43af82a","207127a4c2e347ff8798b8f5c83100b6"]},"id":"emmAR45klxoi","executionInfo":{"status":"ok","timestamp":1620397919551,"user_tz":-120,"elapsed":294794,"user":{"displayName":"Sara Fattouh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjGrdvZo2x_8K8abskRubIgMK3SdtS4cG-aQE01nA=s64","userId":"09360594903764573322"}},"outputId":"029d6cdb-0741-42eb-d070-ec60e5efef44"},"source":["from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n","\n","#  train 65%\n","#  validation 15%\n","#  test 20%\n","# (original training) =>  train, test => 80, 20  val_size = 0.2\n","# (train) => train, val =>  85, 15   val_size = 0.15 \n","\n","\n","# train_img = train[['Image_id','Product_id','labels','product']]\n","train_img = Preprocess.train[['Image_id','Product_id','labels','product']] \n","\n","train_img['image_path'] = Preprocess.train.progress_apply(lambda x: get_img_path(x['Image_id'], x['Product_id'],\n","                                                      path = os.path.join(image_data_path, 'image_training/')),axis=1)\n","\n","\n","\n","\n","tr_df, test_df, tr_labels, test_labels = train_test_split(train_img, train_img['labels'], \n","                                           random_state=2020,\n","                                           test_size = 0.2,\n","                                           stratify=train_img['labels'])\n","\n","\n","train_df, val_df, train_labels, val_labels = train_test_split(tr_df, tr_labels, \n","                                           random_state=2020,\n","                                           test_size = 0.15,\n","                                           stratify=tr_labels)\n","\n","\n","\n","print(\"Train: \", len(train_df))\n","print(\"Val:   \", len(val_df))\n","print(\"Test:  \", len(test_df))\n","print (\"\")"],"execution_count":141,"outputs":[{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"aa0be85fdc614d018a9d1ba64b8e6254","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=0.0, max=55116.0), HTML(value='')))"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["\n","Train:  37478\n","Val:    6614\n","Test:   11024\n","\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:14: SettingWithCopyWarning: \n","A value is trying to be set on a copy of a slice from a DataFrame.\n","Try using .loc[row_indexer,col_indexer] = value instead\n","\n","See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n","  \n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"B_h5_e_Gw-M9","executionInfo":{"status":"ok","timestamp":1620397919552,"user_tz":-120,"elapsed":294782,"user":{"displayName":"Sara Fattouh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjGrdvZo2x_8K8abskRubIgMK3SdtS4cG-aQE01nA=s64","userId":"09360594903764573322"}},"outputId":"47b3e1dc-1be6-4739-b0ae-8f0af113f82b"},"source":["print(\"Original Images Df:   \",  len(train_img))\n","print(\"Train Images DF:      \" , len(train_df))\n","print(\"Validation Images DF: \",  len(val_df))"],"execution_count":142,"outputs":[{"output_type":"stream","text":["Original Images Df:    55116\n","Train Images DF:       37478\n","Validation Images DF:  6614\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"aIAzQAsCTzBs","executionInfo":{"status":"ok","timestamp":1620397919552,"user_tz":-120,"elapsed":294772,"user":{"displayName":"Sara Fattouh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjGrdvZo2x_8K8abskRubIgMK3SdtS4cG-aQE01nA=s64","userId":"09360594903764573322"}}},"source":["# print (train_img['image_path'][0])\n"],"execution_count":143,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"lRigVJwVBOcz"},"source":["# Image Data Augmentation\n","\n","We're going to be making use of Pytorch's transforms for preparing the input images to be used by ouur model. \n","\n","\n","\n","\n","\n","*   We'll need to make sure the images in the training set and validation set are the same size, so we'll be using transforms.Resize\n","*   We'll also be doing a little data augmentation, trying to improve the performance of our model by forcing it to learn about images at different angles and crops, so we'll randomly crop and rotate the images.\n","\n","*    we'll make tensors out of the images, as PyTorch works with tensors. \n","*   Finally, we'll normalize the images, which helps the network work with values that may be have a wide range of different values.\n","\n","\n","*   We then compose all our chosen transforms.\n","\n","It worth mentioning that validation transforms don't have any of the flipping or rotating, as they aren't part of our training set, so the network isn't learning about them\n","\n","\n","\n","\n","\n"]},{"cell_type":"code","metadata":{"id":"-Togfn7XmpW5","executionInfo":{"status":"ok","timestamp":1620397919553,"user_tz":-120,"elapsed":294766,"user":{"displayName":"Sara Fattouh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjGrdvZo2x_8K8abskRubIgMK3SdtS4cG-aQE01nA=s64","userId":"09360594903764573322"}}},"source":["input_size = 224 # for Resnt\n","\n","# Applying Transforms to the Data\n","\n","from torchvision import datasets, models, transforms\n","\n","image_transforms = { \n","    'train': transforms.Compose([\n","        transforms.RandomResizedCrop(size=256, scale=(0.8, 1.0)),\n","        transforms.RandomRotation(degrees=15),\n","        transforms.RandomHorizontalFlip(),\n","        transforms.Resize(size=256),\n","        transforms.CenterCrop(size=input_size),\n","        transforms.ToTensor(),\n","        transforms.Normalize([0.485, 0.456, 0.406],\n","                             [0.229, 0.224, 0.225])\n","    ]),\n","    'valid': transforms.Compose([\n","        transforms.Resize(size=256),\n","        transforms.CenterCrop(size=input_size),\n","        transforms.ToTensor(),\n","        transforms.Normalize([0.485, 0.456, 0.406],\n","                             [0.229, 0.224, 0.225])\n","    ]),\n","    'test': transforms.Compose([\n","        transforms.Resize(size=256),\n","        transforms.CenterCrop(size=input_size),\n","        transforms.ToTensor(),\n","        transforms.Normalize([0.485, 0.456, 0.406],\n","                             [0.229, 0.224, 0.225])\n","    ])\n","}"],"execution_count":144,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"nB9-K4lUNoqT"},"source":["# Text Processing Models - BertForSequenceClassification\n","\n","Thankfully, the huggingface pytorch implementation includes a set of interfaces designed for a variety of NLP tasks. Though these interfaces are all built on top of a trained BERT model, each has different top layers and output types designed to accomodate their specific NLP task.\n","\n","We first want to modify the pre-trained BERT model to give outputs for classification, and then we want to continue training the model on our dataset until that the entire model, end-to-end, is well-suited for our task.\n","\n","**BertForSequenceClassification** is one of the current of classes provided for fine-tuning.\n","\n","This is the normal BERT model with an added single linear layer on top for classification that we will use as a sentence classifier. As we feed input data, the entire pre-trained BERT model and the additional untrained classification layer is trained on our specific task.\n","\n","- Not to forget that Camembet model inherits RobertaModel"]},{"cell_type":"markdown","metadata":{"id":"I_vkDiuRC70z"},"source":["# 4.2 CamemBERT Model"]},{"cell_type":"code","metadata":{"id":"imOtgakyCtFe","executionInfo":{"status":"ok","timestamp":1620397919554,"user_tz":-120,"elapsed":294760,"user":{"displayName":"Sara Fattouh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjGrdvZo2x_8K8abskRubIgMK3SdtS4cG-aQE01nA=s64","userId":"09360594903764573322"}}},"source":["class vec_output_CamembertForSequenceClassification(CamembertModel):\n","    config_class = CamembertConfig\n","\n","    def __init__(self, config):\n","        super().__init__(config)\n","        self.num_labels = config.num_labels\n","\n","        self.roberta = CamembertModel(config)\n","        self.dense = nn.Linear(256*config.hidden_size, config.hidden_size)\n","        self.dropout = nn.Dropout(0.1)\n","        self.out_proj = nn.Linear(config.hidden_size, config.num_labels)\n","        self.init_weights()\n","\n","\n","    def forward(\n","        self,\n","        input_ids=None,\n","        attention_mask=None,\n","        token_type_ids=None,\n","        position_ids=None,\n","        head_mask=None,\n","        inputs_embeds=None,\n","        labels=None,\n","        output_attentions=None,\n","        output_hidden_states=None,\n","    ):\n","        outputs = self.roberta(\n","            input_ids,\n","            attention_mask = attention_mask,\n","            token_type_ids = token_type_ids,\n","            position_ids = position_ids,\n","            head_mask = head_mask,\n","            inputs_embeds=inputs_embeds,\n","#           output_attentions=output_attentions,\n","#           output_hidden_states=output_hidden_states,\n","        )\n","\n","        sequence_output = outputs[0] #(B,256,768)\n","\n","        x = sequence_output.view(sequence_output.shape[0], 256*768)\n","\n","#       x = sequence_output[:, 0, :]  # take <s> token (equiv. to [CLS])-> #(B,768) Image -> (B,2048)\n","\n","        x = self.dense(x)  # 768 -> 768\n","\n","        feat= torch.tanh(x) \n","\n","        logits = self.out_proj(feat) # 768 -> 27\n","\n","        outputs = (logits,) + outputs[2:] #3rd element onwards\n","\n","        return outputs,feat  # (loss), logits, (hidden_states), (attentions)"],"execution_count":145,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"OtPoFtaqDAj6"},"source":["# FlauBERT Model"]},{"cell_type":"code","metadata":{"id":"a16smoYhDCmn","executionInfo":{"status":"ok","timestamp":1620397919554,"user_tz":-120,"elapsed":294752,"user":{"displayName":"Sara Fattouh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjGrdvZo2x_8K8abskRubIgMK3SdtS4cG-aQE01nA=s64","userId":"09360594903764573322"}}},"source":["num_classes = 27\n","\n","class vec_output_FlaubertForSequenceClassification(FlaubertModel):\n","    \n","    config_class = FlaubertConfig\n","    \n","\n","    def __init__(self, config):\n","        super().__init__(config)\n","        self.transformer = FlaubertModel(config)\n","        self.sequence_summary = SequenceSummary(config)\n","        self.init_weights()\n","        self.dropout =  torch.nn.Dropout(0.1)\n","        self.classifier = torch.nn.Linear(config.hidden_size, num_classes)\n","\n","\n","    def forward(\n","        self,\n","        input_ids=None,\n","        attention_mask=None,\n","        langs=None,\n","        token_type_ids=None,\n","        position_ids=None,\n","        lengths=None,\n","        cache=None,\n","        head_mask=None,\n","        inputs_embeds=None,\n","        labels=None,\n","    ):\n","        \n","        \n","        transformer_outputs = self.transformer(\n","            input_ids,\n","            attention_mask=attention_mask,\n","            langs=langs,\n","            token_type_ids=token_type_ids,\n","            position_ids=position_ids,\n","            lengths=lengths,\n","            cache=cache,\n","            head_mask=head_mask,\n","            inputs_embeds=inputs_embeds,\n","        )\n","\n","        #output = self.dropout(output)\n","        output = transformer_outputs[0] \n","        vec = output[:,0]\n","        \n","        \n","        #logits\n","        dense = self.dropout(vec)\n","        \n","        #classifier\n","        logits = self.classifier(dense)\n","        \n","        outputs = (logits,) + transformer_outputs[1:]  # Keep new_mems and attention/hidden states if they are here\n","       \n","        \n","        return outputs,dense"],"execution_count":146,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"b2krGO8BnVfd"},"source":["# Dataset Fusion"]},{"cell_type":"code","metadata":{"id":"qIIQ5-g3gU85","executionInfo":{"status":"ok","timestamp":1620397919555,"user_tz":-120,"elapsed":294747,"user":{"displayName":"Sara Fattouh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjGrdvZo2x_8K8abskRubIgMK3SdtS4cG-aQE01nA=s64","userId":"09360594903764573322"}}},"source":["# TODO DELELTE IMAGES WITH NO DESCRIPTION\n","# From the preprocesssed file"],"execution_count":147,"outputs":[]},{"cell_type":"code","metadata":{"id":"RRq24YrsnU9X","executionInfo":{"status":"ok","timestamp":1620397919555,"user_tz":-120,"elapsed":294741,"user":{"displayName":"Sara Fattouh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjGrdvZo2x_8K8abskRubIgMK3SdtS4cG-aQE01nA=s64","userId":"09360594903764573322"}}},"source":["from torch.utils.data import Dataset, DataLoader, Subset\n","from PIL import Image\n","import matplotlib.pyplot as plt\n","import cv2\n","\n","class FusionDataset(Dataset):\n","    \n","    def __init__(self, df, inputs_cam, masks_cam, inputs_flau, masks_flau, transform=None, mode='train'):\n","        self.df = df\n","        self.transform   = transform\n","        self.mode = mode\n","\n","        self.inputs_cam  = inputs_cam\n","        self.masks_cam   = masks_cam\n","\n","        self.inputs_flau  = inputs_flau\n","        self.masks_flau   = masks_flau\n","         \n","    def __len__(self):\n","        return len(self.df)\n","    \n","    def __getitem__(self,idx):\n","        \n","        im_path = self.df.iloc[idx]['image_path']\n","        img= plt.imread(im_path)\n","        #img = cv2.imread(im_path)\n","        #img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n","        img = Image.fromarray(img)\n","\n","        if self.transform is not None:\n","            img = self.transform(img)\n","\n","        img              = img.cuda()\n","        input_id_cam     = self.inputs_cam[idx].cuda()\n","        input_mask_cam   = self.masks_cam[idx].cuda()\n","        input_id_flau    = self.inputs_flau[idx].cuda()\n","        input_mask_flau  = self.masks_flau[idx].cuda()\n","        \n","        if self.mode =='test':\n","            return img,input_id_cam,input_mask_cam,input_id_flau,input_mask_flau\n","            \n","        else:\n","            labels = torch.tensor(self.df.iloc[idx]['labels']).cuda()             \n","            return img,input_id_cam,input_mask_cam,input_id_flau,input_mask_flau,labels"],"execution_count":148,"outputs":[]},{"cell_type":"code","metadata":{"id":"O5RRyCzd4XSd","executionInfo":{"status":"ok","timestamp":1620397919555,"user_tz":-120,"elapsed":294734,"user":{"displayName":"Sara Fattouh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjGrdvZo2x_8K8abskRubIgMK3SdtS4cG-aQE01nA=s64","userId":"09360594903764573322"}}},"source":["a1 = torch.randn(3,10,10)"],"execution_count":149,"outputs":[]},{"cell_type":"code","metadata":{"id":"o_51PvBY4Zpr","executionInfo":{"status":"ok","timestamp":1620397919555,"user_tz":-120,"elapsed":294728,"user":{"displayName":"Sara Fattouh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjGrdvZo2x_8K8abskRubIgMK3SdtS4cG-aQE01nA=s64","userId":"09360594903764573322"}}},"source":["reduce_dim = nn.Conv1d(in_channels = 10 , out_channels = 1 , kernel_size= 1)"],"execution_count":150,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"wryxtip-4b0m","executionInfo":{"status":"ok","timestamp":1620397919556,"user_tz":-120,"elapsed":294722,"user":{"displayName":"Sara Fattouh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjGrdvZo2x_8K8abskRubIgMK3SdtS4cG-aQE01nA=s64","userId":"09360594903764573322"}},"outputId":"78ea0fba-a337-4807-901a-26a6d0d97af5"},"source":["reduce_dim(a1).view(3,10).shape"],"execution_count":151,"outputs":[{"output_type":"execute_result","data":{"text/plain":["torch.Size([3, 10])"]},"metadata":{"tags":[]},"execution_count":151}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"hTUa-CbEnluo","executionInfo":{"status":"ok","timestamp":1620397922299,"user_tz":-120,"elapsed":297455,"user":{"displayName":"Sara Fattouh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjGrdvZo2x_8K8abskRubIgMK3SdtS4cG-aQE01nA=s64","userId":"09360594903764573322"}},"outputId":"6eb57346-7e0c-48e0-f281-5d55c699c7f9"},"source":["print('Using Camembert')\n","tokenizer_cam = CamembertTokenizer.from_pretrained('camembert-base', do_lowercase=False)\n","print('Using Flaubert')\n","\n","tokenizer_flau = FlaubertTokenizer.from_pretrained('flaubert/flaubert_base_cased', do_lowercase=False)\n","\n","# input_ids_test_flau,attention_masks_test_flau = prep_input(test_sentences, labels=None, max_len=max_len,tokenizer = tokenizer_flau)\n","\n","# input_ids_test_cam,attention_masks_test_cam = prep_input(test_sentences , labels=None, max_len=max_len,tokenizer = tokenizer_cam)"],"execution_count":152,"outputs":[{"output_type":"stream","text":["Using Camembert\n","Using Flaubert\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"e8gijJIM5pQX","executionInfo":{"status":"ok","timestamp":1620397922301,"user_tz":-120,"elapsed":297447,"user":{"displayName":"Sara Fattouh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjGrdvZo2x_8K8abskRubIgMK3SdtS4cG-aQE01nA=s64","userId":"09360594903764573322"}}},"source":["# print(type(Preprocess.test_sentences))\n","# print(len(Preprocess.test_sentences))"],"execution_count":153,"outputs":[]},{"cell_type":"code","metadata":{"id":"HXbLhSkyCilV","executionInfo":{"status":"ok","timestamp":1620397922301,"user_tz":-120,"elapsed":297440,"user":{"displayName":"Sara Fattouh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjGrdvZo2x_8K8abskRubIgMK3SdtS4cG-aQE01nA=s64","userId":"09360594903764573322"}}},"source":["# Moodels path \n","resnet_model_path = '/content/Rakuten/models/RESNET_baseline_model.pt' \n","camembert_model_path = '/content/Rakuten/models/CamemBERT_best_model_baseline.pt' \n","flaubert_model_path = '/content/Rakuten/models/FlauBERT_best_model_baseline.pt'\n","\n","#my_flau_path  = '/content/Rakuten/models/FlauBERT_best_model_description.pt'\n"],"execution_count":154,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"09NiYnUxfi94"},"source":["# Fuse\n"," When using pretrained models, PyTorch sets the model to be unfrozen (will have its weights adjusted) by default"]},{"cell_type":"code","metadata":{"id":"TxW8Ups_nr8O","executionInfo":{"status":"ok","timestamp":1620397922302,"user_tz":-120,"elapsed":297433,"user":{"displayName":"Sara Fattouh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjGrdvZo2x_8K8abskRubIgMK3SdtS4cG-aQE01nA=s64","userId":"09360594903764573322"}}},"source":["class vector_fusion(nn.Module):    \n","    def __init__(self):\n","        super(vector_fusion, self).__init__()\n","\n","        self.img_model = SEResnext50_32x4d(pretrained=None)\n","        self.img_model.load_state_dict(torch.load(resnet_model_path))\n","        self.img_model.l0=Identity()\n","        for params in self.img_model.parameters():\n","            params.requires_grad=False\n","\n","        self.cam_model= vec_output_CamembertForSequenceClassification.from_pretrained(\n","         'camembert-base', # Use the 12-layer BERT model, with an uncased vocab.\n","          num_labels = 27, # The number of output labels--2 for binary classification.\n","                    # You can increase this for multi-class tasks.   \n","          output_attentions = False, # Whether the model returns attentions weights.\n","          output_hidden_states = False,) # Whether the model returns all hidden-states.\n","        \n","        \n","        checkpoint = torch.load(camembert_model_path)\n","        self.cam_model.load_state_dict(checkpoint)\n","\n","        \n","\n","        for param in self.cam_model.parameters():\n","            param.requires_grad=False\n","        self.cam_model.out_proj=Identity()\n","        \n","        self.flau_model = vec_output_FlaubertForSequenceClassification.from_pretrained(\n","        'flaubert/flaubert_base_cased', \n","        num_labels = 27, \n","        output_attentions = False,\n","        output_hidden_states = False,)\n","        checkpoint = torch.load(flaubert_model_path)\n","        self.flau_model.load_state_dict(checkpoint)\n","\n","        for param in self.flau_model.parameters():\n","            param.requires_grad=False\n","        self.flau_model.classifier=Identity()\n","        \n","\n","        self.reduce_dim=nn.Conv1d(in_channels = 2048 , out_channels = 768 , kernel_size= 1)\n","        self.reduce_dim2=nn.Conv1d(in_channels = 768 , out_channels = 1 , kernel_size= 1)\n","        self.out=nn.Linear(768, 27)\n","        \n","        #gamma\n","#         self.w1 = nn.Parameter(torch.zeros(1))\n","#         self.w2 = nn.Parameter(torch.zeros(1))\n","#         self.w3 = nn.Parameter(torch.zeros(1))\n","        \n","    def forward(self,img,input_id_cam,input_mask_cam,input_id_flau,input_mask_flau):\n","        \n","        cam_emb,vec1 =self.cam_model(input_id_cam, \n","                     token_type_ids=None, \n","                     attention_mask=input_mask_cam)\n","        \n","        flau_emb,vec2 =self.flau_model(input_id_flau, \n","                     token_type_ids=None, \n","                     attention_mask=input_mask_flau)\n","        \n","        #Projecting the image embedding to lower dimension\n","        img_emb = self.img_model(img)\n","        \n","        img_emb = img_emb.view(img_emb.shape[0],img_emb.shape[1],1)\n","        img_emb = self.reduce_dim(img_emb)\n","        img_emb = img_emb.view(img_emb.shape[0],img_emb.shape[1]) ###### bs * 768 \n","        \n","        #summing up the vectors\n","        #text_emb = cam_emb[0] + flau_emb[0]\n","        \n","        #Bilinear\n","        #text_emb = text_emb.view(text_emb.shape[0],1,text_emb.shape[1])  ##### bs * 1 * 768\n","        \n","        #Bilinear Pooling\n","        #pool_emb = torch.bmm(img_emb,text_emb) ### bs * 768 * 768\n","        #pool_emb = self.reduce_dim2(pool_emb).view(text_emb.shape[0],768)  #### bs * 1 * 768\n","        \n","        fuse = img_emb + cam_emb[0] + flau_emb[0]\n","        fuse = fuse/3\n","        \n","        \n","        logits=self.out(fuse)\n","        \n","        return logits"],"execution_count":155,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"J1VU1vrSQFzD","executionInfo":{"status":"ok","timestamp":1620397924551,"user_tz":-120,"elapsed":299675,"user":{"displayName":"Sara Fattouh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjGrdvZo2x_8K8abskRubIgMK3SdtS4cG-aQE01nA=s64","userId":"09360594903764573322"}},"outputId":"508e9894-fcd0-4459-e3bf-2467670db143"},"source":[" img_model = SEResnext50_32x4d(pretrained=None)\n"," img_model.load_state_dict(torch.load(resnet_model_path))"],"execution_count":156,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<All keys matched successfully>"]},"metadata":{"tags":[]},"execution_count":156}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"JoAwxoKvP1rw","executionInfo":{"status":"ok","timestamp":1620397962190,"user_tz":-120,"elapsed":337304,"user":{"displayName":"Sara Fattouh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjGrdvZo2x_8K8abskRubIgMK3SdtS4cG-aQE01nA=s64","userId":"09360594903764573322"}},"outputId":"cc8cfe86-8c50-4faa-9218-58ac6579979f"},"source":["cam_model= vec_output_CamembertForSequenceClassification.from_pretrained(\n","         'camembert-base', # Use the 12-layer BERT model, with an uncased vocab.\n","          num_labels = 27, # The number of output labels--2 for binary classification.\n","                    # You can increase this for multi-class tasks.   \n","          output_attentions = False, # Whether the model returns attentions weights.\n","          output_hidden_states = False,) # Whether the model returns all hidden-states.\n","        \n","        \n","checkpoint = torch.load(camembert_model_path)\n","cam_model.load_state_dict(checkpoint)"],"execution_count":157,"outputs":[{"output_type":"stream","text":["Some weights of the model checkpoint at camembert-base were not used when initializing vec_output_CamembertForSequenceClassification: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight']\n","- This IS expected if you are initializing vec_output_CamembertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing vec_output_CamembertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of vec_output_CamembertForSequenceClassification were not initialized from the model checkpoint at camembert-base and are newly initialized: ['embeddings.word_embeddings.weight', 'embeddings.position_embeddings.weight', 'embeddings.token_type_embeddings.weight', 'embeddings.LayerNorm.weight', 'embeddings.LayerNorm.bias', 'encoder.layer.0.attention.self.query.weight', 'encoder.layer.0.attention.self.query.bias', 'encoder.layer.0.attention.self.key.weight', 'encoder.layer.0.attention.self.key.bias', 'encoder.layer.0.attention.self.value.weight', 'encoder.layer.0.attention.self.value.bias', 'encoder.layer.0.attention.output.dense.weight', 'encoder.layer.0.attention.output.dense.bias', 'encoder.layer.0.attention.output.LayerNorm.weight', 'encoder.layer.0.attention.output.LayerNorm.bias', 'encoder.layer.0.intermediate.dense.weight', 'encoder.layer.0.intermediate.dense.bias', 'encoder.layer.0.output.dense.weight', 'encoder.layer.0.output.dense.bias', 'encoder.layer.0.output.LayerNorm.weight', 'encoder.layer.0.output.LayerNorm.bias', 'encoder.layer.1.attention.self.query.weight', 'encoder.layer.1.attention.self.query.bias', 'encoder.layer.1.attention.self.key.weight', 'encoder.layer.1.attention.self.key.bias', 'encoder.layer.1.attention.self.value.weight', 'encoder.layer.1.attention.self.value.bias', 'encoder.layer.1.attention.output.dense.weight', 'encoder.layer.1.attention.output.dense.bias', 'encoder.layer.1.attention.output.LayerNorm.weight', 'encoder.layer.1.attention.output.LayerNorm.bias', 'encoder.layer.1.intermediate.dense.weight', 'encoder.layer.1.intermediate.dense.bias', 'encoder.layer.1.output.dense.weight', 'encoder.layer.1.output.dense.bias', 'encoder.layer.1.output.LayerNorm.weight', 'encoder.layer.1.output.LayerNorm.bias', 'encoder.layer.2.attention.self.query.weight', 'encoder.layer.2.attention.self.query.bias', 'encoder.layer.2.attention.self.key.weight', 'encoder.layer.2.attention.self.key.bias', 'encoder.layer.2.attention.self.value.weight', 'encoder.layer.2.attention.self.value.bias', 'encoder.layer.2.attention.output.dense.weight', 'encoder.layer.2.attention.output.dense.bias', 'encoder.layer.2.attention.output.LayerNorm.weight', 'encoder.layer.2.attention.output.LayerNorm.bias', 'encoder.layer.2.intermediate.dense.weight', 'encoder.layer.2.intermediate.dense.bias', 'encoder.layer.2.output.dense.weight', 'encoder.layer.2.output.dense.bias', 'encoder.layer.2.output.LayerNorm.weight', 'encoder.layer.2.output.LayerNorm.bias', 'encoder.layer.3.attention.self.query.weight', 'encoder.layer.3.attention.self.query.bias', 'encoder.layer.3.attention.self.key.weight', 'encoder.layer.3.attention.self.key.bias', 'encoder.layer.3.attention.self.value.weight', 'encoder.layer.3.attention.self.value.bias', 'encoder.layer.3.attention.output.dense.weight', 'encoder.layer.3.attention.output.dense.bias', 'encoder.layer.3.attention.output.LayerNorm.weight', 'encoder.layer.3.attention.output.LayerNorm.bias', 'encoder.layer.3.intermediate.dense.weight', 'encoder.layer.3.intermediate.dense.bias', 'encoder.layer.3.output.dense.weight', 'encoder.layer.3.output.dense.bias', 'encoder.layer.3.output.LayerNorm.weight', 'encoder.layer.3.output.LayerNorm.bias', 'encoder.layer.4.attention.self.query.weight', 'encoder.layer.4.attention.self.query.bias', 'encoder.layer.4.attention.self.key.weight', 'encoder.layer.4.attention.self.key.bias', 'encoder.layer.4.attention.self.value.weight', 'encoder.layer.4.attention.self.value.bias', 'encoder.layer.4.attention.output.dense.weight', 'encoder.layer.4.attention.output.dense.bias', 'encoder.layer.4.attention.output.LayerNorm.weight', 'encoder.layer.4.attention.output.LayerNorm.bias', 'encoder.layer.4.intermediate.dense.weight', 'encoder.layer.4.intermediate.dense.bias', 'encoder.layer.4.output.dense.weight', 'encoder.layer.4.output.dense.bias', 'encoder.layer.4.output.LayerNorm.weight', 'encoder.layer.4.output.LayerNorm.bias', 'encoder.layer.5.attention.self.query.weight', 'encoder.layer.5.attention.self.query.bias', 'encoder.layer.5.attention.self.key.weight', 'encoder.layer.5.attention.self.key.bias', 'encoder.layer.5.attention.self.value.weight', 'encoder.layer.5.attention.self.value.bias', 'encoder.layer.5.attention.output.dense.weight', 'encoder.layer.5.attention.output.dense.bias', 'encoder.layer.5.attention.output.LayerNorm.weight', 'encoder.layer.5.attention.output.LayerNorm.bias', 'encoder.layer.5.intermediate.dense.weight', 'encoder.layer.5.intermediate.dense.bias', 'encoder.layer.5.output.dense.weight', 'encoder.layer.5.output.dense.bias', 'encoder.layer.5.output.LayerNorm.weight', 'encoder.layer.5.output.LayerNorm.bias', 'encoder.layer.6.attention.self.query.weight', 'encoder.layer.6.attention.self.query.bias', 'encoder.layer.6.attention.self.key.weight', 'encoder.layer.6.attention.self.key.bias', 'encoder.layer.6.attention.self.value.weight', 'encoder.layer.6.attention.self.value.bias', 'encoder.layer.6.attention.output.dense.weight', 'encoder.layer.6.attention.output.dense.bias', 'encoder.layer.6.attention.output.LayerNorm.weight', 'encoder.layer.6.attention.output.LayerNorm.bias', 'encoder.layer.6.intermediate.dense.weight', 'encoder.layer.6.intermediate.dense.bias', 'encoder.layer.6.output.dense.weight', 'encoder.layer.6.output.dense.bias', 'encoder.layer.6.output.LayerNorm.weight', 'encoder.layer.6.output.LayerNorm.bias', 'encoder.layer.7.attention.self.query.weight', 'encoder.layer.7.attention.self.query.bias', 'encoder.layer.7.attention.self.key.weight', 'encoder.layer.7.attention.self.key.bias', 'encoder.layer.7.attention.self.value.weight', 'encoder.layer.7.attention.self.value.bias', 'encoder.layer.7.attention.output.dense.weight', 'encoder.layer.7.attention.output.dense.bias', 'encoder.layer.7.attention.output.LayerNorm.weight', 'encoder.layer.7.attention.output.LayerNorm.bias', 'encoder.layer.7.intermediate.dense.weight', 'encoder.layer.7.intermediate.dense.bias', 'encoder.layer.7.output.dense.weight', 'encoder.layer.7.output.dense.bias', 'encoder.layer.7.output.LayerNorm.weight', 'encoder.layer.7.output.LayerNorm.bias', 'encoder.layer.8.attention.self.query.weight', 'encoder.layer.8.attention.self.query.bias', 'encoder.layer.8.attention.self.key.weight', 'encoder.layer.8.attention.self.key.bias', 'encoder.layer.8.attention.self.value.weight', 'encoder.layer.8.attention.self.value.bias', 'encoder.layer.8.attention.output.dense.weight', 'encoder.layer.8.attention.output.dense.bias', 'encoder.layer.8.attention.output.LayerNorm.weight', 'encoder.layer.8.attention.output.LayerNorm.bias', 'encoder.layer.8.intermediate.dense.weight', 'encoder.layer.8.intermediate.dense.bias', 'encoder.layer.8.output.dense.weight', 'encoder.layer.8.output.dense.bias', 'encoder.layer.8.output.LayerNorm.weight', 'encoder.layer.8.output.LayerNorm.bias', 'encoder.layer.9.attention.self.query.weight', 'encoder.layer.9.attention.self.query.bias', 'encoder.layer.9.attention.self.key.weight', 'encoder.layer.9.attention.self.key.bias', 'encoder.layer.9.attention.self.value.weight', 'encoder.layer.9.attention.self.value.bias', 'encoder.layer.9.attention.output.dense.weight', 'encoder.layer.9.attention.output.dense.bias', 'encoder.layer.9.attention.output.LayerNorm.weight', 'encoder.layer.9.attention.output.LayerNorm.bias', 'encoder.layer.9.intermediate.dense.weight', 'encoder.layer.9.intermediate.dense.bias', 'encoder.layer.9.output.dense.weight', 'encoder.layer.9.output.dense.bias', 'encoder.layer.9.output.LayerNorm.weight', 'encoder.layer.9.output.LayerNorm.bias', 'encoder.layer.10.attention.self.query.weight', 'encoder.layer.10.attention.self.query.bias', 'encoder.layer.10.attention.self.key.weight', 'encoder.layer.10.attention.self.key.bias', 'encoder.layer.10.attention.self.value.weight', 'encoder.layer.10.attention.self.value.bias', 'encoder.layer.10.attention.output.dense.weight', 'encoder.layer.10.attention.output.dense.bias', 'encoder.layer.10.attention.output.LayerNorm.weight', 'encoder.layer.10.attention.output.LayerNorm.bias', 'encoder.layer.10.intermediate.dense.weight', 'encoder.layer.10.intermediate.dense.bias', 'encoder.layer.10.output.dense.weight', 'encoder.layer.10.output.dense.bias', 'encoder.layer.10.output.LayerNorm.weight', 'encoder.layer.10.output.LayerNorm.bias', 'encoder.layer.11.attention.self.query.weight', 'encoder.layer.11.attention.self.query.bias', 'encoder.layer.11.attention.self.key.weight', 'encoder.layer.11.attention.self.key.bias', 'encoder.layer.11.attention.self.value.weight', 'encoder.layer.11.attention.self.value.bias', 'encoder.layer.11.attention.output.dense.weight', 'encoder.layer.11.attention.output.dense.bias', 'encoder.layer.11.attention.output.LayerNorm.weight', 'encoder.layer.11.attention.output.LayerNorm.bias', 'encoder.layer.11.intermediate.dense.weight', 'encoder.layer.11.intermediate.dense.bias', 'encoder.layer.11.output.dense.weight', 'encoder.layer.11.output.dense.bias', 'encoder.layer.11.output.LayerNorm.weight', 'encoder.layer.11.output.LayerNorm.bias', 'pooler.dense.weight', 'pooler.dense.bias', 'dense.weight', 'dense.bias', 'out_proj.weight', 'out_proj.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"],"name":"stderr"},{"output_type":"execute_result","data":{"text/plain":["<All keys matched successfully>"]},"metadata":{"tags":[]},"execution_count":157}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"FZfncq4PO_EY","executionInfo":{"status":"ok","timestamp":1620397994842,"user_tz":-120,"elapsed":369943,"user":{"displayName":"Sara Fattouh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjGrdvZo2x_8K8abskRubIgMK3SdtS4cG-aQE01nA=s64","userId":"09360594903764573322"}},"outputId":"0d95a9bb-f8b6-4b1b-96b3-c0441a39dce3"},"source":["flau_model = vec_output_FlaubertForSequenceClassification.from_pretrained(\n","        'flaubert/flaubert_base_cased', \n","        num_labels = 27, \n","        output_attentions = False,\n","        output_hidden_states = False,)\n","\n","\n","checkpoint = torch.load(flaubert_model_path)\n","flau_model.load_state_dict(checkpoint)"],"execution_count":158,"outputs":[{"output_type":"stream","text":["Some weights of the model checkpoint at flaubert/flaubert_base_cased were not used when initializing vec_output_FlaubertForSequenceClassification: ['pred_layer.proj.bias', 'pred_layer.proj.weight']\n","- This IS expected if you are initializing vec_output_FlaubertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing vec_output_FlaubertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of vec_output_FlaubertForSequenceClassification were not initialized from the model checkpoint at flaubert/flaubert_base_cased and are newly initialized: ['position_embeddings.weight', 'embeddings.weight', 'layer_norm_emb.weight', 'layer_norm_emb.bias', 'attentions.0.q_lin.weight', 'attentions.0.q_lin.bias', 'attentions.0.k_lin.weight', 'attentions.0.k_lin.bias', 'attentions.0.v_lin.weight', 'attentions.0.v_lin.bias', 'attentions.0.out_lin.weight', 'attentions.0.out_lin.bias', 'attentions.1.q_lin.weight', 'attentions.1.q_lin.bias', 'attentions.1.k_lin.weight', 'attentions.1.k_lin.bias', 'attentions.1.v_lin.weight', 'attentions.1.v_lin.bias', 'attentions.1.out_lin.weight', 'attentions.1.out_lin.bias', 'attentions.2.q_lin.weight', 'attentions.2.q_lin.bias', 'attentions.2.k_lin.weight', 'attentions.2.k_lin.bias', 'attentions.2.v_lin.weight', 'attentions.2.v_lin.bias', 'attentions.2.out_lin.weight', 'attentions.2.out_lin.bias', 'attentions.3.q_lin.weight', 'attentions.3.q_lin.bias', 'attentions.3.k_lin.weight', 'attentions.3.k_lin.bias', 'attentions.3.v_lin.weight', 'attentions.3.v_lin.bias', 'attentions.3.out_lin.weight', 'attentions.3.out_lin.bias', 'attentions.4.q_lin.weight', 'attentions.4.q_lin.bias', 'attentions.4.k_lin.weight', 'attentions.4.k_lin.bias', 'attentions.4.v_lin.weight', 'attentions.4.v_lin.bias', 'attentions.4.out_lin.weight', 'attentions.4.out_lin.bias', 'attentions.5.q_lin.weight', 'attentions.5.q_lin.bias', 'attentions.5.k_lin.weight', 'attentions.5.k_lin.bias', 'attentions.5.v_lin.weight', 'attentions.5.v_lin.bias', 'attentions.5.out_lin.weight', 'attentions.5.out_lin.bias', 'attentions.6.q_lin.weight', 'attentions.6.q_lin.bias', 'attentions.6.k_lin.weight', 'attentions.6.k_lin.bias', 'attentions.6.v_lin.weight', 'attentions.6.v_lin.bias', 'attentions.6.out_lin.weight', 'attentions.6.out_lin.bias', 'attentions.7.q_lin.weight', 'attentions.7.q_lin.bias', 'attentions.7.k_lin.weight', 'attentions.7.k_lin.bias', 'attentions.7.v_lin.weight', 'attentions.7.v_lin.bias', 'attentions.7.out_lin.weight', 'attentions.7.out_lin.bias', 'attentions.8.q_lin.weight', 'attentions.8.q_lin.bias', 'attentions.8.k_lin.weight', 'attentions.8.k_lin.bias', 'attentions.8.v_lin.weight', 'attentions.8.v_lin.bias', 'attentions.8.out_lin.weight', 'attentions.8.out_lin.bias', 'attentions.9.q_lin.weight', 'attentions.9.q_lin.bias', 'attentions.9.k_lin.weight', 'attentions.9.k_lin.bias', 'attentions.9.v_lin.weight', 'attentions.9.v_lin.bias', 'attentions.9.out_lin.weight', 'attentions.9.out_lin.bias', 'attentions.10.q_lin.weight', 'attentions.10.q_lin.bias', 'attentions.10.k_lin.weight', 'attentions.10.k_lin.bias', 'attentions.10.v_lin.weight', 'attentions.10.v_lin.bias', 'attentions.10.out_lin.weight', 'attentions.10.out_lin.bias', 'attentions.11.q_lin.weight', 'attentions.11.q_lin.bias', 'attentions.11.k_lin.weight', 'attentions.11.k_lin.bias', 'attentions.11.v_lin.weight', 'attentions.11.v_lin.bias', 'attentions.11.out_lin.weight', 'attentions.11.out_lin.bias', 'layer_norm1.0.weight', 'layer_norm1.0.bias', 'layer_norm1.1.weight', 'layer_norm1.1.bias', 'layer_norm1.2.weight', 'layer_norm1.2.bias', 'layer_norm1.3.weight', 'layer_norm1.3.bias', 'layer_norm1.4.weight', 'layer_norm1.4.bias', 'layer_norm1.5.weight', 'layer_norm1.5.bias', 'layer_norm1.6.weight', 'layer_norm1.6.bias', 'layer_norm1.7.weight', 'layer_norm1.7.bias', 'layer_norm1.8.weight', 'layer_norm1.8.bias', 'layer_norm1.9.weight', 'layer_norm1.9.bias', 'layer_norm1.10.weight', 'layer_norm1.10.bias', 'layer_norm1.11.weight', 'layer_norm1.11.bias', 'ffns.0.lin1.weight', 'ffns.0.lin1.bias', 'ffns.0.lin2.weight', 'ffns.0.lin2.bias', 'ffns.1.lin1.weight', 'ffns.1.lin1.bias', 'ffns.1.lin2.weight', 'ffns.1.lin2.bias', 'ffns.2.lin1.weight', 'ffns.2.lin1.bias', 'ffns.2.lin2.weight', 'ffns.2.lin2.bias', 'ffns.3.lin1.weight', 'ffns.3.lin1.bias', 'ffns.3.lin2.weight', 'ffns.3.lin2.bias', 'ffns.4.lin1.weight', 'ffns.4.lin1.bias', 'ffns.4.lin2.weight', 'ffns.4.lin2.bias', 'ffns.5.lin1.weight', 'ffns.5.lin1.bias', 'ffns.5.lin2.weight', 'ffns.5.lin2.bias', 'ffns.6.lin1.weight', 'ffns.6.lin1.bias', 'ffns.6.lin2.weight', 'ffns.6.lin2.bias', 'ffns.7.lin1.weight', 'ffns.7.lin1.bias', 'ffns.7.lin2.weight', 'ffns.7.lin2.bias', 'ffns.8.lin1.weight', 'ffns.8.lin1.bias', 'ffns.8.lin2.weight', 'ffns.8.lin2.bias', 'ffns.9.lin1.weight', 'ffns.9.lin1.bias', 'ffns.9.lin2.weight', 'ffns.9.lin2.bias', 'ffns.10.lin1.weight', 'ffns.10.lin1.bias', 'ffns.10.lin2.weight', 'ffns.10.lin2.bias', 'ffns.11.lin1.weight', 'ffns.11.lin1.bias', 'ffns.11.lin2.weight', 'ffns.11.lin2.bias', 'layer_norm2.0.weight', 'layer_norm2.0.bias', 'layer_norm2.1.weight', 'layer_norm2.1.bias', 'layer_norm2.2.weight', 'layer_norm2.2.bias', 'layer_norm2.3.weight', 'layer_norm2.3.bias', 'layer_norm2.4.weight', 'layer_norm2.4.bias', 'layer_norm2.5.weight', 'layer_norm2.5.bias', 'layer_norm2.6.weight', 'layer_norm2.6.bias', 'layer_norm2.7.weight', 'layer_norm2.7.bias', 'layer_norm2.8.weight', 'layer_norm2.8.bias', 'layer_norm2.9.weight', 'layer_norm2.9.bias', 'layer_norm2.10.weight', 'layer_norm2.10.bias', 'layer_norm2.11.weight', 'layer_norm2.11.bias', 'sequence_summary.summary.weight', 'sequence_summary.summary.bias', 'classifier.weight', 'classifier.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"],"name":"stderr"},{"output_type":"execute_result","data":{"text/plain":["<All keys matched successfully>"]},"metadata":{"tags":[]},"execution_count":158}]},{"cell_type":"markdown","metadata":{"id":"eU5pHNZKNKr3"},"source":["#  Instantiation  & Training of Fusion Model "]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"8Qpj2jQYu0P-","executionInfo":{"status":"ok","timestamp":1620398018663,"user_tz":-120,"elapsed":393752,"user":{"displayName":"Sara Fattouh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjGrdvZo2x_8K8abskRubIgMK3SdtS4cG-aQE01nA=s64","userId":"09360594903764573322"}},"outputId":"a5b0b571-3014-4809-9c43-59c82cbeb0c6"},"source":["model = vector_fusion() "],"execution_count":159,"outputs":[{"output_type":"stream","text":["Some weights of the model checkpoint at camembert-base were not used when initializing vec_output_CamembertForSequenceClassification: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight']\n","- This IS expected if you are initializing vec_output_CamembertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing vec_output_CamembertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of vec_output_CamembertForSequenceClassification were not initialized from the model checkpoint at camembert-base and are newly initialized: ['embeddings.word_embeddings.weight', 'embeddings.position_embeddings.weight', 'embeddings.token_type_embeddings.weight', 'embeddings.LayerNorm.weight', 'embeddings.LayerNorm.bias', 'encoder.layer.0.attention.self.query.weight', 'encoder.layer.0.attention.self.query.bias', 'encoder.layer.0.attention.self.key.weight', 'encoder.layer.0.attention.self.key.bias', 'encoder.layer.0.attention.self.value.weight', 'encoder.layer.0.attention.self.value.bias', 'encoder.layer.0.attention.output.dense.weight', 'encoder.layer.0.attention.output.dense.bias', 'encoder.layer.0.attention.output.LayerNorm.weight', 'encoder.layer.0.attention.output.LayerNorm.bias', 'encoder.layer.0.intermediate.dense.weight', 'encoder.layer.0.intermediate.dense.bias', 'encoder.layer.0.output.dense.weight', 'encoder.layer.0.output.dense.bias', 'encoder.layer.0.output.LayerNorm.weight', 'encoder.layer.0.output.LayerNorm.bias', 'encoder.layer.1.attention.self.query.weight', 'encoder.layer.1.attention.self.query.bias', 'encoder.layer.1.attention.self.key.weight', 'encoder.layer.1.attention.self.key.bias', 'encoder.layer.1.attention.self.value.weight', 'encoder.layer.1.attention.self.value.bias', 'encoder.layer.1.attention.output.dense.weight', 'encoder.layer.1.attention.output.dense.bias', 'encoder.layer.1.attention.output.LayerNorm.weight', 'encoder.layer.1.attention.output.LayerNorm.bias', 'encoder.layer.1.intermediate.dense.weight', 'encoder.layer.1.intermediate.dense.bias', 'encoder.layer.1.output.dense.weight', 'encoder.layer.1.output.dense.bias', 'encoder.layer.1.output.LayerNorm.weight', 'encoder.layer.1.output.LayerNorm.bias', 'encoder.layer.2.attention.self.query.weight', 'encoder.layer.2.attention.self.query.bias', 'encoder.layer.2.attention.self.key.weight', 'encoder.layer.2.attention.self.key.bias', 'encoder.layer.2.attention.self.value.weight', 'encoder.layer.2.attention.self.value.bias', 'encoder.layer.2.attention.output.dense.weight', 'encoder.layer.2.attention.output.dense.bias', 'encoder.layer.2.attention.output.LayerNorm.weight', 'encoder.layer.2.attention.output.LayerNorm.bias', 'encoder.layer.2.intermediate.dense.weight', 'encoder.layer.2.intermediate.dense.bias', 'encoder.layer.2.output.dense.weight', 'encoder.layer.2.output.dense.bias', 'encoder.layer.2.output.LayerNorm.weight', 'encoder.layer.2.output.LayerNorm.bias', 'encoder.layer.3.attention.self.query.weight', 'encoder.layer.3.attention.self.query.bias', 'encoder.layer.3.attention.self.key.weight', 'encoder.layer.3.attention.self.key.bias', 'encoder.layer.3.attention.self.value.weight', 'encoder.layer.3.attention.self.value.bias', 'encoder.layer.3.attention.output.dense.weight', 'encoder.layer.3.attention.output.dense.bias', 'encoder.layer.3.attention.output.LayerNorm.weight', 'encoder.layer.3.attention.output.LayerNorm.bias', 'encoder.layer.3.intermediate.dense.weight', 'encoder.layer.3.intermediate.dense.bias', 'encoder.layer.3.output.dense.weight', 'encoder.layer.3.output.dense.bias', 'encoder.layer.3.output.LayerNorm.weight', 'encoder.layer.3.output.LayerNorm.bias', 'encoder.layer.4.attention.self.query.weight', 'encoder.layer.4.attention.self.query.bias', 'encoder.layer.4.attention.self.key.weight', 'encoder.layer.4.attention.self.key.bias', 'encoder.layer.4.attention.self.value.weight', 'encoder.layer.4.attention.self.value.bias', 'encoder.layer.4.attention.output.dense.weight', 'encoder.layer.4.attention.output.dense.bias', 'encoder.layer.4.attention.output.LayerNorm.weight', 'encoder.layer.4.attention.output.LayerNorm.bias', 'encoder.layer.4.intermediate.dense.weight', 'encoder.layer.4.intermediate.dense.bias', 'encoder.layer.4.output.dense.weight', 'encoder.layer.4.output.dense.bias', 'encoder.layer.4.output.LayerNorm.weight', 'encoder.layer.4.output.LayerNorm.bias', 'encoder.layer.5.attention.self.query.weight', 'encoder.layer.5.attention.self.query.bias', 'encoder.layer.5.attention.self.key.weight', 'encoder.layer.5.attention.self.key.bias', 'encoder.layer.5.attention.self.value.weight', 'encoder.layer.5.attention.self.value.bias', 'encoder.layer.5.attention.output.dense.weight', 'encoder.layer.5.attention.output.dense.bias', 'encoder.layer.5.attention.output.LayerNorm.weight', 'encoder.layer.5.attention.output.LayerNorm.bias', 'encoder.layer.5.intermediate.dense.weight', 'encoder.layer.5.intermediate.dense.bias', 'encoder.layer.5.output.dense.weight', 'encoder.layer.5.output.dense.bias', 'encoder.layer.5.output.LayerNorm.weight', 'encoder.layer.5.output.LayerNorm.bias', 'encoder.layer.6.attention.self.query.weight', 'encoder.layer.6.attention.self.query.bias', 'encoder.layer.6.attention.self.key.weight', 'encoder.layer.6.attention.self.key.bias', 'encoder.layer.6.attention.self.value.weight', 'encoder.layer.6.attention.self.value.bias', 'encoder.layer.6.attention.output.dense.weight', 'encoder.layer.6.attention.output.dense.bias', 'encoder.layer.6.attention.output.LayerNorm.weight', 'encoder.layer.6.attention.output.LayerNorm.bias', 'encoder.layer.6.intermediate.dense.weight', 'encoder.layer.6.intermediate.dense.bias', 'encoder.layer.6.output.dense.weight', 'encoder.layer.6.output.dense.bias', 'encoder.layer.6.output.LayerNorm.weight', 'encoder.layer.6.output.LayerNorm.bias', 'encoder.layer.7.attention.self.query.weight', 'encoder.layer.7.attention.self.query.bias', 'encoder.layer.7.attention.self.key.weight', 'encoder.layer.7.attention.self.key.bias', 'encoder.layer.7.attention.self.value.weight', 'encoder.layer.7.attention.self.value.bias', 'encoder.layer.7.attention.output.dense.weight', 'encoder.layer.7.attention.output.dense.bias', 'encoder.layer.7.attention.output.LayerNorm.weight', 'encoder.layer.7.attention.output.LayerNorm.bias', 'encoder.layer.7.intermediate.dense.weight', 'encoder.layer.7.intermediate.dense.bias', 'encoder.layer.7.output.dense.weight', 'encoder.layer.7.output.dense.bias', 'encoder.layer.7.output.LayerNorm.weight', 'encoder.layer.7.output.LayerNorm.bias', 'encoder.layer.8.attention.self.query.weight', 'encoder.layer.8.attention.self.query.bias', 'encoder.layer.8.attention.self.key.weight', 'encoder.layer.8.attention.self.key.bias', 'encoder.layer.8.attention.self.value.weight', 'encoder.layer.8.attention.self.value.bias', 'encoder.layer.8.attention.output.dense.weight', 'encoder.layer.8.attention.output.dense.bias', 'encoder.layer.8.attention.output.LayerNorm.weight', 'encoder.layer.8.attention.output.LayerNorm.bias', 'encoder.layer.8.intermediate.dense.weight', 'encoder.layer.8.intermediate.dense.bias', 'encoder.layer.8.output.dense.weight', 'encoder.layer.8.output.dense.bias', 'encoder.layer.8.output.LayerNorm.weight', 'encoder.layer.8.output.LayerNorm.bias', 'encoder.layer.9.attention.self.query.weight', 'encoder.layer.9.attention.self.query.bias', 'encoder.layer.9.attention.self.key.weight', 'encoder.layer.9.attention.self.key.bias', 'encoder.layer.9.attention.self.value.weight', 'encoder.layer.9.attention.self.value.bias', 'encoder.layer.9.attention.output.dense.weight', 'encoder.layer.9.attention.output.dense.bias', 'encoder.layer.9.attention.output.LayerNorm.weight', 'encoder.layer.9.attention.output.LayerNorm.bias', 'encoder.layer.9.intermediate.dense.weight', 'encoder.layer.9.intermediate.dense.bias', 'encoder.layer.9.output.dense.weight', 'encoder.layer.9.output.dense.bias', 'encoder.layer.9.output.LayerNorm.weight', 'encoder.layer.9.output.LayerNorm.bias', 'encoder.layer.10.attention.self.query.weight', 'encoder.layer.10.attention.self.query.bias', 'encoder.layer.10.attention.self.key.weight', 'encoder.layer.10.attention.self.key.bias', 'encoder.layer.10.attention.self.value.weight', 'encoder.layer.10.attention.self.value.bias', 'encoder.layer.10.attention.output.dense.weight', 'encoder.layer.10.attention.output.dense.bias', 'encoder.layer.10.attention.output.LayerNorm.weight', 'encoder.layer.10.attention.output.LayerNorm.bias', 'encoder.layer.10.intermediate.dense.weight', 'encoder.layer.10.intermediate.dense.bias', 'encoder.layer.10.output.dense.weight', 'encoder.layer.10.output.dense.bias', 'encoder.layer.10.output.LayerNorm.weight', 'encoder.layer.10.output.LayerNorm.bias', 'encoder.layer.11.attention.self.query.weight', 'encoder.layer.11.attention.self.query.bias', 'encoder.layer.11.attention.self.key.weight', 'encoder.layer.11.attention.self.key.bias', 'encoder.layer.11.attention.self.value.weight', 'encoder.layer.11.attention.self.value.bias', 'encoder.layer.11.attention.output.dense.weight', 'encoder.layer.11.attention.output.dense.bias', 'encoder.layer.11.attention.output.LayerNorm.weight', 'encoder.layer.11.attention.output.LayerNorm.bias', 'encoder.layer.11.intermediate.dense.weight', 'encoder.layer.11.intermediate.dense.bias', 'encoder.layer.11.output.dense.weight', 'encoder.layer.11.output.dense.bias', 'encoder.layer.11.output.LayerNorm.weight', 'encoder.layer.11.output.LayerNorm.bias', 'pooler.dense.weight', 'pooler.dense.bias', 'dense.weight', 'dense.bias', 'out_proj.weight', 'out_proj.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of the model checkpoint at flaubert/flaubert_base_cased were not used when initializing vec_output_FlaubertForSequenceClassification: ['pred_layer.proj.bias', 'pred_layer.proj.weight']\n","- This IS expected if you are initializing vec_output_FlaubertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing vec_output_FlaubertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of vec_output_FlaubertForSequenceClassification were not initialized from the model checkpoint at flaubert/flaubert_base_cased and are newly initialized: ['position_embeddings.weight', 'embeddings.weight', 'layer_norm_emb.weight', 'layer_norm_emb.bias', 'attentions.0.q_lin.weight', 'attentions.0.q_lin.bias', 'attentions.0.k_lin.weight', 'attentions.0.k_lin.bias', 'attentions.0.v_lin.weight', 'attentions.0.v_lin.bias', 'attentions.0.out_lin.weight', 'attentions.0.out_lin.bias', 'attentions.1.q_lin.weight', 'attentions.1.q_lin.bias', 'attentions.1.k_lin.weight', 'attentions.1.k_lin.bias', 'attentions.1.v_lin.weight', 'attentions.1.v_lin.bias', 'attentions.1.out_lin.weight', 'attentions.1.out_lin.bias', 'attentions.2.q_lin.weight', 'attentions.2.q_lin.bias', 'attentions.2.k_lin.weight', 'attentions.2.k_lin.bias', 'attentions.2.v_lin.weight', 'attentions.2.v_lin.bias', 'attentions.2.out_lin.weight', 'attentions.2.out_lin.bias', 'attentions.3.q_lin.weight', 'attentions.3.q_lin.bias', 'attentions.3.k_lin.weight', 'attentions.3.k_lin.bias', 'attentions.3.v_lin.weight', 'attentions.3.v_lin.bias', 'attentions.3.out_lin.weight', 'attentions.3.out_lin.bias', 'attentions.4.q_lin.weight', 'attentions.4.q_lin.bias', 'attentions.4.k_lin.weight', 'attentions.4.k_lin.bias', 'attentions.4.v_lin.weight', 'attentions.4.v_lin.bias', 'attentions.4.out_lin.weight', 'attentions.4.out_lin.bias', 'attentions.5.q_lin.weight', 'attentions.5.q_lin.bias', 'attentions.5.k_lin.weight', 'attentions.5.k_lin.bias', 'attentions.5.v_lin.weight', 'attentions.5.v_lin.bias', 'attentions.5.out_lin.weight', 'attentions.5.out_lin.bias', 'attentions.6.q_lin.weight', 'attentions.6.q_lin.bias', 'attentions.6.k_lin.weight', 'attentions.6.k_lin.bias', 'attentions.6.v_lin.weight', 'attentions.6.v_lin.bias', 'attentions.6.out_lin.weight', 'attentions.6.out_lin.bias', 'attentions.7.q_lin.weight', 'attentions.7.q_lin.bias', 'attentions.7.k_lin.weight', 'attentions.7.k_lin.bias', 'attentions.7.v_lin.weight', 'attentions.7.v_lin.bias', 'attentions.7.out_lin.weight', 'attentions.7.out_lin.bias', 'attentions.8.q_lin.weight', 'attentions.8.q_lin.bias', 'attentions.8.k_lin.weight', 'attentions.8.k_lin.bias', 'attentions.8.v_lin.weight', 'attentions.8.v_lin.bias', 'attentions.8.out_lin.weight', 'attentions.8.out_lin.bias', 'attentions.9.q_lin.weight', 'attentions.9.q_lin.bias', 'attentions.9.k_lin.weight', 'attentions.9.k_lin.bias', 'attentions.9.v_lin.weight', 'attentions.9.v_lin.bias', 'attentions.9.out_lin.weight', 'attentions.9.out_lin.bias', 'attentions.10.q_lin.weight', 'attentions.10.q_lin.bias', 'attentions.10.k_lin.weight', 'attentions.10.k_lin.bias', 'attentions.10.v_lin.weight', 'attentions.10.v_lin.bias', 'attentions.10.out_lin.weight', 'attentions.10.out_lin.bias', 'attentions.11.q_lin.weight', 'attentions.11.q_lin.bias', 'attentions.11.k_lin.weight', 'attentions.11.k_lin.bias', 'attentions.11.v_lin.weight', 'attentions.11.v_lin.bias', 'attentions.11.out_lin.weight', 'attentions.11.out_lin.bias', 'layer_norm1.0.weight', 'layer_norm1.0.bias', 'layer_norm1.1.weight', 'layer_norm1.1.bias', 'layer_norm1.2.weight', 'layer_norm1.2.bias', 'layer_norm1.3.weight', 'layer_norm1.3.bias', 'layer_norm1.4.weight', 'layer_norm1.4.bias', 'layer_norm1.5.weight', 'layer_norm1.5.bias', 'layer_norm1.6.weight', 'layer_norm1.6.bias', 'layer_norm1.7.weight', 'layer_norm1.7.bias', 'layer_norm1.8.weight', 'layer_norm1.8.bias', 'layer_norm1.9.weight', 'layer_norm1.9.bias', 'layer_norm1.10.weight', 'layer_norm1.10.bias', 'layer_norm1.11.weight', 'layer_norm1.11.bias', 'ffns.0.lin1.weight', 'ffns.0.lin1.bias', 'ffns.0.lin2.weight', 'ffns.0.lin2.bias', 'ffns.1.lin1.weight', 'ffns.1.lin1.bias', 'ffns.1.lin2.weight', 'ffns.1.lin2.bias', 'ffns.2.lin1.weight', 'ffns.2.lin1.bias', 'ffns.2.lin2.weight', 'ffns.2.lin2.bias', 'ffns.3.lin1.weight', 'ffns.3.lin1.bias', 'ffns.3.lin2.weight', 'ffns.3.lin2.bias', 'ffns.4.lin1.weight', 'ffns.4.lin1.bias', 'ffns.4.lin2.weight', 'ffns.4.lin2.bias', 'ffns.5.lin1.weight', 'ffns.5.lin1.bias', 'ffns.5.lin2.weight', 'ffns.5.lin2.bias', 'ffns.6.lin1.weight', 'ffns.6.lin1.bias', 'ffns.6.lin2.weight', 'ffns.6.lin2.bias', 'ffns.7.lin1.weight', 'ffns.7.lin1.bias', 'ffns.7.lin2.weight', 'ffns.7.lin2.bias', 'ffns.8.lin1.weight', 'ffns.8.lin1.bias', 'ffns.8.lin2.weight', 'ffns.8.lin2.bias', 'ffns.9.lin1.weight', 'ffns.9.lin1.bias', 'ffns.9.lin2.weight', 'ffns.9.lin2.bias', 'ffns.10.lin1.weight', 'ffns.10.lin1.bias', 'ffns.10.lin2.weight', 'ffns.10.lin2.bias', 'ffns.11.lin1.weight', 'ffns.11.lin1.bias', 'ffns.11.lin2.weight', 'ffns.11.lin2.bias', 'layer_norm2.0.weight', 'layer_norm2.0.bias', 'layer_norm2.1.weight', 'layer_norm2.1.bias', 'layer_norm2.2.weight', 'layer_norm2.2.bias', 'layer_norm2.3.weight', 'layer_norm2.3.bias', 'layer_norm2.4.weight', 'layer_norm2.4.bias', 'layer_norm2.5.weight', 'layer_norm2.5.bias', 'layer_norm2.6.weight', 'layer_norm2.6.bias', 'layer_norm2.7.weight', 'layer_norm2.7.bias', 'layer_norm2.8.weight', 'layer_norm2.8.bias', 'layer_norm2.9.weight', 'layer_norm2.9.bias', 'layer_norm2.10.weight', 'layer_norm2.10.bias', 'layer_norm2.11.weight', 'layer_norm2.11.bias', 'sequence_summary.summary.weight', 'sequence_summary.summary.bias', 'classifier.weight', 'classifier.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"GN9HIANRu1_R","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1620398019164,"user_tz":-120,"elapsed":394241,"user":{"displayName":"Sara Fattouh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjGrdvZo2x_8K8abskRubIgMK3SdtS4cG-aQE01nA=s64","userId":"09360594903764573322"}},"outputId":"529f88a9-8e9c-4566-d1e5-3945655ab2f9"},"source":["model.cuda()"],"execution_count":160,"outputs":[{"output_type":"execute_result","data":{"text/plain":["vector_fusion(\n","  (img_model): SEResnext50_32x4d(\n","    (base_model): SENet(\n","      (layer0): Sequential(\n","        (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n","        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (relu1): ReLU(inplace=True)\n","        (pool): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=True)\n","      )\n","      (layer1): Sequential(\n","        (0): SEResNeXtBottleneck(\n","          (conv1): Conv2d(64, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n","          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (conv3): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu): ReLU(inplace=True)\n","          (se_module): SEModule(\n","            (avg_pool): AdaptiveAvgPool2d(output_size=1)\n","            (fc1): Conv2d(256, 16, kernel_size=(1, 1), stride=(1, 1))\n","            (relu): ReLU(inplace=True)\n","            (fc2): Conv2d(16, 256, kernel_size=(1, 1), stride=(1, 1))\n","            (sigmoid): Sigmoid()\n","          )\n","          (downsample): Sequential(\n","            (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","            (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          )\n","        )\n","        (1): SEResNeXtBottleneck(\n","          (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n","          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (conv3): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu): ReLU(inplace=True)\n","          (se_module): SEModule(\n","            (avg_pool): AdaptiveAvgPool2d(output_size=1)\n","            (fc1): Conv2d(256, 16, kernel_size=(1, 1), stride=(1, 1))\n","            (relu): ReLU(inplace=True)\n","            (fc2): Conv2d(16, 256, kernel_size=(1, 1), stride=(1, 1))\n","            (sigmoid): Sigmoid()\n","          )\n","        )\n","        (2): SEResNeXtBottleneck(\n","          (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n","          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (conv3): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu): ReLU(inplace=True)\n","          (se_module): SEModule(\n","            (avg_pool): AdaptiveAvgPool2d(output_size=1)\n","            (fc1): Conv2d(256, 16, kernel_size=(1, 1), stride=(1, 1))\n","            (relu): ReLU(inplace=True)\n","            (fc2): Conv2d(16, 256, kernel_size=(1, 1), stride=(1, 1))\n","            (sigmoid): Sigmoid()\n","          )\n","        )\n","      )\n","      (layer2): Sequential(\n","        (0): SEResNeXtBottleneck(\n","          (conv1): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=32, bias=False)\n","          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (conv3): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu): ReLU(inplace=True)\n","          (se_module): SEModule(\n","            (avg_pool): AdaptiveAvgPool2d(output_size=1)\n","            (fc1): Conv2d(512, 32, kernel_size=(1, 1), stride=(1, 1))\n","            (relu): ReLU(inplace=True)\n","            (fc2): Conv2d(32, 512, kernel_size=(1, 1), stride=(1, 1))\n","            (sigmoid): Sigmoid()\n","          )\n","          (downsample): Sequential(\n","            (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n","            (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          )\n","        )\n","        (1): SEResNeXtBottleneck(\n","          (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n","          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (conv3): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu): ReLU(inplace=True)\n","          (se_module): SEModule(\n","            (avg_pool): AdaptiveAvgPool2d(output_size=1)\n","            (fc1): Conv2d(512, 32, kernel_size=(1, 1), stride=(1, 1))\n","            (relu): ReLU(inplace=True)\n","            (fc2): Conv2d(32, 512, kernel_size=(1, 1), stride=(1, 1))\n","            (sigmoid): Sigmoid()\n","          )\n","        )\n","        (2): SEResNeXtBottleneck(\n","          (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n","          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (conv3): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu): ReLU(inplace=True)\n","          (se_module): SEModule(\n","            (avg_pool): AdaptiveAvgPool2d(output_size=1)\n","            (fc1): Conv2d(512, 32, kernel_size=(1, 1), stride=(1, 1))\n","            (relu): ReLU(inplace=True)\n","            (fc2): Conv2d(32, 512, kernel_size=(1, 1), stride=(1, 1))\n","            (sigmoid): Sigmoid()\n","          )\n","        )\n","        (3): SEResNeXtBottleneck(\n","          (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n","          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (conv3): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu): ReLU(inplace=True)\n","          (se_module): SEModule(\n","            (avg_pool): AdaptiveAvgPool2d(output_size=1)\n","            (fc1): Conv2d(512, 32, kernel_size=(1, 1), stride=(1, 1))\n","            (relu): ReLU(inplace=True)\n","            (fc2): Conv2d(32, 512, kernel_size=(1, 1), stride=(1, 1))\n","            (sigmoid): Sigmoid()\n","          )\n","        )\n","      )\n","      (layer3): Sequential(\n","        (0): SEResNeXtBottleneck(\n","          (conv1): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=32, bias=False)\n","          (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (conv3): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu): ReLU(inplace=True)\n","          (se_module): SEModule(\n","            (avg_pool): AdaptiveAvgPool2d(output_size=1)\n","            (fc1): Conv2d(1024, 64, kernel_size=(1, 1), stride=(1, 1))\n","            (relu): ReLU(inplace=True)\n","            (fc2): Conv2d(64, 1024, kernel_size=(1, 1), stride=(1, 1))\n","            (sigmoid): Sigmoid()\n","          )\n","          (downsample): Sequential(\n","            (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n","            (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          )\n","        )\n","        (1): SEResNeXtBottleneck(\n","          (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n","          (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (conv3): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu): ReLU(inplace=True)\n","          (se_module): SEModule(\n","            (avg_pool): AdaptiveAvgPool2d(output_size=1)\n","            (fc1): Conv2d(1024, 64, kernel_size=(1, 1), stride=(1, 1))\n","            (relu): ReLU(inplace=True)\n","            (fc2): Conv2d(64, 1024, kernel_size=(1, 1), stride=(1, 1))\n","            (sigmoid): Sigmoid()\n","          )\n","        )\n","        (2): SEResNeXtBottleneck(\n","          (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n","          (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (conv3): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu): ReLU(inplace=True)\n","          (se_module): SEModule(\n","            (avg_pool): AdaptiveAvgPool2d(output_size=1)\n","            (fc1): Conv2d(1024, 64, kernel_size=(1, 1), stride=(1, 1))\n","            (relu): ReLU(inplace=True)\n","            (fc2): Conv2d(64, 1024, kernel_size=(1, 1), stride=(1, 1))\n","            (sigmoid): Sigmoid()\n","          )\n","        )\n","        (3): SEResNeXtBottleneck(\n","          (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n","          (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (conv3): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu): ReLU(inplace=True)\n","          (se_module): SEModule(\n","            (avg_pool): AdaptiveAvgPool2d(output_size=1)\n","            (fc1): Conv2d(1024, 64, kernel_size=(1, 1), stride=(1, 1))\n","            (relu): ReLU(inplace=True)\n","            (fc2): Conv2d(64, 1024, kernel_size=(1, 1), stride=(1, 1))\n","            (sigmoid): Sigmoid()\n","          )\n","        )\n","        (4): SEResNeXtBottleneck(\n","          (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n","          (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (conv3): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu): ReLU(inplace=True)\n","          (se_module): SEModule(\n","            (avg_pool): AdaptiveAvgPool2d(output_size=1)\n","            (fc1): Conv2d(1024, 64, kernel_size=(1, 1), stride=(1, 1))\n","            (relu): ReLU(inplace=True)\n","            (fc2): Conv2d(64, 1024, kernel_size=(1, 1), stride=(1, 1))\n","            (sigmoid): Sigmoid()\n","          )\n","        )\n","        (5): SEResNeXtBottleneck(\n","          (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n","          (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (conv3): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu): ReLU(inplace=True)\n","          (se_module): SEModule(\n","            (avg_pool): AdaptiveAvgPool2d(output_size=1)\n","            (fc1): Conv2d(1024, 64, kernel_size=(1, 1), stride=(1, 1))\n","            (relu): ReLU(inplace=True)\n","            (fc2): Conv2d(64, 1024, kernel_size=(1, 1), stride=(1, 1))\n","            (sigmoid): Sigmoid()\n","          )\n","        )\n","      )\n","      (layer4): Sequential(\n","        (0): SEResNeXtBottleneck(\n","          (conv1): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (conv2): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=32, bias=False)\n","          (bn2): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (conv3): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu): ReLU(inplace=True)\n","          (se_module): SEModule(\n","            (avg_pool): AdaptiveAvgPool2d(output_size=1)\n","            (fc1): Conv2d(2048, 128, kernel_size=(1, 1), stride=(1, 1))\n","            (relu): ReLU(inplace=True)\n","            (fc2): Conv2d(128, 2048, kernel_size=(1, 1), stride=(1, 1))\n","            (sigmoid): Sigmoid()\n","          )\n","          (downsample): Sequential(\n","            (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n","            (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          )\n","        )\n","        (1): SEResNeXtBottleneck(\n","          (conv1): Conv2d(2048, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (conv2): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n","          (bn2): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (conv3): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu): ReLU(inplace=True)\n","          (se_module): SEModule(\n","            (avg_pool): AdaptiveAvgPool2d(output_size=1)\n","            (fc1): Conv2d(2048, 128, kernel_size=(1, 1), stride=(1, 1))\n","            (relu): ReLU(inplace=True)\n","            (fc2): Conv2d(128, 2048, kernel_size=(1, 1), stride=(1, 1))\n","            (sigmoid): Sigmoid()\n","          )\n","        )\n","        (2): SEResNeXtBottleneck(\n","          (conv1): Conv2d(2048, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (conv2): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n","          (bn2): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (conv3): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu): ReLU(inplace=True)\n","          (se_module): SEModule(\n","            (avg_pool): AdaptiveAvgPool2d(output_size=1)\n","            (fc1): Conv2d(2048, 128, kernel_size=(1, 1), stride=(1, 1))\n","            (relu): ReLU(inplace=True)\n","            (fc2): Conv2d(128, 2048, kernel_size=(1, 1), stride=(1, 1))\n","            (sigmoid): Sigmoid()\n","          )\n","        )\n","      )\n","      (avg_pool): AvgPool2d(kernel_size=7, stride=1, padding=0)\n","      (last_linear): Linear(in_features=2048, out_features=1000, bias=True)\n","    )\n","    (l0): Identity()\n","  )\n","  (cam_model): vec_output_CamembertForSequenceClassification(\n","    (embeddings): RobertaEmbeddings(\n","      (word_embeddings): Embedding(32005, 768, padding_idx=1)\n","      (position_embeddings): Embedding(514, 768, padding_idx=1)\n","      (token_type_embeddings): Embedding(1, 768)\n","      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","      (dropout): Dropout(p=0.1, inplace=False)\n","    )\n","    (encoder): RobertaEncoder(\n","      (layer): ModuleList(\n","        (0): RobertaLayer(\n","          (attention): RobertaAttention(\n","            (self): RobertaSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): RobertaSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): RobertaIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): RobertaOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (1): RobertaLayer(\n","          (attention): RobertaAttention(\n","            (self): RobertaSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): RobertaSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): RobertaIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): RobertaOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (2): RobertaLayer(\n","          (attention): RobertaAttention(\n","            (self): RobertaSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): RobertaSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): RobertaIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): RobertaOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (3): RobertaLayer(\n","          (attention): RobertaAttention(\n","            (self): RobertaSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): RobertaSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): RobertaIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): RobertaOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (4): RobertaLayer(\n","          (attention): RobertaAttention(\n","            (self): RobertaSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): RobertaSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): RobertaIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): RobertaOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (5): RobertaLayer(\n","          (attention): RobertaAttention(\n","            (self): RobertaSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): RobertaSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): RobertaIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): RobertaOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (6): RobertaLayer(\n","          (attention): RobertaAttention(\n","            (self): RobertaSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): RobertaSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): RobertaIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): RobertaOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (7): RobertaLayer(\n","          (attention): RobertaAttention(\n","            (self): RobertaSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): RobertaSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): RobertaIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): RobertaOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (8): RobertaLayer(\n","          (attention): RobertaAttention(\n","            (self): RobertaSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): RobertaSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): RobertaIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): RobertaOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (9): RobertaLayer(\n","          (attention): RobertaAttention(\n","            (self): RobertaSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): RobertaSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): RobertaIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): RobertaOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (10): RobertaLayer(\n","          (attention): RobertaAttention(\n","            (self): RobertaSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): RobertaSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): RobertaIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): RobertaOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (11): RobertaLayer(\n","          (attention): RobertaAttention(\n","            (self): RobertaSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): RobertaSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): RobertaIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): RobertaOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","    )\n","    (pooler): RobertaPooler(\n","      (dense): Linear(in_features=768, out_features=768, bias=True)\n","      (activation): Tanh()\n","    )\n","    (roberta): CamembertModel(\n","      (embeddings): RobertaEmbeddings(\n","        (word_embeddings): Embedding(32005, 768, padding_idx=1)\n","        (position_embeddings): Embedding(514, 768, padding_idx=1)\n","        (token_type_embeddings): Embedding(1, 768)\n","        (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","        (dropout): Dropout(p=0.1, inplace=False)\n","      )\n","      (encoder): RobertaEncoder(\n","        (layer): ModuleList(\n","          (0): RobertaLayer(\n","            (attention): RobertaAttention(\n","              (self): RobertaSelfAttention(\n","                (query): Linear(in_features=768, out_features=768, bias=True)\n","                (key): Linear(in_features=768, out_features=768, bias=True)\n","                (value): Linear(in_features=768, out_features=768, bias=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","              (output): RobertaSelfOutput(\n","                (dense): Linear(in_features=768, out_features=768, bias=True)\n","                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","            )\n","            (intermediate): RobertaIntermediate(\n","              (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            )\n","            (output): RobertaOutput(\n","              (dense): Linear(in_features=3072, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (1): RobertaLayer(\n","            (attention): RobertaAttention(\n","              (self): RobertaSelfAttention(\n","                (query): Linear(in_features=768, out_features=768, bias=True)\n","                (key): Linear(in_features=768, out_features=768, bias=True)\n","                (value): Linear(in_features=768, out_features=768, bias=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","              (output): RobertaSelfOutput(\n","                (dense): Linear(in_features=768, out_features=768, bias=True)\n","                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","            )\n","            (intermediate): RobertaIntermediate(\n","              (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            )\n","            (output): RobertaOutput(\n","              (dense): Linear(in_features=3072, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (2): RobertaLayer(\n","            (attention): RobertaAttention(\n","              (self): RobertaSelfAttention(\n","                (query): Linear(in_features=768, out_features=768, bias=True)\n","                (key): Linear(in_features=768, out_features=768, bias=True)\n","                (value): Linear(in_features=768, out_features=768, bias=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","              (output): RobertaSelfOutput(\n","                (dense): Linear(in_features=768, out_features=768, bias=True)\n","                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","            )\n","            (intermediate): RobertaIntermediate(\n","              (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            )\n","            (output): RobertaOutput(\n","              (dense): Linear(in_features=3072, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (3): RobertaLayer(\n","            (attention): RobertaAttention(\n","              (self): RobertaSelfAttention(\n","                (query): Linear(in_features=768, out_features=768, bias=True)\n","                (key): Linear(in_features=768, out_features=768, bias=True)\n","                (value): Linear(in_features=768, out_features=768, bias=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","              (output): RobertaSelfOutput(\n","                (dense): Linear(in_features=768, out_features=768, bias=True)\n","                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","            )\n","            (intermediate): RobertaIntermediate(\n","              (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            )\n","            (output): RobertaOutput(\n","              (dense): Linear(in_features=3072, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (4): RobertaLayer(\n","            (attention): RobertaAttention(\n","              (self): RobertaSelfAttention(\n","                (query): Linear(in_features=768, out_features=768, bias=True)\n","                (key): Linear(in_features=768, out_features=768, bias=True)\n","                (value): Linear(in_features=768, out_features=768, bias=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","              (output): RobertaSelfOutput(\n","                (dense): Linear(in_features=768, out_features=768, bias=True)\n","                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","            )\n","            (intermediate): RobertaIntermediate(\n","              (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            )\n","            (output): RobertaOutput(\n","              (dense): Linear(in_features=3072, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (5): RobertaLayer(\n","            (attention): RobertaAttention(\n","              (self): RobertaSelfAttention(\n","                (query): Linear(in_features=768, out_features=768, bias=True)\n","                (key): Linear(in_features=768, out_features=768, bias=True)\n","                (value): Linear(in_features=768, out_features=768, bias=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","              (output): RobertaSelfOutput(\n","                (dense): Linear(in_features=768, out_features=768, bias=True)\n","                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","            )\n","            (intermediate): RobertaIntermediate(\n","              (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            )\n","            (output): RobertaOutput(\n","              (dense): Linear(in_features=3072, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (6): RobertaLayer(\n","            (attention): RobertaAttention(\n","              (self): RobertaSelfAttention(\n","                (query): Linear(in_features=768, out_features=768, bias=True)\n","                (key): Linear(in_features=768, out_features=768, bias=True)\n","                (value): Linear(in_features=768, out_features=768, bias=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","              (output): RobertaSelfOutput(\n","                (dense): Linear(in_features=768, out_features=768, bias=True)\n","                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","            )\n","            (intermediate): RobertaIntermediate(\n","              (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            )\n","            (output): RobertaOutput(\n","              (dense): Linear(in_features=3072, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (7): RobertaLayer(\n","            (attention): RobertaAttention(\n","              (self): RobertaSelfAttention(\n","                (query): Linear(in_features=768, out_features=768, bias=True)\n","                (key): Linear(in_features=768, out_features=768, bias=True)\n","                (value): Linear(in_features=768, out_features=768, bias=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","              (output): RobertaSelfOutput(\n","                (dense): Linear(in_features=768, out_features=768, bias=True)\n","                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","            )\n","            (intermediate): RobertaIntermediate(\n","              (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            )\n","            (output): RobertaOutput(\n","              (dense): Linear(in_features=3072, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (8): RobertaLayer(\n","            (attention): RobertaAttention(\n","              (self): RobertaSelfAttention(\n","                (query): Linear(in_features=768, out_features=768, bias=True)\n","                (key): Linear(in_features=768, out_features=768, bias=True)\n","                (value): Linear(in_features=768, out_features=768, bias=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","              (output): RobertaSelfOutput(\n","                (dense): Linear(in_features=768, out_features=768, bias=True)\n","                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","            )\n","            (intermediate): RobertaIntermediate(\n","              (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            )\n","            (output): RobertaOutput(\n","              (dense): Linear(in_features=3072, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (9): RobertaLayer(\n","            (attention): RobertaAttention(\n","              (self): RobertaSelfAttention(\n","                (query): Linear(in_features=768, out_features=768, bias=True)\n","                (key): Linear(in_features=768, out_features=768, bias=True)\n","                (value): Linear(in_features=768, out_features=768, bias=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","              (output): RobertaSelfOutput(\n","                (dense): Linear(in_features=768, out_features=768, bias=True)\n","                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","            )\n","            (intermediate): RobertaIntermediate(\n","              (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            )\n","            (output): RobertaOutput(\n","              (dense): Linear(in_features=3072, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (10): RobertaLayer(\n","            (attention): RobertaAttention(\n","              (self): RobertaSelfAttention(\n","                (query): Linear(in_features=768, out_features=768, bias=True)\n","                (key): Linear(in_features=768, out_features=768, bias=True)\n","                (value): Linear(in_features=768, out_features=768, bias=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","              (output): RobertaSelfOutput(\n","                (dense): Linear(in_features=768, out_features=768, bias=True)\n","                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","            )\n","            (intermediate): RobertaIntermediate(\n","              (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            )\n","            (output): RobertaOutput(\n","              (dense): Linear(in_features=3072, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (11): RobertaLayer(\n","            (attention): RobertaAttention(\n","              (self): RobertaSelfAttention(\n","                (query): Linear(in_features=768, out_features=768, bias=True)\n","                (key): Linear(in_features=768, out_features=768, bias=True)\n","                (value): Linear(in_features=768, out_features=768, bias=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","              (output): RobertaSelfOutput(\n","                (dense): Linear(in_features=768, out_features=768, bias=True)\n","                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","            )\n","            (intermediate): RobertaIntermediate(\n","              (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            )\n","            (output): RobertaOutput(\n","              (dense): Linear(in_features=3072, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","        )\n","      )\n","      (pooler): RobertaPooler(\n","        (dense): Linear(in_features=768, out_features=768, bias=True)\n","        (activation): Tanh()\n","      )\n","    )\n","    (dense): Linear(in_features=196608, out_features=768, bias=True)\n","    (dropout): Dropout(p=0.1, inplace=False)\n","    (out_proj): Identity()\n","  )\n","  (flau_model): vec_output_FlaubertForSequenceClassification(\n","    (position_embeddings): Embedding(512, 768)\n","    (embeddings): Embedding(68729, 768, padding_idx=2)\n","    (layer_norm_emb): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","    (attentions): ModuleList(\n","      (0): MultiHeadAttention(\n","        (q_lin): Linear(in_features=768, out_features=768, bias=True)\n","        (k_lin): Linear(in_features=768, out_features=768, bias=True)\n","        (v_lin): Linear(in_features=768, out_features=768, bias=True)\n","        (out_lin): Linear(in_features=768, out_features=768, bias=True)\n","      )\n","      (1): MultiHeadAttention(\n","        (q_lin): Linear(in_features=768, out_features=768, bias=True)\n","        (k_lin): Linear(in_features=768, out_features=768, bias=True)\n","        (v_lin): Linear(in_features=768, out_features=768, bias=True)\n","        (out_lin): Linear(in_features=768, out_features=768, bias=True)\n","      )\n","      (2): MultiHeadAttention(\n","        (q_lin): Linear(in_features=768, out_features=768, bias=True)\n","        (k_lin): Linear(in_features=768, out_features=768, bias=True)\n","        (v_lin): Linear(in_features=768, out_features=768, bias=True)\n","        (out_lin): Linear(in_features=768, out_features=768, bias=True)\n","      )\n","      (3): MultiHeadAttention(\n","        (q_lin): Linear(in_features=768, out_features=768, bias=True)\n","        (k_lin): Linear(in_features=768, out_features=768, bias=True)\n","        (v_lin): Linear(in_features=768, out_features=768, bias=True)\n","        (out_lin): Linear(in_features=768, out_features=768, bias=True)\n","      )\n","      (4): MultiHeadAttention(\n","        (q_lin): Linear(in_features=768, out_features=768, bias=True)\n","        (k_lin): Linear(in_features=768, out_features=768, bias=True)\n","        (v_lin): Linear(in_features=768, out_features=768, bias=True)\n","        (out_lin): Linear(in_features=768, out_features=768, bias=True)\n","      )\n","      (5): MultiHeadAttention(\n","        (q_lin): Linear(in_features=768, out_features=768, bias=True)\n","        (k_lin): Linear(in_features=768, out_features=768, bias=True)\n","        (v_lin): Linear(in_features=768, out_features=768, bias=True)\n","        (out_lin): Linear(in_features=768, out_features=768, bias=True)\n","      )\n","      (6): MultiHeadAttention(\n","        (q_lin): Linear(in_features=768, out_features=768, bias=True)\n","        (k_lin): Linear(in_features=768, out_features=768, bias=True)\n","        (v_lin): Linear(in_features=768, out_features=768, bias=True)\n","        (out_lin): Linear(in_features=768, out_features=768, bias=True)\n","      )\n","      (7): MultiHeadAttention(\n","        (q_lin): Linear(in_features=768, out_features=768, bias=True)\n","        (k_lin): Linear(in_features=768, out_features=768, bias=True)\n","        (v_lin): Linear(in_features=768, out_features=768, bias=True)\n","        (out_lin): Linear(in_features=768, out_features=768, bias=True)\n","      )\n","      (8): MultiHeadAttention(\n","        (q_lin): Linear(in_features=768, out_features=768, bias=True)\n","        (k_lin): Linear(in_features=768, out_features=768, bias=True)\n","        (v_lin): Linear(in_features=768, out_features=768, bias=True)\n","        (out_lin): Linear(in_features=768, out_features=768, bias=True)\n","      )\n","      (9): MultiHeadAttention(\n","        (q_lin): Linear(in_features=768, out_features=768, bias=True)\n","        (k_lin): Linear(in_features=768, out_features=768, bias=True)\n","        (v_lin): Linear(in_features=768, out_features=768, bias=True)\n","        (out_lin): Linear(in_features=768, out_features=768, bias=True)\n","      )\n","      (10): MultiHeadAttention(\n","        (q_lin): Linear(in_features=768, out_features=768, bias=True)\n","        (k_lin): Linear(in_features=768, out_features=768, bias=True)\n","        (v_lin): Linear(in_features=768, out_features=768, bias=True)\n","        (out_lin): Linear(in_features=768, out_features=768, bias=True)\n","      )\n","      (11): MultiHeadAttention(\n","        (q_lin): Linear(in_features=768, out_features=768, bias=True)\n","        (k_lin): Linear(in_features=768, out_features=768, bias=True)\n","        (v_lin): Linear(in_features=768, out_features=768, bias=True)\n","        (out_lin): Linear(in_features=768, out_features=768, bias=True)\n","      )\n","    )\n","    (layer_norm1): ModuleList(\n","      (0): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","      (1): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","      (2): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","      (3): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","      (4): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","      (5): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","      (6): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","      (7): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","      (8): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","      (9): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","      (10): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","      (11): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","    )\n","    (ffns): ModuleList(\n","      (0): TransformerFFN(\n","        (lin1): Linear(in_features=768, out_features=3072, bias=True)\n","        (lin2): Linear(in_features=3072, out_features=768, bias=True)\n","      )\n","      (1): TransformerFFN(\n","        (lin1): Linear(in_features=768, out_features=3072, bias=True)\n","        (lin2): Linear(in_features=3072, out_features=768, bias=True)\n","      )\n","      (2): TransformerFFN(\n","        (lin1): Linear(in_features=768, out_features=3072, bias=True)\n","        (lin2): Linear(in_features=3072, out_features=768, bias=True)\n","      )\n","      (3): TransformerFFN(\n","        (lin1): Linear(in_features=768, out_features=3072, bias=True)\n","        (lin2): Linear(in_features=3072, out_features=768, bias=True)\n","      )\n","      (4): TransformerFFN(\n","        (lin1): Linear(in_features=768, out_features=3072, bias=True)\n","        (lin2): Linear(in_features=3072, out_features=768, bias=True)\n","      )\n","      (5): TransformerFFN(\n","        (lin1): Linear(in_features=768, out_features=3072, bias=True)\n","        (lin2): Linear(in_features=3072, out_features=768, bias=True)\n","      )\n","      (6): TransformerFFN(\n","        (lin1): Linear(in_features=768, out_features=3072, bias=True)\n","        (lin2): Linear(in_features=3072, out_features=768, bias=True)\n","      )\n","      (7): TransformerFFN(\n","        (lin1): Linear(in_features=768, out_features=3072, bias=True)\n","        (lin2): Linear(in_features=3072, out_features=768, bias=True)\n","      )\n","      (8): TransformerFFN(\n","        (lin1): Linear(in_features=768, out_features=3072, bias=True)\n","        (lin2): Linear(in_features=3072, out_features=768, bias=True)\n","      )\n","      (9): TransformerFFN(\n","        (lin1): Linear(in_features=768, out_features=3072, bias=True)\n","        (lin2): Linear(in_features=3072, out_features=768, bias=True)\n","      )\n","      (10): TransformerFFN(\n","        (lin1): Linear(in_features=768, out_features=3072, bias=True)\n","        (lin2): Linear(in_features=3072, out_features=768, bias=True)\n","      )\n","      (11): TransformerFFN(\n","        (lin1): Linear(in_features=768, out_features=3072, bias=True)\n","        (lin2): Linear(in_features=3072, out_features=768, bias=True)\n","      )\n","    )\n","    (layer_norm2): ModuleList(\n","      (0): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","      (1): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","      (2): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","      (3): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","      (4): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","      (5): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","      (6): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","      (7): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","      (8): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","      (9): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","      (10): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","      (11): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","    )\n","    (transformer): FlaubertModel(\n","      (position_embeddings): Embedding(512, 768)\n","      (embeddings): Embedding(68729, 768, padding_idx=2)\n","      (layer_norm_emb): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","      (attentions): ModuleList(\n","        (0): MultiHeadAttention(\n","          (q_lin): Linear(in_features=768, out_features=768, bias=True)\n","          (k_lin): Linear(in_features=768, out_features=768, bias=True)\n","          (v_lin): Linear(in_features=768, out_features=768, bias=True)\n","          (out_lin): Linear(in_features=768, out_features=768, bias=True)\n","        )\n","        (1): MultiHeadAttention(\n","          (q_lin): Linear(in_features=768, out_features=768, bias=True)\n","          (k_lin): Linear(in_features=768, out_features=768, bias=True)\n","          (v_lin): Linear(in_features=768, out_features=768, bias=True)\n","          (out_lin): Linear(in_features=768, out_features=768, bias=True)\n","        )\n","        (2): MultiHeadAttention(\n","          (q_lin): Linear(in_features=768, out_features=768, bias=True)\n","          (k_lin): Linear(in_features=768, out_features=768, bias=True)\n","          (v_lin): Linear(in_features=768, out_features=768, bias=True)\n","          (out_lin): Linear(in_features=768, out_features=768, bias=True)\n","        )\n","        (3): MultiHeadAttention(\n","          (q_lin): Linear(in_features=768, out_features=768, bias=True)\n","          (k_lin): Linear(in_features=768, out_features=768, bias=True)\n","          (v_lin): Linear(in_features=768, out_features=768, bias=True)\n","          (out_lin): Linear(in_features=768, out_features=768, bias=True)\n","        )\n","        (4): MultiHeadAttention(\n","          (q_lin): Linear(in_features=768, out_features=768, bias=True)\n","          (k_lin): Linear(in_features=768, out_features=768, bias=True)\n","          (v_lin): Linear(in_features=768, out_features=768, bias=True)\n","          (out_lin): Linear(in_features=768, out_features=768, bias=True)\n","        )\n","        (5): MultiHeadAttention(\n","          (q_lin): Linear(in_features=768, out_features=768, bias=True)\n","          (k_lin): Linear(in_features=768, out_features=768, bias=True)\n","          (v_lin): Linear(in_features=768, out_features=768, bias=True)\n","          (out_lin): Linear(in_features=768, out_features=768, bias=True)\n","        )\n","        (6): MultiHeadAttention(\n","          (q_lin): Linear(in_features=768, out_features=768, bias=True)\n","          (k_lin): Linear(in_features=768, out_features=768, bias=True)\n","          (v_lin): Linear(in_features=768, out_features=768, bias=True)\n","          (out_lin): Linear(in_features=768, out_features=768, bias=True)\n","        )\n","        (7): MultiHeadAttention(\n","          (q_lin): Linear(in_features=768, out_features=768, bias=True)\n","          (k_lin): Linear(in_features=768, out_features=768, bias=True)\n","          (v_lin): Linear(in_features=768, out_features=768, bias=True)\n","          (out_lin): Linear(in_features=768, out_features=768, bias=True)\n","        )\n","        (8): MultiHeadAttention(\n","          (q_lin): Linear(in_features=768, out_features=768, bias=True)\n","          (k_lin): Linear(in_features=768, out_features=768, bias=True)\n","          (v_lin): Linear(in_features=768, out_features=768, bias=True)\n","          (out_lin): Linear(in_features=768, out_features=768, bias=True)\n","        )\n","        (9): MultiHeadAttention(\n","          (q_lin): Linear(in_features=768, out_features=768, bias=True)\n","          (k_lin): Linear(in_features=768, out_features=768, bias=True)\n","          (v_lin): Linear(in_features=768, out_features=768, bias=True)\n","          (out_lin): Linear(in_features=768, out_features=768, bias=True)\n","        )\n","        (10): MultiHeadAttention(\n","          (q_lin): Linear(in_features=768, out_features=768, bias=True)\n","          (k_lin): Linear(in_features=768, out_features=768, bias=True)\n","          (v_lin): Linear(in_features=768, out_features=768, bias=True)\n","          (out_lin): Linear(in_features=768, out_features=768, bias=True)\n","        )\n","        (11): MultiHeadAttention(\n","          (q_lin): Linear(in_features=768, out_features=768, bias=True)\n","          (k_lin): Linear(in_features=768, out_features=768, bias=True)\n","          (v_lin): Linear(in_features=768, out_features=768, bias=True)\n","          (out_lin): Linear(in_features=768, out_features=768, bias=True)\n","        )\n","      )\n","      (layer_norm1): ModuleList(\n","        (0): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","        (1): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","        (2): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","        (3): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","        (4): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","        (5): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","        (6): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","        (7): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","        (8): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","        (9): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","        (10): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","        (11): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","      )\n","      (ffns): ModuleList(\n","        (0): TransformerFFN(\n","          (lin1): Linear(in_features=768, out_features=3072, bias=True)\n","          (lin2): Linear(in_features=3072, out_features=768, bias=True)\n","        )\n","        (1): TransformerFFN(\n","          (lin1): Linear(in_features=768, out_features=3072, bias=True)\n","          (lin2): Linear(in_features=3072, out_features=768, bias=True)\n","        )\n","        (2): TransformerFFN(\n","          (lin1): Linear(in_features=768, out_features=3072, bias=True)\n","          (lin2): Linear(in_features=3072, out_features=768, bias=True)\n","        )\n","        (3): TransformerFFN(\n","          (lin1): Linear(in_features=768, out_features=3072, bias=True)\n","          (lin2): Linear(in_features=3072, out_features=768, bias=True)\n","        )\n","        (4): TransformerFFN(\n","          (lin1): Linear(in_features=768, out_features=3072, bias=True)\n","          (lin2): Linear(in_features=3072, out_features=768, bias=True)\n","        )\n","        (5): TransformerFFN(\n","          (lin1): Linear(in_features=768, out_features=3072, bias=True)\n","          (lin2): Linear(in_features=3072, out_features=768, bias=True)\n","        )\n","        (6): TransformerFFN(\n","          (lin1): Linear(in_features=768, out_features=3072, bias=True)\n","          (lin2): Linear(in_features=3072, out_features=768, bias=True)\n","        )\n","        (7): TransformerFFN(\n","          (lin1): Linear(in_features=768, out_features=3072, bias=True)\n","          (lin2): Linear(in_features=3072, out_features=768, bias=True)\n","        )\n","        (8): TransformerFFN(\n","          (lin1): Linear(in_features=768, out_features=3072, bias=True)\n","          (lin2): Linear(in_features=3072, out_features=768, bias=True)\n","        )\n","        (9): TransformerFFN(\n","          (lin1): Linear(in_features=768, out_features=3072, bias=True)\n","          (lin2): Linear(in_features=3072, out_features=768, bias=True)\n","        )\n","        (10): TransformerFFN(\n","          (lin1): Linear(in_features=768, out_features=3072, bias=True)\n","          (lin2): Linear(in_features=3072, out_features=768, bias=True)\n","        )\n","        (11): TransformerFFN(\n","          (lin1): Linear(in_features=768, out_features=3072, bias=True)\n","          (lin2): Linear(in_features=3072, out_features=768, bias=True)\n","        )\n","      )\n","      (layer_norm2): ModuleList(\n","        (0): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","        (1): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","        (2): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","        (3): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","        (4): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","        (5): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","        (6): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","        (7): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","        (8): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","        (9): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","        (10): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","        (11): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","      )\n","    )\n","    (sequence_summary): SequenceSummary(\n","      (summary): Linear(in_features=768, out_features=27, bias=True)\n","      (activation): Identity()\n","      (first_dropout): Dropout(p=0.1, inplace=False)\n","      (last_dropout): Identity()\n","    )\n","    (dropout): Dropout(p=0.1, inplace=False)\n","    (classifier): Identity()\n","  )\n","  (reduce_dim): Conv1d(2048, 768, kernel_size=(1,), stride=(1,))\n","  (reduce_dim2): Conv1d(768, 1, kernel_size=(1,), stride=(1,))\n","  (out): Linear(in_features=768, out_features=27, bias=True)\n",")"]},"metadata":{"tags":[]},"execution_count":160}]},{"cell_type":"markdown","metadata":{"id":"wbah-djKPyRB"},"source":["# Fuse Input Data"]},{"cell_type":"code","metadata":{"id":"ZZBQFQnFSn6L","executionInfo":{"status":"ok","timestamp":1620398019166,"user_tz":-120,"elapsed":394233,"user":{"displayName":"Sara Fattouh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjGrdvZo2x_8K8abskRubIgMK3SdtS4cG-aQE01nA=s64","userId":"09360594903764573322"}}},"source":["train_dataset = FusionDataset (train_df, tr_inputs_cam, tr_masks_cam, tr_inputs_flau, tr_masks_flau,\n","                            transform = image_transforms['train'])\n","\n","val_dataset = FusionDataset (val_df, val_inputs_cam, val_masks_cam, val_inputs_flau, val_masks_flau,\n","                          transform = image_transforms['valid'])\n","\n","test_dataset = FusionDataset (test_df, input_ids_test_cam, attention_masks_test_cam, input_ids_test_flau, attention_masks_test_flau\n","                           , transform = image_transforms['test'])"],"execution_count":161,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"yPY9mTkrs2Jl","executionInfo":{"status":"ok","timestamp":1620398019167,"user_tz":-120,"elapsed":394227,"user":{"displayName":"Sara Fattouh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjGrdvZo2x_8K8abskRubIgMK3SdtS4cG-aQE01nA=s64","userId":"09360594903764573322"}},"outputId":"0ffcc412-0718-48e3-ec97-7f7ab22e1444"},"source":["print(len(train_df), len(tr_inputs_cam), len(tr_inputs_flau))"],"execution_count":162,"outputs":[{"output_type":"stream","text":["37478 37478 37478\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"Uh2PccgCXJeI"},"source":["# Data Loaders\n","\n","We need to use the DataLoaders to create iterable objects for us to work with. We tell it which datasets we want to use, give it a batch size, and shuffle the data"]},{"cell_type":"code","metadata":{"id":"nfFhYJYpSr05","executionInfo":{"status":"ok","timestamp":1620398019167,"user_tz":-120,"elapsed":394217,"user":{"displayName":"Sara Fattouh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjGrdvZo2x_8K8abskRubIgMK3SdtS4cG-aQE01nA=s64","userId":"09360594903764573322"}}},"source":["batch_size = 32 #increase batch size to reduce the noise \n","\n","train_dataloader = DataLoader(train_dataset,batch_size=batch_size,shuffle=True)\n","\n","validation_dataloader = DataLoader(val_dataset,batch_size=batch_size,shuffle=False)\n"," \n","test_dataloader = DataLoader(test_dataset,batch_size=batch_size,shuffle=False)"],"execution_count":163,"outputs":[]},{"cell_type":"code","metadata":{"id":"Y-gP7nVnSwAt","executionInfo":{"status":"ok","timestamp":1620398019168,"user_tz":-120,"elapsed":394211,"user":{"displayName":"Sara Fattouh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjGrdvZo2x_8K8abskRubIgMK3SdtS4cG-aQE01nA=s64","userId":"09360594903764573322"}}},"source":["optimizer = AdamW( model.parameters(),\n","                  lr = 2e-4, # args.learning_rate - default is 5e-5, our notebook had 2e-5\n","                  eps = 1e-8, # args.adam_epsilon  - default is 1e-8.\n","                  weight_decay= 0.001\n","                )"],"execution_count":164,"outputs":[]},{"cell_type":"code","metadata":{"id":"PgoWU2g6S1CZ","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1620398019168,"user_tz":-120,"elapsed":394205,"user":{"displayName":"Sara Fattouh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjGrdvZo2x_8K8abskRubIgMK3SdtS4cG-aQE01nA=s64","userId":"09360594903764573322"}},"outputId":"75335cc2-74c1-4939-f558-199d9c0e7c63"},"source":["def count_parameters(model):\n","    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n","count_parameters(model)"],"execution_count":165,"outputs":[{"output_type":"execute_result","data":{"text/plain":["1595164"]},"metadata":{"tags":[]},"execution_count":165}]},{"cell_type":"code","metadata":{"id":"0CRNzbwCS6vU","executionInfo":{"status":"ok","timestamp":1620398019168,"user_tz":-120,"elapsed":394194,"user":{"displayName":"Sara Fattouh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjGrdvZo2x_8K8abskRubIgMK3SdtS4cG-aQE01nA=s64","userId":"09360594903764573322"}}},"source":["from transformers import get_linear_schedule_with_warmup\n","\n","# Number of training epochs. The BERT authors recommend between 2 and 4. \n","# We chose to run for 4, but we'll see later that this may be over-fitting the\n","# training data.\n","epochs = 6\n","\n","# Total number of training steps is [number of batches] x [number of epochs]. \n","# (Note that this is not the same as the number of training samples).\n","total_steps = len(train_dataloader) * epochs\n","\n","# Create the learning rate scheduler.\n","scheduler = get_linear_schedule_with_warmup(optimizer, \n","                                            num_warmup_steps = 0, # Default value in run_glue.py\n","                                            num_training_steps = total_steps)"],"execution_count":166,"outputs":[]},{"cell_type":"code","metadata":{"id":"en6CE-_8S-Vw","executionInfo":{"status":"ok","timestamp":1620398019169,"user_tz":-120,"elapsed":394189,"user":{"displayName":"Sara Fattouh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjGrdvZo2x_8K8abskRubIgMK3SdtS4cG-aQE01nA=s64","userId":"09360594903764573322"}}},"source":["import torch.nn as nn\n","loss_criterion = nn.CrossEntropyLoss()"],"execution_count":167,"outputs":[]},{"cell_type":"code","metadata":{"id":"MLg8d2UmTBLp","executionInfo":{"status":"ok","timestamp":1620398019169,"user_tz":-120,"elapsed":394182,"user":{"displayName":"Sara Fattouh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjGrdvZo2x_8K8abskRubIgMK3SdtS4cG-aQE01nA=s64","userId":"09360594903764573322"}}},"source":["def flat_accuracy(preds, labels):\n","    pred_flat = np.argmax(preds, axis=1).flatten()\n","    labels_flat = labels.flatten()\n","    return np.sum(pred_flat == labels_flat) / len(labels_flat)"],"execution_count":168,"outputs":[]},{"cell_type":"code","metadata":{"id":"kY7xbQXqTHUF","colab":{"base_uri":"https://localhost:8080/","height":0},"executionInfo":{"status":"ok","timestamp":1620406151776,"user_tz":-120,"elapsed":8526782,"user":{"displayName":"Sara Fattouh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjGrdvZo2x_8K8abskRubIgMK3SdtS4cG-aQE01nA=s64","userId":"09360594903764573322"}},"outputId":"77031fed-03da-45fd-babb-f940d0375a36"},"source":["from sklearn.metrics import f1_score\n","\n","seed_val = 42\n","#seed_val = 56\n","\n","random.seed(seed_val)\n","np.random.seed(seed_val)\n","torch.manual_seed(seed_val)\n","torch.cuda.manual_seed_all(seed_val)\n","\n","# We'll store a number of quantities such as training and validation loss, \n","# validation accuracy, and timings.\n","training_stats = []\n","train_loss_values = []\n","\n","val_loss_values = []\n","logits_values =[]\n","\n","############\n","\n","total_train_accuracy = 0\n","avg_train_accuracy = 0\n","\n","train_accuracy_values = []\n","val_accuracy_values = []\n","\n","##########\n","\n","# Measure the total training time for the whole run.\n","total_t0 = time.time()\n","\n","\n","\n","# For each epoch...\n","for epoch_i in range(0, epochs):\n","    \n","    # ========================================\n","    #               Training\n","    # ========================================\n","    \n","    # Perform one full pass over the training set.\n","\n","    print(\"\")\n","    print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n","    print('Training...')\n","    \n","    #tr and val\n","#     vec_output_tr = []\n","#     vec_output_val =[]\n","\n","    # Measure how long the training epoch takes.\n","    t0 = time.time()\n","\n","    # Reset the total loss for this epoch.\n","    total_train_loss = 0\n","    total_train_accuracy = 0\n","    predictions=[]\n","    true_labels=[]\n","\n","    # Put the model into training mode. Don't be mislead--the call to \n","    # `train` just changes the *mode*, it doesn't *perform* the training.\n","    # `dropout` and `batchnorm` layers behave differently during training\n","    # vs. test (source: https://stackoverflow.com/questions/51433378/what-does-model-train-do-in-pytorch)\n","\n","    model.to(device)\n","    best_f1 = 0\n","    model.train()\n","\n","    # For each batch of training data...\n","    for step, batch in (enumerate(train_dataloader)):\n","        \n","        # Unpack this training batch from our dataloader. \n","        #   \n","        # As we unpack the batch, we'll also copy each tensor to the GPU using the \n","        # `to` method.\n","        #\n","        # `batch` contains three pytorch tensors:\n","        #   [0]: input ids \n","        #   [1]: attention masks\n","        #   [2]: labels \n","#         return img,input_id_cam,input_mask_cam,input_id_flau,input_mask_flau\n","\n","        b_img               = batch[0].to(device)\n","        b_input_id_cam      = batch[1].to(device)\n","        b_input_mask_cam    = batch[2].to(device)\n","        b_input_id_flau     = batch[3].to(device)\n","        b_input_mask_flau   = batch[4].to(device)\n","        b_labels            = batch[5].to(device)\n","        \n","        \n","        model.zero_grad()    #set the gradients to zero before starting to do backpropragation because PyTorch accumulates \n","                            # the gradients on subsequent backward passes\n","\n","        \n","        logits = model(b_img, b_input_id_cam , b_input_mask_cam, b_input_id_flau, b_input_mask_flau)  # 27\n","                            \n","        #Defining the loss\n","        loss = loss_criterion(logits, b_labels)\n","        \n","        #saving the features_tr\n","#         vec = vec.detach().cpu().numpy()\n","#         vec_output_tr.extend(vec)\n","        \n","        # Accumulate the training loss over all of the batches so that we can\n","        # calculate the average loss at the end. `loss` is a Tensor containing a\n","        # single value; the `.item()` function just returns the Python value \n","        # from the tensor.\n","\n","        total_train_loss += loss.item()\n","#-------------------------------------------------------\n","\n","        # Move logits and labels to CPU\n","        logits = logits.detach().cpu().numpy()\n","\n","        # Move logits and labels to CPU\n","        predicted_labels=np.argmax(logits,axis=1)\n","        predictions.extend(predicted_labels)\n","        label_ids = b_labels.to('cpu').numpy()\n","        true_labels.extend(label_ids)\n","\n","        total_train_accuracy += flat_accuracy(logits, label_ids)\n","\n","\n","#-------------------------------------------------------\n","\n","        # Perform a backward pass to calculate the gradients.\n","        loss.backward()\n","\n","        # Clip the norm of the gradients to 1.0.\n","        # This is to help prevent the \"exploding gradients\" problem.\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n","\n","        # Update parameters and take a step using the computed gradient.\n","        # The optimizer dictates the \"update rule\"--how the parameters are\n","        # modified based on their gradients, the learning rate, etc.\n","        optimizer.step()\n","\n","        # Update the learning rate.\n","        scheduler.step()\n","\n","\n","  # ------------------------------------------------------\n","        \n","    avg_train_accuracy = total_train_accuracy / len(train_dataloader)\n","\n","    print(\"\")\n","    print(\"Training Accuracy: {}\".format(avg_train_accuracy))\n","    train_accuracy_values.append(avg_train_accuracy)\n","\n","    ######################################################################################\n","\n","    # Calculate the average loss over all of the batches.\n","    avg_train_loss = total_train_loss / len(train_dataloader)  \n","    train_loss_values.append(avg_train_loss)  #-- move 2 lines up (newly added code block)\n","             \n","    \n","    # Measure how long this epoch took.\n","    training_time = format_time(time.time() - t0)\n","\n","    \n","    print(\"  Average training loss: {} \".format(avg_train_loss))\n","    print(\"  Training epcoh took: {:} \".format(training_time))\n","        \n","    # ========================================\n","    #               Validation\n","    # ========================================\n","    # After the completion of each training epoch, measure our performance on\n","    # our validation set.\n","\n","    print(\"\")\n","    print(\"Running Validation...\")\n","\n","    t0 = time.time()\n","\n","    # Put the model in evaluation mode--the dropout layers behave differently\n","    # during evaluation.\n","    model.eval()\n","\n","    # Tracking variables \n","    total_eval_accuracy = 0\n","    total_eval_loss = 0\n","    nb_eval_steps = 0\n","    predictions=[]\n","    true_labels=[]\n","    \n","\n","    # Evaluate data for one epoch\n","    for batch in (validation_dataloader):\n","        \n","        # Unpack this training batch from our dataloader. \n","        #\n","        # As we unpack the batch, we'll also copy each tensor to the GPU using \n","        # the `to` method.\n","        #\n","        # `batch` contains three pytorch tensors:\n","        #   [0]: input ids \n","        #   [1]: attention masks\n","        #   [2]: labels \n","        \n","        b_img = batch[0].to(device)\n","\n","        b_input_id_cam = batch[1].to(device)\n","        b_input_mask_cam = batch[2].to(device)\n","        b_input_id_flau = batch[3].to(device)\n","        b_input_mask_flau = batch[4].to(device)\n","\n","        b_labels = batch[5].to(device)\n","        \n","        \n","        # Tell pytorch not to bother with constructing the compute graph during\n","        # the forward pass, since this is only needed for backprop (training).\n","        with torch.no_grad():       \n","        \n","\n","            # Forward pass, calculate logit predictions.\n","            # token_type_ids is the same as the \"segment ids\", which \n","            # differentiates sentence 1 and 2 in 2-sentence tasks.\n","            # The documentation for this `model` function is here: \n","            # https://huggingface.co/transformers/v2.2.0/model_doc/bert.html#transformers.BertForSequenceClassification\n","            # Get the \"logits\" output by the model. The \"logits\" are the output\n","            # values prior to applying an activation function like the softmax.\n","            logits = model(b_img,b_input_id_cam ,b_input_mask_cam,b_input_id_flau,b_input_mask_flau)\n","            \n","        #new\n","        \n","        #defining the val loss\n","        loss = loss_criterion(logits, b_labels)\n","        \n","        # Accumulate the validation loss.\n","        total_eval_loss += loss.item()\n","\n","        # Move logits and labels to CPU\n","        logits = logits.detach().cpu().numpy()\n","\n","        # Move logits and labels to CPU\n","        predicted_labels=np.argmax(logits,axis=1)\n","        predictions.extend(predicted_labels)\n","        label_ids = b_labels.to('cpu').numpy()\n","        true_labels.extend(label_ids)\n","\n","        ##########################################################################\n","\n","        logits_values.append(predicted_labels)\n","\n","        ##########################################################################\n","\n","        #saving the features_tr\n","#         vec = vec.detach().cpu().numpy()\n","#         vec_output_val.extend(vec)\n","        \n","\n","        # Calculate the accuracy for this batch of test sentences, and\n","        # accumulate it over all batches.\n","        total_eval_accuracy += flat_accuracy(logits, label_ids)\n","        \n","\n","    # Report the final accuracy for this validation run.\n","    avg_val_accuracy = total_eval_accuracy / len(validation_dataloader)\n","\n","#--------------------------------\n","    val_accuracy_values.append(avg_val_accuracy)\n","#--------------------------------\n","    print(\"  Accuracy: {}\".format(avg_val_accuracy))\n","\n","    # Calculate the average loss over all of the batches.\n","    avg_val_loss = total_eval_loss / len(validation_dataloader)\n","#-----------------------------\n","    val_loss_values.append(avg_val_loss)\n","\n","    \n","    \n","    # Measure how long the validation run took.\n","    validation_time = format_time(time.time() - t0)\n","    \n","    print(\"  Validation Loss: {}\".format(avg_val_loss))\n","    print(\"  Validation took: {:}\".format(validation_time))\n","    print(\"Validation F1-Score: {}\".format(f1_score(true_labels,predictions,average='macro')))\n","    curr_f1=f1_score(true_labels,predictions,average='macro')\n","    if curr_f1 > best_f1:\n","        best_f1=curr_f1\n","        torch.save(model.state_dict(), '/content/drive/My Drive/Rakuten/models/baseline_Average_final_fusion.pt')\n","#         np.save('best_vec_train_model_train.npy',vec_output_tr)\n","#         np.save('best_vec_val.npy',vec_output_val)\n","        \n","    # Record all statistics from this epoch.\n","#     training_stats.append(\n","#         {\n","#             'epoch': epoch_i + 1,\n","#             'Training Loss': avg_train_loss,\n","#             'Valid. Loss': avg_val_loss,\n","#             'Valid. Accur.': avg_val_accuracy,\n","#             'Training Time': training_time,\n","#             'Validation Time': validation_time\n","#         }\n","#     )\n","\n","print(\"\")\n","print(\"Training complete!\")\n","\n","print(\"Total training took {:} (h:mm:ss)\".format(format_time(time.time()-total_t0)))\n","\n","print()\n","\n","plt.plot(np.array(train_loss_values), 'r', label='train loss')\n","plt.plot(np.array(val_loss_values), 'b', label='val loss'  )\n","plt.legend()\n","plt.title('Loss Curve')\n","plt.show()\n","\n","print()\n","\n","plt.plot(np.array(train_accuracy_values), 'r', label='train accuracy')\n","plt.plot(np.array(val_accuracy_values), 'b', label='val accuracy'  )\n","plt.legend()\n","plt.title('Train Curve')\n","plt.show()\n","\n","#print(logits_values)\n"],"execution_count":169,"outputs":[{"output_type":"stream","text":["\n","======== Epoch 1 / 6 ========\n","Training...\n","\n","Training Accuracy: 0.9498720136518771\n","  Average training loss: 0.2996046843774833 \n","  Training epcoh took: 0:19:17 \n","\n","Running Validation...\n","  Accuracy: 0.9211270311813791\n","  Validation Loss: 0.28702854193693056\n","  Validation took: 0:03:09\n","Validation F1-Score: 0.879916002857713\n","\n","======== Epoch 2 / 6 ========\n","Training...\n","\n","Training Accuracy: 0.9965603668941979\n","  Average training loss: 0.02520390672258772 \n","  Training epcoh took: 0:18:58 \n","\n","Running Validation...\n","  Accuracy: 0.9230895915678525\n","  Validation Loss: 0.3129074929792722\n","  Validation took: 0:03:08\n","Validation F1-Score: 0.891073768468202\n","\n","======== Epoch 3 / 6 ========\n","Training...\n","\n","Training Accuracy: 0.9970136518771331\n","  Average training loss: 0.016418302145730163 \n","  Training epcoh took: 0:19:09 \n","\n","Running Validation...\n","  Accuracy: 0.9206741326306545\n","  Validation Loss: 0.32733099477743544\n","  Validation took: 0:03:12\n","Validation F1-Score: 0.8880269562587327\n","\n","======== Epoch 4 / 6 ========\n","Training...\n","\n","Training Accuracy: 0.9972002986348123\n","  Average training loss: 0.013769309843493841 \n","  Training epcoh took: 0:19:10 \n","\n","Running Validation...\n","  Accuracy: 0.9202212340799298\n","  Validation Loss: 0.34554000442216604\n","  Validation took: 0:03:11\n","Validation F1-Score: 0.8858378957773129\n","\n","======== Epoch 5 / 6 ========\n","Training...\n","\n","Training Accuracy: 0.9976802474402731\n","  Average training loss: 0.011106305754011054 \n","  Training epcoh took: 0:19:14 \n","\n","Running Validation...\n","  Accuracy: 0.921277997364954\n","  Validation Loss: 0.3448716633430691\n","  Validation took: 0:03:08\n","Validation F1-Score: 0.8856839600928886\n","\n","======== Epoch 6 / 6 ========\n","Training...\n","\n","Training Accuracy: 0.9976269197952219\n","  Average training loss: 0.010709006838621088 \n","  Training epcoh took: 0:18:59 \n","\n","Running Validation...\n","  Accuracy: 0.9209760649978042\n","  Validation Loss: 0.3461399324584734\n","  Validation took: 0:03:10\n","Validation F1-Score: 0.8846258607845324\n","\n","Training complete!\n","Total training took 2:15:31 (h:mm:ss)\n","\n"],"name":"stdout"},{"output_type":"display_data","data":{"image/png":"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\n","text/plain":["<Figure size 432x288 with 1 Axes>"]},"metadata":{"tags":[],"needs_background":"light"}},{"output_type":"stream","text":["\n"],"name":"stdout"},{"output_type":"display_data","data":{"image/png":"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\n","text/plain":["<Figure size 432x288 with 1 Axes>"]},"metadata":{"tags":[],"needs_background":"light"}}]},{"cell_type":"code","metadata":{"id":"1ptp-vlMkCx0","executionInfo":{"status":"ok","timestamp":1620406151780,"user_tz":-120,"elapsed":8526774,"user":{"displayName":"Sara Fattouh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjGrdvZo2x_8K8abskRubIgMK3SdtS4cG-aQE01nA=s64","userId":"09360594903764573322"}}},"source":["model_path =  '/content/drive/My Drive/Rakuten/models/baseline_Average_final_fusion.pt'"],"execution_count":170,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"EjQ2KpIIkHAF","executionInfo":{"status":"ok","timestamp":1620406160963,"user_tz":-120,"elapsed":8535950,"user":{"displayName":"Sara Fattouh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjGrdvZo2x_8K8abskRubIgMK3SdtS4cG-aQE01nA=s64","userId":"09360594903764573322"}},"outputId":"680f8581-9aec-4773-abcb-72e889817105"},"source":["checkpoint = torch.load(model_path)\n","model.load_state_dict(checkpoint) # A state_dict is simply a Python dictionary object \n","                                  # that maps each layer to its parameter tensor"],"execution_count":171,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<All keys matched successfully>"]},"metadata":{"tags":[]},"execution_count":171}]},{"cell_type":"code","metadata":{"id":"Y_VJZ6BCkIXi","executionInfo":{"status":"ok","timestamp":1620406160964,"user_tz":-120,"elapsed":8535941,"user":{"displayName":"Sara Fattouh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjGrdvZo2x_8K8abskRubIgMK3SdtS4cG-aQE01nA=s64","userId":"09360594903764573322"}}},"source":["import matplotlib.pyplot as plt\n","from sklearn.metrics import f1_score\n","\n","def predict_pyt(model, prediction_dataloader):\n","    \"\"\"\n","    model: pytorch model\n","    prediction_dataloader: DataLoader object for which the predictions has to be made.\n","    return:\n","        predictions:    - Direct predicted labels\n","        softmax_logits: - logits which are normalized with softmax on output\"\"\"\n","    \n","    \n","    print(\"\")\n","    print(\"Running Testing...\")\n","\n","    t0 = time.time()\n","\n","    # Put the model in evaluation mode--the dropout layers behave differently\n","    # during evaluation.\n","    model.eval()\n","\n","    # Tracking variables \n","    total_eval_accuracy = 0\n","    total_eval_loss = 0\n","    nb_eval_steps = 0\n","    predictions=[]\n","    true_labels=[]\n","    logits_values =[]\n","    val_accuracy_values = []\n","    val_loss_values = []\n","    \n","    \n","    total_t0 = time.time()\n","    # Evaluate data for one epoch\n","    for batch in (prediction_dataloader):\n","        \n","        # Unpack this training batch from our dataloader. \n","        #\n","        # As we unpack the batch, we'll also copy each tensor to the GPU using \n","        # the `to` method.\n","        #\n","        # `batch` contains three pytorch tensors:\n","        #   [0]: input ids \n","        #   [1]: attention masks\n","        #   [2]: labels \n","        \n","        b_img = batch[0].to(device)\n","\n","        b_input_id_cam = batch[1].to(device)\n","        b_input_mask_cam = batch[2].to(device)\n","        b_input_id_flau = batch[3].to(device)\n","        b_input_mask_flau = batch[4].to(device)\n","\n","        b_labels = batch[5].to(device)\n","        \n","        \n","        # Tell pytorch not to bother with constructing the compute graph during\n","        # the forward pass, since this is only needed for backprop (training).\n","        with torch.no_grad():       \n","        \n","\n","            # Forward pass, calculate logit predictions.\n","            # token_type_ids is the same as the \"segment ids\", which \n","            # differentiates sentence 1 and 2 in 2-sentence tasks.\n","            # The documentation for this `model` function is here: \n","            # https://huggingface.co/transformers/v2.2.0/model_doc/bert.html#transformers.BertForSequenceClassification\n","            # Get the \"logits\" output by the model. The \"logits\" are the output\n","            # values prior to applying an activation function like the softmax.\n","\n","\n","            logits = model(b_img,b_input_id_cam ,b_input_mask_cam,b_input_id_flau,b_input_mask_flau)\n","            \n","        #new\n","        \n","        #defining the val loss\n","        loss = loss_criterion(logits, b_labels)\n","        \n","        # Accumulate the validation loss.\n","        total_eval_loss += loss.item()\n","\n","        # Move logits and labels to CPU\n","        logits = logits.detach().cpu().numpy()\n","\n","        # Move logits and labels to CPU\n","        predicted_labels=np.argmax(logits,axis=1)\n","        predictions.extend(predicted_labels)\n","        label_ids = b_labels.to('cpu').numpy()\n","        true_labels.extend(label_ids)\n","\n","        ##########################################################################\n","\n","        logits_values.append(predicted_labels)\n","\n","        ##########################################################################\n","\n","        # Calculate the accuracy for this batch of test sentences, and\n","        # accumulate it over all batches.\n","        total_eval_accuracy += flat_accuracy(logits, label_ids)\n","        \n","    # Report the final accuracy for this validation run.\n","    avg_val_accuracy = total_eval_accuracy / len(prediction_dataloader)\n","    val_accuracy_values.append(avg_val_accuracy)\n","#--------------------------------\n","    print(\"  Accuracy: {}\".format(avg_val_accuracy))\n","\n","    # Calculate the average loss over all of the batches.\n","    avg_val_loss = total_eval_loss / len(prediction_dataloader)\n","#-----------------------------\n","    val_loss_values.append(avg_val_loss)\n","\n","    \n","    \n","    # Measure how long the validation run took.\n","    validation_time = format_time(time.time() - t0)\n","    \n","    print(\"  Test Loss: {}\".format(avg_val_loss))\n","    print(\"  Test took: {:}\".format(validation_time))\n","    print(\"Test F1-Score: {}\".format(f1_score(true_labels,predictions,average='macro')))\n","    curr_f1=f1_score(true_labels,predictions,average='macro')\n","\n","\n","    print(\"\")\n","    print(\"Testing complete!\")\n","\n","    #print(\"Total testing took {:} (h:mm:ss)\".format(format_time(time.time()-total_t0)))\n","    print()\n","    plt.plot(np.array(val_accuracy_values), 'r', label='Test accuracy')\n","    plt.legend()\n","    plt.title('Test Curve')\n","    plt.show()\n","\n","    print()\n","    print('DONE')\n"],"execution_count":172,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":0},"id":"-KcbbPdzkKmh","executionInfo":{"status":"ok","timestamp":1620406499970,"user_tz":-120,"elapsed":8874940,"user":{"displayName":"Sara Fattouh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjGrdvZo2x_8K8abskRubIgMK3SdtS4cG-aQE01nA=s64","userId":"09360594903764573322"}},"outputId":"1a5ce496-b3d6-47f6-c3a7-f2619cc5d248"},"source":["predict_pyt(model, test_dataloader)"],"execution_count":173,"outputs":[{"output_type":"stream","text":["\n","Running Testing...\n","  Accuracy: 0.9228260869565217\n","  Test Loss: 0.3686892760887175\n","  Test took: 0:05:38\n","Test F1-Score: 0.903134762827769\n","\n","Testing complete!\n","\n"],"name":"stdout"},{"output_type":"display_data","data":{"image/png":"iVBORw0KGgoAAAANSUhEUgAAAXoAAAEICAYAAABRSj9aAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAXxUlEQVR4nO3df5BV5Z3n8fdHmh8moAh0CKERcNZJaLAbtMGVVBaMEDAVlR/OBEOCOhqXZcGaXakEwiRDyFCJRkvXxCnD7vp7DbgYoonZNaIwsVZnoVEUGQZBJKERJz0QECT8/u4f9zR76VzoC32b7n78vKpu9TnP85zT3+d21aefPuf2vYoIzMwsXee0dgFmZtayHPRmZolz0JuZJc5Bb2aWOAe9mVniHPRmZolz0JuZJc5Bb+2KpH15j2OS/pi3P/UMzrdS0q1NjOkkab6kTZI+lLRV0kOSBpzpPMzOJge9tSsR0bXhAfwOuCav7X+00LddClwLfAU4H6gG1gBXne6JJJWVtjSzpjnoLQmSzpE0R9I7knZKekpSj6yvi6QnsvbdklZL6i1pIfA54MfZXwQ/LnDeMcBY4LqIWB0RRyJiT0Q8EBH/PRuzNRvXcMx8SU9k2wMkhaRbJP0OeEnS/5I0s9H3eUPSpGz7M5JekLRL0kZJf9lCT5t9RDjoLRWzgAnAKOBTwB+AB7K+G8mtxPsBPYHpwB8jYh7wMjAz+4tg5p+cFcYAqyJiWzPrGwUMAsYBPwVuaOiQVAn0B56T9HHgBeBJ4BPAFODvszFmZ8RBb6mYDsyLiLqIOAjMB67PLpUcJhfw/yYijkbEmoj4oMjz9gR2lKC++RHxYUT8EVgGDJXUP+ubCvwsq/tLwNaIeDj76+F14GngL0pQg31EOegtFf2BZdmlmd3ABuAo0Bt4HHgeWCzpPUl3SepY5Hl3An1KUN/xvwgiYi/wHLnVOuRW9w33F/oDlzfMI5vLVOCTJajBPqIc9JaKbcDVEdE979ElIrZHxOGI+G5EVAIjya2ap2XHNfX2rcuBEZIqTjHmQ+BjefuFQrnx9/kpcIOkK4AuwIq8efxDo3l0jYj/0ESdZifloLdUPAgsbLgcIqlc0nXZ9pWSLpHUAfiA3KWcY9lx/wJcdLKTRsRyctfMl0m6TFKZpG6Spkv6q2zYWmCKpI6SaoDri6j3V+RW7wuAJRHRUM8vgT+X9LXsfB0lDZc06DSeC7MTOOgtFf8FeBb4taS9wD8Cl2d9nyT3EskPyF3S+Qdyl3Majrte0h8k3X+Sc19PLpiXAHuAt4Aacqt9gG8Df0buBvB3yd1IPaXsevzPyN3sfTKvfS/wBXKXdd4D3gfuBDo3dU6zk5E/eMTMLG1e0ZuZJc5Bb2aWOAe9mVniHPRmZolrc2+w1KtXrxgwYEBrl2Fm1q6sWbPmXyOivFBfmwv6AQMGUFtb29plmJm1K5J+e7I+X7oxM0ucg97MLHEOejOzxLW5a/Rm1jYcPnyYuro6Dhw40NqlWJ4uXbpQUVFBx47FvgGrg97MTqKuro5u3boxYMAAJLV2OQZEBDt37qSuro6BAwcWfZwv3ZhZQQcOHKBnz54O+TZEEj179jztv7Ic9GZ2Ug75tudMfiYOejOzxPkavZm1OTt37uSqq64C4P3336dDhw6Ul+f+6XPVqlV06tTplMevXLmSTp06MXLkyBavtT1w0JtZm9OzZ0/Wrl0LwPz58+natSuzZ88u+viVK1fStWvXVg/6o0eP0qFDh1atAXzpxszaiTVr1jBq1Cguu+wyxo0bx44dOwC4//77qayspKqqiilTprB161YefPBB7r33XoYOHcrLL798wnlWrVrFFVdcwbBhwxg5ciQbN24EcqE8e/ZshgwZQlVVFT/60Y8AWL16NSNHjqS6upoRI0awd+9eHnnkEWbOnHn8nF/60pdYuXIlAF27duWOO+6gurqaV199lQULFjB8+HCGDBnCbbfdRsOHPW3evJkxY8ZQXV3NpZdeyjvvvMO0adP4+c9/fvy8U6dO5Zlnnmn2c+cVvZk17a//GrIVdskMHQr33VfU0Ihg1qxZPPPMM5SXl7NkyRLmzZvHQw89xA9+8APeffddOnfuzO7du+nevTvTp08/6V8Bn/nMZ3j55ZcpKytj+fLlfOtb3+Lpp59m0aJFbN26lbVr11JWVsauXbs4dOgQX/7yl1myZAnDhw/ngw8+4Nxzzz1lrR9++CGXX34599xzDwCVlZV85zvfAeBrX/sav/zlL7nmmmuYOnUqc+bMYeLEiRw4cIBjx45xyy23cO+99zJhwgT27NnDK6+8wqOPPnqaT+yfctCbWZt38OBB3nrrLcaOHQvkVt99+vQBoKqqiqlTpzJhwgQmTJjQ5Ln27NnDjTfeyKZNm5DE4cOHAVi+fDnTp0+nrCwXiz169GDdunX06dOH4cOHA3Deeec1ef4OHTowefLk4/srVqzgrrvuYv/+/ezatYvBgwczevRotm/fzsSJE4HcP0EBjBo1ihkzZlBfX8/TTz/N5MmTj9fTHA56M2takSvvlhIRDB48mFdfffVP+p577jl+85vf8Itf/IKFCxeybt26U57r29/+NldeeSXLli1j69atjB49+rTrKSsr49ixY8f381/X3qVLl+PX5Q8cOMCMGTOora2lX79+zJ8/v8nXwE+bNo0nnniCxYsX8/DDD592bYX4Gr2ZtXmdO3emvr7+eNAfPnyY9evXc+zYMbZt28aVV17JnXfeyZ49e9i3bx/dunVj7969Bc+1Z88e+vbtC8AjjzxyvH3s2LH85Cc/4ciRIwDs2rWLT3/60+zYsYPVq1cDsHfvXo4cOcKAAQNYu3bt8e+/atWqgt+rIdR79erFvn37WLp0KQDdunWjoqLi+PX4gwcPsn//fgBuuukm7st+sVZWVp7xc5bPQW9mbd4555zD0qVL+eY3v0l1dTVDhw7llVde4ejRo3z1q1/lkksuYdiwYdx+++10796da665hmXLlhW8GfuNb3yDuXPnMmzYsOOhDnDrrbdy4YUXUlVVRXV1NU8++SSdOnViyZIlzJo1i+rqasaOHcuBAwf47Gc/y8CBA6msrOT222/n0ksvLVh39+7d+frXv86QIUMYN27c8UtAAI8//jj3338/VVVVjBw5kvfffx+A3r17M2jQIG6++eaSPX9quAPcVtTU1IQ/eMSs9W3YsIFBgwa1dhkfOfv37+eSSy7htdde4/zzzy84ptDPRtKaiKgpNN4rejOzNmL58uUMGjSIWbNmnTTkz4RvxpqZtRFjxozht7896ScCnjGv6M3spNrapV07s5+Jg97MCurSpQs7d+502LchDe9H3/C6+2L50o2ZFVRRUUFdXR319fWtXYrlafiEqdPhoDezgjp27Hhan2JkbZcv3ZiZJa6ooJc0XtJGSZslzSnQ31/Si5LelLRSUkVe34WSfi1pg6R/kjSgdOWbmVlTmgx6SR2AB4CrgUrgBkmN/y/3buCxiKgCFgDfz+t7DPhhRAwCRgC/L0XhZmZWnGJW9COAzRGxJSIOAYuB6xqNqQReyrZXNPRnvxDKIuIFgIjYFxH7S1K5mZkVpZig7wtsy9uvy9ryvQFMyrYnAt0k9QT+HNgt6WeSXpf0w+wvhBNIuk1SraRa3+E3MyutUt2MnQ2MkvQ6MArYDhwl96qez2X9w4GLgJsaHxwRiyKiJiJqGj4X0szMSqOYoN8O9Mvbr8jajouI9yJiUkQMA+ZlbbvJrf7XZpd9jgA/Bwq/zZuZmbWIYoJ+NXCxpIGSOgFTgGfzB0jqJanhXHOBh/KO7S6pYZn+eeCfml+2mZkVq8mgz1biM4HngQ3AUxGxXtICSddmw0YDGyW9DfQGFmbHHiV32eZFSesAAf+15LMwM7OT8vvRm5klwO9Hb2b2EeagNzNLnIPezCxxDnozs8Q56M3MEuegNzNLnIPezCxxDnozs8Q56M3MEuegNzNLnIPezCxxDnozs8Q56M3MEuegNzNLnIPezCxxDnozs8Q56M3MEuegNzNLnIPezCxxDnozs8Q56M3MEuegNzNLnIPezCxxDnozs8Q56M3MEuegNzNLnIPezCxxDnozs8Q56M3MEuegNzNLnIPezCxxDnozs8Q56M3MEuegNzNLXFFBL2m8pI2SNkuaU6C/v6QXJb0paaWkikb950mqk/TjUhVuZmbFaTLoJXUAHgCuBiqBGyRVNhp2N/BYRFQBC4DvN+r/HvCb5pdrZmanq5gV/Qhgc0RsiYhDwGLgukZjKoGXsu0V+f2SLgN6A79ufrlmZna6ign6vsC2vP26rC3fG8CkbHsi0E1ST0nnAPcAs0/1DSTdJqlWUm19fX1xlZuZWVFKdTN2NjBK0uvAKGA7cBSYAfwqIupOdXBELIqImoioKS8vL1FJZmYGUFbEmO1Av7z9iqztuIh4j2xFL6krMDkidku6AvicpBlAV6CTpH0R8Sc3dM3MrGUUE/SrgYslDSQX8FOAr+QPkNQL2BURx4C5wEMAETE1b8xNQI1D3szs7Gry0k1EHAFmAs8DG4CnImK9pAWSrs2GjQY2Snqb3I3XhS1Ur5mZnSZFRGvXcIKampqora1t7TLMzNoVSWsioqZQn/8z1swscQ56M7PEOejNzBLnoDczS5yD3swscQ56M7PEOejNzBLnoDczS5yD3swscQ56M7PEOejNzBLnoDczS5yD3swscQ56M7PEOejNzBLnoDczS5yD3swscQ56M7PEOejNzBLnoDczS5yD3swscQ56M7PEOejNzBLnoDczS5yD3swscQ56M7PEOejNzBLnoDczS5yD3swscQ56M7PEOejNzBLnoDczS5yD3swscQ56M7PEFRX0ksZL2ihps6Q5Bfr7S3pR0puSVkqqyNqHSnpV0vqs78ulnoCZmZ1ak0EvqQPwAHA1UAncIKmy0bC7gcciogpYAHw/a98PTIuIwcB44D5J3UtVvJmZNa2YFf0IYHNEbImIQ8Bi4LpGYyqBl7LtFQ39EfF2RGzKtt8Dfg+Ul6JwMzMrTjFB3xfYlrdfl7XlewOYlG1PBLpJ6pk/QNIIoBPwTuNvIOk2SbWSauvr64ut3czMilCqm7GzgVGSXgdGAduBow2dkvoAjwM3R8SxxgdHxKKIqImImvJyL/jNzEqprIgx24F+efsVWdtx2WWZSQCSugKTI2J3tn8e8BwwLyL+sRRFm5lZ8YpZ0a8GLpY0UFInYArwbP4ASb0kNZxrLvBQ1t4JWEbuRu3S0pVtZmbFajLoI+IIMBN4HtgAPBUR6yUtkHRtNmw0sFHS20BvYGHW/pfAvwNukrQ2ewwt9STMzOzkFBGtXcMJampqora2trXLMDNrVyStiYiaQn3+z1gzs8Q56M3MEuegNzNLnIPezCxxDnozs8Q56M3MEuegNzNLnIPezCxxDnozs8Q56M3MEuegNzNLnIPezCxxDnozs8Q56M3MEuegNzNLnIPezCxxDnozs8Q56M3MEuegNzNLnIPezCxxDnozs8Q56M3MEuegNzNLnIPezCxxDnozs8Q56M3MEuegNzNLnIPezCxxDnozs8Q56M3MEuegNzNLnIPezCxxDnozs8Q56M3MEldU0EsaL2mjpM2S5hTo7y/pRUlvSlopqSKv70ZJm7LHjaUs3szMmtZk0EvqADwAXA1UAjdIqmw07G7gsYioAhYA38+O7QH8LXA5MAL4W0kXlK58MzNrSjEr+hHA5ojYEhGHgMXAdY3GVAIvZdsr8vrHAS9ExK6I+APwAjC++WWbmVmxign6vsC2vP26rC3fG8CkbHsi0E1SzyKPRdJtkmol1dbX1xdbu5mZFaFUN2NnA6MkvQ6MArYDR4s9OCIWRURNRNSUl5eXqCQzMwMoK2LMdqBf3n5F1nZcRLxHtqKX1BWYHBG7JW0HRjc6dmUz6jUzs9NUzIp+NXCxpIGSOgFTgGfzB0jqJanhXHOBh7Lt54EvSLoguwn7hazNzMzOkiaDPiKOADPJBfQG4KmIWC9pgaRrs2GjgY2S3gZ6AwuzY3cB3yP3y2I1sCBrMzOzs0QR0do1nKCmpiZqa2tbuwwzs3ZF0pqIqCnU5/+MNTNLnIPezCxxDnozs8Q56M3MEuegNzNLnIPezCxxDnozs8Q56M3MEuegNzNLnIPezCxxDnozs8Q56M3MEuegNzNLnIPezCxxDnozs8Q56M3MEuegNzNLnIPezCxxDnozs8Q56M3MEuegNzNLnIPezCxxDnozs8Q56M3MEuegNzNLnIPezCxxDnozs8Q56M3MEuegNzNLnIPezCxxDnozs8Q56M3MEuegNzNLnIPezCxxRQW9pPGSNkraLGlOgf4LJa2Q9LqkNyV9MWvvKOlRSeskbZA0t9QTMDOzU2sy6CV1AB4ArgYqgRskVTYa9jfAUxExDJgC/H3W/hdA54i4BLgM+PeSBpSmdDMzK0YxK/oRwOaI2BIRh4DFwHWNxgRwXrZ9PvBeXvvHJZUB5wKHgA+aXbWZmRWtmKDvC2zL26/L2vLNB74qqQ74FTAra18KfAjsAH4H3B0Ruxp/A0m3SaqVVFtfX396MzAzs1Mq1c3YG4BHIqIC+CLwuKRzyP01cBT4FDAQuEPSRY0PjohFEVETETXl5eUlKsnMzKC4oN8O9Mvbr8ja8t0CPAUQEa8CXYBewFeA/x0RhyPi98D/AWqaW7SZmRWvmKBfDVwsaaCkTuRutj7baMzvgKsAJA0iF/T1Wfvns/aPA/8W+OfSlG5mZsVoMugj4ggwE3ge2EDu1TXrJS2QdG027A7g65LeAH4K3BQRQe7VOl0lrSf3C+PhiHizJSZiZmaFKZfHbUdNTU3U1ta2dhlmZu2KpDURUfDSuP8z1swscQ56M7PEOejNzBLnoDczS5yD3swscQ56M7PEOejNzBLnoDczS5yD3swscQ56M7PEOejNzBLX5t7rRlI98NvWruMM9AL+tbWLOMs8548Gz7l96B8RBT/Qo80FfXslqfZkbyiUKs/5o8Fzbv986cbMLHEOejOzxDnoS2dRaxfQCjznjwbPuZ3zNXozs8R5RW9mljgHvZlZ4hz0p0FSD0kvSNqUfb3gJONuzMZsknRjgf5nJb3V8hU3X3PmLOljkp6T9M+S1kv6wdmtvniSxkvaKGmzpDkF+jtLWpL1/19JA/L65mbtGyWNO5t1N8eZzlnSWElrJK3Lvn7+bNd+pprzc876L5S0T9Lss1VzSUSEH0U+gLuAOdn2HODOAmN6AFuyrxdk2xfk9U8CngTeau35tPScgY8BV2ZjOgEvA1e39pwK1N8BeAe4KKvzDaCy0ZgZwIPZ9hRgSbZdmY3vDAzMztOhtefUwnMeBnwq2x4CbG/t+bT0nPP6lwL/E5jd2vM5nYdX9KfnOuDRbPtRYEKBMeOAFyJiV0T8AXgBGA8gqSvwn4G/Owu1lsoZzzki9kfECoCIOAS8BlSchZpP1whgc0RsyepcTG7e+fKfh6XAVZKUtS+OiIMR8S6wOTtfW3fGc46I1yPivax9PXCupM5npermac7PGUkTgHfJzbldcdCfnt4RsSPbfh/oXWBMX2Bb3n5d1gbwPeAeYH+LVVh6zZ0zAJK6A9cAL7ZEkc3UZP35YyLiCLAH6FnksW1Rc+acbzLwWkQcbKE6S+mM55wt0r4JfPcs1FlyZa1dQFsjaTnwyQJd8/J3IiIkFf3aVElDgT+LiP/U+Lpfa2upOeedvwz4KXB/RGw5syqtrZE0GLgT+EJr13IWzAfujYh92QK/XXHQNxIRY07WJ+lfJPWJiB2S+gC/LzBsOzA6b78CWAlcAdRI2kruef+EpJURMZpW1oJzbrAI2BQR95Wg3JawHeiXt1+RtRUaU5f94jof2FnksW1Rc+aMpApgGTAtIt5p+XJLojlzvhy4XtJdQHfgmKQDEfHjli+7BFr7JkF7egA/5MQbk3cVGNOD3HW8C7LHu0CPRmMG0H5uxjZrzuTuRzwNnNPacznFHMvI3UAeyP+/STe40Zj/yIk36Z7Ktgdz4s3YLbSPm7HNmXP3bPyk1p7H2ZpzozHzaWc3Y1u9gPb0IHd98kVgE7A8L8xqgP+WN+6vyN2U2wzcXOA87Snoz3jO5FZMAWwA1maPW1t7TieZ5xeBt8m9KmNe1rYAuDbb7kLu1RabgVXARXnHzsuO20gbfFVRqecM/A3wYd7PdC3widaeT0v/nPPO0e6C3m+BYGaWOL/qxswscQ56M7PEOejNzBLnoDczS5yD3swscQ56M7PEOejNzBL3/wAHFuHK/zvNEQAAAABJRU5ErkJggg==\n","text/plain":["<Figure size 432x288 with 1 Axes>"]},"metadata":{"tags":[],"needs_background":"light"}},{"output_type":"stream","text":["\n","DONE\n"],"name":"stdout"}]}]}