{"nbformat":4,"nbformat_minor":0,"metadata":{"accelerator":"GPU","colab":{"name":"(90-10) Hir CamemBERT (Description).ipynb","provenance":[{"file_id":"1Zix7n9Z65np1w-7P-aVqeFACBBRdz3Xy","timestamp":1618514412770},{"file_id":"1WMdwkjwiqafZpYgyTXC8YnLmMBfv1aT1","timestamp":1618513875366},{"file_id":"1KxwdjlCrcdVVCkh7-onSiJRphxWlzJpE","timestamp":1616342202749},{"file_id":"13FZdt9Qy8rbzUYOZ2LRN7npXi2_nFNO6","timestamp":1610897460394}],"collapsed_sections":[],"toc_visible":true,"mount_file_id":"18ORrdbt64bo0nwSUDHkn_ItVStqNJSVS","authorship_tag":"ABX9TyNSIXJECp5zHro1QRTnZiHO"},"kernelspec":{"display_name":"Python 3","name":"python3"},"widgets":{"application/vnd.jupyter.widget-state+json":{"b3c0be48df834878a7d8861d152fdf4b":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_5468c6ea55e2453d8115d79cbb5be05d","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_339bc25b7bb64246a87531597c9f4f0c","IPY_MODEL_06139fcdd8c6436db3cb698fab8ed276"]}},"5468c6ea55e2453d8115d79cbb5be05d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"339bc25b7bb64246a87531597c9f4f0c":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_73396b62c3be4d71aaff0be18061fd2d","_dom_classes":[],"description":"Downloading: 100%","_model_name":"FloatProgressModel","bar_style":"success","max":810912,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":810912,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_18368b847ae74455ad1d49304f7cbc35"}},"06139fcdd8c6436db3cb698fab8ed276":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_8bc35773f3a34de0b891c5bd655d594b","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 811k/811k [00:01&lt;00:00, 425kB/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_0033b6ab2bea41608a312fde6a2fdf5c"}},"73396b62c3be4d71aaff0be18061fd2d":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"18368b847ae74455ad1d49304f7cbc35":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"8bc35773f3a34de0b891c5bd655d594b":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"0033b6ab2bea41608a312fde6a2fdf5c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"eccfaeeddd6e4069a0532375c923f663":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_9d7fe9c8cd8d48b0b31ebd697e0c0c18","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_5776f32da7314543a1446a66c505a3a4","IPY_MODEL_3a5eda55fba9421a96e38b9f70760b6f"]}},"9d7fe9c8cd8d48b0b31ebd697e0c0c18":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"5776f32da7314543a1446a66c505a3a4":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_b0ea351dddaf4656a04d612af7e6b763","_dom_classes":[],"description":"Downloading: 100%","_model_name":"FloatProgressModel","bar_style":"success","max":1395301,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":1395301,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_c13aa24c2a6345b3bc5619d774af23bd"}},"3a5eda55fba9421a96e38b9f70760b6f":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_05c39f17dfcb43ed93f171d75a1f8339","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 1.40M/1.40M [00:00&lt;00:00, 2.85MB/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_ac0d26427a014ba4968fac4a4427d3d2"}},"b0ea351dddaf4656a04d612af7e6b763":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"c13aa24c2a6345b3bc5619d774af23bd":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"05c39f17dfcb43ed93f171d75a1f8339":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"ac0d26427a014ba4968fac4a4427d3d2":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"35985184ff31491cb463b9844d2bad6b":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_5dcb25da607949e9b39fe6791116990c","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_46d0bc4295aa4e93b0d546e8db59d05f","IPY_MODEL_66d8ba741be84d0fb533778bb741b6c9"]}},"5dcb25da607949e9b39fe6791116990c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"46d0bc4295aa4e93b0d546e8db59d05f":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_a6208841eb1a4f81a5ce9147b892d641","_dom_classes":[],"description":"Downloading: 100%","_model_name":"FloatProgressModel","bar_style":"success","max":508,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":508,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_397b2671ba094ef3b0cdab5dd0a3e125"}},"66d8ba741be84d0fb533778bb741b6c9":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_0b4847b68f294e2c96c0eb054b9aa36a","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 508/508 [00:00&lt;00:00, 1.06kB/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_c40f1569af1647b1994967b502768a2b"}},"a6208841eb1a4f81a5ce9147b892d641":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"397b2671ba094ef3b0cdab5dd0a3e125":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"0b4847b68f294e2c96c0eb054b9aa36a":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"c40f1569af1647b1994967b502768a2b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"a490b51178ee4cc5bdeecbf8d21e6487":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_d3b697ecbd794e1db05a6d64a6a28cfb","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_f55b93a86b3e4d57bd6dccf4bb56c3cc","IPY_MODEL_47effa7283234911b7fbda7ab6f74ec7"]}},"d3b697ecbd794e1db05a6d64a6a28cfb":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"f55b93a86b3e4d57bd6dccf4bb56c3cc":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_3f91f490afda4ea0ab0e30b25c17c4c2","_dom_classes":[],"description":"Downloading: 100%","_model_name":"FloatProgressModel","bar_style":"success","max":445032417,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":445032417,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_9511691c3c63403dbce5b1d167f3b184"}},"47effa7283234911b7fbda7ab6f74ec7":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_e8aee6e517354b219890423b189c256d","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 445M/445M [00:10&lt;00:00, 42.3MB/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_b747db2106454f85a9440b257e4d014e"}},"3f91f490afda4ea0ab0e30b25c17c4c2":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"9511691c3c63403dbce5b1d167f3b184":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"e8aee6e517354b219890423b189c256d":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"b747db2106454f85a9440b257e4d014e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}}}}},"cells":[{"cell_type":"markdown","metadata":{"id":"169so1pjjIVV"},"source":["# Multimodal Calssification on Rakuten France Dataset\n","\n","# Text Processing with CamemBERT Model.\n"," \n","\n"]},{"cell_type":"code","metadata":{"id":"DxN_U1itLF40","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1619698783800,"user_tz":-120,"elapsed":724,"user":{"displayName":"Sara Fattouh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjGrdvZo2x_8K8abskRubIgMK3SdtS4cG-aQE01nA=s64","userId":"09360594903764573322"}},"outputId":"c4a1a7b3-bce4-43bf-e3e7-6bce3f6a2af4"},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":2,"outputs":[{"output_type":"stream","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"TCrFrKABTokH","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1619698784535,"user_tz":-120,"elapsed":1447,"user":{"displayName":"Sara Fattouh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjGrdvZo2x_8K8abskRubIgMK3SdtS4cG-aQE01nA=s64","userId":"09360594903764573322"}},"outputId":"4ac02788-f251-47ba-dcf3-803b5d34b9fa"},"source":["!ls"],"execution_count":3,"outputs":[{"output_type":"stream","text":["drive  sample_data\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"1opc_4DAKwCm","executionInfo":{"status":"ok","timestamp":1619698784536,"user_tz":-120,"elapsed":1446,"user":{"displayName":"Sara Fattouh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjGrdvZo2x_8K8abskRubIgMK3SdtS4cG-aQE01nA=s64","userId":"09360594903764573322"}}},"source":["!mkdir Rakuten"],"execution_count":4,"outputs":[]},{"cell_type":"code","metadata":{"id":"0S0Nf1YaK46u","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1619698784537,"user_tz":-120,"elapsed":1438,"user":{"displayName":"Sara Fattouh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjGrdvZo2x_8K8abskRubIgMK3SdtS4cG-aQE01nA=s64","userId":"09360594903764573322"}},"outputId":"b5b54de4-7307-482a-b6f7-9b753c89ec12"},"source":["cd '/content/Rakuten'"],"execution_count":5,"outputs":[{"output_type":"stream","text":["/content/Rakuten\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"gI-IUTsjgjfL","executionInfo":{"status":"ok","timestamp":1619698784538,"user_tz":-120,"elapsed":1428,"user":{"displayName":"Sara Fattouh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjGrdvZo2x_8K8abskRubIgMK3SdtS4cG-aQE01nA=s64","userId":"09360594903764573322"}}},"source":["!mkdir data models"],"execution_count":6,"outputs":[]},{"cell_type":"code","metadata":{"id":"fNigp4Lagl2z","colab":{"base_uri":"https://localhost:8080/","height":35},"executionInfo":{"status":"ok","timestamp":1619698784540,"user_tz":-120,"elapsed":1419,"user":{"displayName":"Sara Fattouh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjGrdvZo2x_8K8abskRubIgMK3SdtS4cG-aQE01nA=s64","userId":"09360594903764573322"}},"outputId":"c2f495e2-48fc-4194-be38-35a6522deede"},"source":["pwd"],"execution_count":7,"outputs":[{"output_type":"execute_result","data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'/content/Rakuten'"]},"metadata":{"tags":[]},"execution_count":7}]},{"cell_type":"code","metadata":{"id":"fCfJYNwlgp-_","executionInfo":{"status":"ok","timestamp":1619698788858,"user_tz":-120,"elapsed":5734,"user":{"displayName":"Sara Fattouh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjGrdvZo2x_8K8abskRubIgMK3SdtS4cG-aQE01nA=s64","userId":"09360594903764573322"}}},"source":["#!cp '/content/drive/My Drive/Rakuten/data/NewTest.csv' Rakuten \n","!cp '/content/drive/My Drive/Rakuten/data/NewTraining.csv' data \n","!cp '/content/drive/My Drive/Rakuten/data/catalog_english_taxonomy.tsv' data \n","!cp '/content/drive/My Drive/Rakuten/data/Y_train.tsv' data "],"execution_count":8,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"RtNQkegUMTmJ","executionInfo":{"status":"ok","timestamp":1619698789251,"user_tz":-120,"elapsed":6115,"user":{"displayName":"Sara Fattouh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjGrdvZo2x_8K8abskRubIgMK3SdtS4cG-aQE01nA=s64","userId":"09360594903764573322"}},"outputId":"8fa0a8e1-51fa-40d4-d01e-a9388b80251c"},"source":["!ls"],"execution_count":9,"outputs":[{"output_type":"stream","text":["data  models\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"63YqpKCshZSp","executionInfo":{"status":"ok","timestamp":1619698789252,"user_tz":-120,"elapsed":6106,"user":{"displayName":"Sara Fattouh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjGrdvZo2x_8K8abskRubIgMK3SdtS4cG-aQE01nA=s64","userId":"09360594903764573322"}},"outputId":"c2fb8541-8da1-4d98-e708-6cbc6c66c019"},"source":["cd '../'"],"execution_count":10,"outputs":[{"output_type":"stream","text":["/content\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ksW1W3DSh2Sm","executionInfo":{"status":"ok","timestamp":1619698789253,"user_tz":-120,"elapsed":6098,"user":{"displayName":"Sara Fattouh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjGrdvZo2x_8K8abskRubIgMK3SdtS4cG-aQE01nA=s64","userId":"09360594903764573322"}},"outputId":"95154350-6f7f-4b5d-d9a7-6c02bf6c42d1"},"source":["cd './Rakuten'"],"execution_count":11,"outputs":[{"output_type":"stream","text":["/content/Rakuten\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"shRwSBnGh6C9","executionInfo":{"status":"ok","timestamp":1619698789253,"user_tz":-120,"elapsed":6088,"user":{"displayName":"Sara Fattouh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjGrdvZo2x_8K8abskRubIgMK3SdtS4cG-aQE01nA=s64","userId":"09360594903764573322"}},"outputId":"67b5dfbd-1d53-4bc4-a341-7aff4c28a6a4"},"source":["ls"],"execution_count":12,"outputs":[{"output_type":"stream","text":["\u001b[0m\u001b[01;34mdata\u001b[0m/  \u001b[01;34mmodels\u001b[0m/\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":0},"id":"KFVTFGqciXQ-","executionInfo":{"status":"ok","timestamp":1619698789254,"user_tz":-120,"elapsed":6080,"user":{"displayName":"Sara Fattouh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjGrdvZo2x_8K8abskRubIgMK3SdtS4cG-aQE01nA=s64","userId":"09360594903764573322"}},"outputId":"de48e024-0a12-4226-c4c9-60a06f1f5e90"},"source":["pwd"],"execution_count":13,"outputs":[{"output_type":"execute_result","data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'/content/Rakuten'"]},"metadata":{"tags":[]},"execution_count":13}]},{"cell_type":"markdown","metadata":{"id":"6ZuYHDxwLtRW"},"source":["# 1. Setup\n"]},{"cell_type":"markdown","metadata":{"id":"g7LRHc7YNk5P"},"source":["# 1.1 Using Colab GPU for Training\n","Since we’ll be training a large neural network it’s best to take advantage of the free GPUs and TPUs that Google offers (in this case we’ll attach a GPU), otherwise training will take a very long time.\n","\n","A GPU can be added by going to the menu and selecting:\n","\n","Edit 🡒 Notebook Settings 🡒 Hardware accelerator 🡒 (GPU)"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"A_wpBMC_WfnC","executionInfo":{"status":"ok","timestamp":1619698794569,"user_tz":-120,"elapsed":11385,"user":{"displayName":"Sara Fattouh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjGrdvZo2x_8K8abskRubIgMK3SdtS4cG-aQE01nA=s64","userId":"09360594903764573322"}},"outputId":"c8fff1ca-7c69-432d-c56a-483cec2d6358"},"source":["import tensorflow as tf\n","\n","# Get the GPU device name.\n","device_name = tf.test.gpu_device_name()\n","\n","# The device name should look like the following:\n","if device_name == '/device:GPU:0':\n","    print('Found GPU at: {}'.format(device_name))\n","else:\n","    raise SystemError('GPU device not found')"],"execution_count":14,"outputs":[{"output_type":"stream","text":["Found GPU at: /device:GPU:0\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"rhNOtfarPTCa","executionInfo":{"status":"ok","timestamp":1619698794569,"user_tz":-120,"elapsed":11374,"user":{"displayName":"Sara Fattouh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjGrdvZo2x_8K8abskRubIgMK3SdtS4cG-aQE01nA=s64","userId":"09360594903764573322"}},"outputId":"72d6a4ef-9b76-402e-b019-39cb6a501fe1"},"source":["gpu_info = !nvidia-smi\n","gpu_info = '\\n'.join(gpu_info)\n","if gpu_info.find('failed') >= 0:\n","  print('Select the Runtime > \"Change runtime type\" menu to enable a GPU accelerator, ')\n","  print('and then re-execute this cell.')\n","else:\n","  print(gpu_info)"],"execution_count":15,"outputs":[{"output_type":"stream","text":["Thu Apr 29 12:19:52 2021       \n","+-----------------------------------------------------------------------------+\n","| NVIDIA-SMI 465.19.01    Driver Version: 460.32.03    CUDA Version: 11.2     |\n","|-------------------------------+----------------------+----------------------+\n","| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n","| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n","|                               |                      |               MIG M. |\n","|===============================+======================+======================|\n","|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n","| N/A   40C    P0    32W / 250W |    349MiB / 16280MiB |      1%      Default |\n","|                               |                      |                  N/A |\n","+-------------------------------+----------------------+----------------------+\n","                                                                               \n","+-----------------------------------------------------------------------------+\n","| Processes:                                                                  |\n","|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n","|        ID   ID                                                   Usage      |\n","|=============================================================================|\n","+-----------------------------------------------------------------------------+\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"dBP4wKiPLlrE"},"source":["# 1.2. Installing the Hugging Face Library\n","\n","Install the transformers package from Hugging Face which will give us a pytorch interface for working with BERT. This library contains interfaces for other pretrained language models.\n","\n","We’ve selected the pytorch interface because it strikes a nice balance between the high-level APIs (which are easy to use but don’t provide insight into how things work) and tensorflow code (which contains lots of details but often sidetracks us into lessons about tensorflow, when the purpose here is BERT!).\n","\n","At the moment, the Hugging Face library seems to be the most widely accepted and powerful pytorch interface for working with BERT. In addition to supporting a variety of different pre-trained transformer models, the library also includes pre-built modifications of these models suited to your specific task.\n","E.g \"BertForSequenceClassification\" that we will be using."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"EK890k2AFjSq","executionInfo":{"status":"ok","timestamp":1619698802391,"user_tz":-120,"elapsed":19186,"user":{"displayName":"Sara Fattouh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjGrdvZo2x_8K8abskRubIgMK3SdtS4cG-aQE01nA=s64","userId":"09360594903764573322"}},"outputId":"aeb07cc9-1425-4748-946d-22dfb6fc7b5e"},"source":["!pip install transformers\n","!pip install sentencepiece\n"],"execution_count":16,"outputs":[{"output_type":"stream","text":["Collecting transformers\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d8/b2/57495b5309f09fa501866e225c84532d1fd89536ea62406b2181933fb418/transformers-4.5.1-py3-none-any.whl (2.1MB)\n","\u001b[K     |████████████████████████████████| 2.1MB 8.4MB/s \n","\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.0.12)\n","Collecting tokenizers<0.11,>=0.10.1\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ae/04/5b870f26a858552025a62f1649c20d29d2672c02ff3c3fb4c688ca46467a/tokenizers-0.10.2-cp37-cp37m-manylinux2010_x86_64.whl (3.3MB)\n","\u001b[K     |████████████████████████████████| 3.3MB 51.3MB/s \n","\u001b[?25hCollecting sacremoses\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/75/ee/67241dc87f266093c533a2d4d3d69438e57d7a90abb216fa076e7d475d4a/sacremoses-0.0.45-py3-none-any.whl (895kB)\n","\u001b[K     |████████████████████████████████| 901kB 51.9MB/s \n","\u001b[?25hRequirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from transformers) (3.10.1)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers) (20.9)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n","Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.41.1)\n","Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.0.1)\n","Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.4.1)\n","Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.7.4.3)\n","Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers) (2.4.7)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2020.12.5)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n","Installing collected packages: tokenizers, sacremoses, transformers\n","Successfully installed sacremoses-0.0.45 tokenizers-0.10.2 transformers-4.5.1\n","Collecting sentencepiece\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f5/99/e0808cb947ba10f575839c43e8fafc9cc44e4a7a2c8f79c60db48220a577/sentencepiece-0.1.95-cp37-cp37m-manylinux2014_x86_64.whl (1.2MB)\n","\u001b[K     |████████████████████████████████| 1.2MB 7.4MB/s \n","\u001b[?25hInstalling collected packages: sentencepiece\n","Successfully installed sentencepiece-0.1.95\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"6wKDUqYCNQnq"},"source":["# 2.  Dataset Loading and Preprocessing"]},{"cell_type":"code","metadata":{"id":"Tx-hG-O-hHHU","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1619698805147,"user_tz":-120,"elapsed":21933,"user":{"displayName":"Sara Fattouh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjGrdvZo2x_8K8abskRubIgMK3SdtS4cG-aQE01nA=s64","userId":"09360594903764573322"}},"outputId":"8e75f879-9096-454b-bb87-b1361e9b3a2b"},"source":["import os\n","import pandas as pd\n","from tqdm._tqdm_notebook import tqdm_notebook\n","from google.colab import drive\n","tqdm_notebook.pandas(desc=\"Progress\")\n","import torch\n","import torch.nn as nn\n","import numpy as np\n","from sklearn.model_selection import train_test_split\n","import torch.utils.data as data_utils\n","\n","import html as ihtml\n","from bs4 import BeautifulSoup\n"],"execution_count":17,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:3: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n","Please use `tqdm.notebook.*` instead of `tqdm._tqdm_notebook.*`\n","  This is separate from the ipykernel package so we can avoid doing imports until\n","/usr/local/lib/python3.7/dist-packages/tqdm/std.py:658: FutureWarning: The Panel class is removed from pandas. Accessing it from the top-level namespace will also be removed in the next version\n","  from pandas import Panel\n"],"name":"stderr"}]},{"cell_type":"markdown","metadata":{"id":"TSB6DzZ2N_w6"},"source":["# 2.1 Dataset Loading"]},{"cell_type":"code","metadata":{"id":"tInW7CfxM0Rh","executionInfo":{"status":"ok","timestamp":1619698805148,"user_tz":-120,"elapsed":21932,"user":{"displayName":"Sara Fattouh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjGrdvZo2x_8K8abskRubIgMK3SdtS4cG-aQE01nA=s64","userId":"09360594903764573322"}}},"source":["#image_path = '/content/Rakuten/image/'\n","data_path = '/content/Rakuten/data'\n","text_data_path = data_path"],"execution_count":18,"outputs":[]},{"cell_type":"code","metadata":{"id":"cEwqxe9zaInz","executionInfo":{"status":"ok","timestamp":1619698805149,"user_tz":-120,"elapsed":21931,"user":{"displayName":"Sara Fattouh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjGrdvZo2x_8K8abskRubIgMK3SdtS4cG-aQE01nA=s64","userId":"09360594903764573322"}}},"source":["class SigirPreprocess():\n","\n","  def __init__(self, text_data_path):\n","    \n","        self.text_data_path = text_data_path\n","        self.train = None # Merged X_train and Y_train\n","        self.dict_code_to_id = {}\n","        self.dict_id_to_code = {}\n","        self.list_tags = {} #unique type code\n","        self.sentences = []\n","        self.labels = []\n","        self.text_col = None\n","        self.X_test = None\n","\n","  def prepare_data(self):\n","        \n","        #loading the Merged, preprocessed text data and test data\n","        train = pd.read_csv(self.text_data_path+\"/NewTraining.csv\")\n","        # new_train =  train[train['Description'] != \" \"]\n","        # new_train = new_train[new_train['Description'].notna()]\n","        self.train = train\n","\n","        \n","  def get_sentences(self, text_col, remove_null_rows=True):\n","\n","       #get values of a specific column\n","        self.text_col = text_col        \n","\n","        new_train = self.train.copy()  \n","        self.sentences = new_train[text_col].values\n","        self.labels = new_train['labels'].values\n","\n","\n","  # def prepare_test(self, text_col):\n","    \n","  #       X_test = pd.read_csv(self.text_data_path + \"/NewTest.csv\")\n","  #       self.X_test = X_test\n","  #       X_test['title_desc'] = X_test['Title'] + \" \" + X_test['Description']\n","  #       self.test_sentences = X_test[text_col].values\n","  #       return self.test_sentences\n","        "],"execution_count":19,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"zw0AwwTuCUAJ","executionInfo":{"status":"ok","timestamp":1619698806459,"user_tz":-120,"elapsed":23233,"user":{"displayName":"Sara Fattouh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjGrdvZo2x_8K8abskRubIgMK3SdtS4cG-aQE01nA=s64","userId":"09360594903764573322"}},"outputId":"8bf4db88-39b7-4580-c626-17b8a7b930d1"},"source":["Preprocess = SigirPreprocess(data_path)\n","Preprocess.prepare_data()\n","\n","\n","print(\"Numner of records:  \",len(Preprocess.train))\n","print(\"Numner of corresponding Labels:  \",len(Preprocess.labels))\n"],"execution_count":20,"outputs":[{"output_type":"stream","text":["Numner of records:   55025\n","Numner of corresponding Labels:   0\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"kmiL8Cmobgpj","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1619698807298,"user_tz":-120,"elapsed":24063,"user":{"displayName":"Sara Fattouh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjGrdvZo2x_8K8abskRubIgMK3SdtS4cG-aQE01nA=s64","userId":"09360594903764573322"}},"outputId":"2411276f-fce1-49dd-de37-b172a44d1807"},"source":["#Load train and test data (test for specific column)\n","\n","text_col = 'Description'\n","\n","max_len = 256\n","\n","num_classes = 27\n","\n","Preprocess = SigirPreprocess(text_data_path)\n","Preprocess.prepare_data()\n","train = Preprocess.train\n","print(\"Trian:  \", len(Preprocess.train))\n","\n","\n","Preprocess.get_sentences(text_col)\n","print(\"Labels: \", len(Preprocess.labels))\n","\n","# X_test = Preprocess.prepare_test(text_col)\n","# print(\"Test:   \", len(Preprocess.X_test))"],"execution_count":21,"outputs":[{"output_type":"stream","text":["Trian:   55025\n","Labels:  55025\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"FWIvd9WLOGmi"},"source":["# 2.2 Drop Records With No Description"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"QWf4x6KXpiLG","executionInfo":{"status":"ok","timestamp":1619698808423,"user_tz":-120,"elapsed":25179,"user":{"displayName":"Sara Fattouh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjGrdvZo2x_8K8abskRubIgMK3SdtS4cG-aQE01nA=s64","userId":"09360594903764573322"}},"outputId":"c87b2e0b-162b-40bb-d4c0-9bb80527ca75"},"source":["Preprocess = SigirPreprocess(text_data_path)\n","Preprocess.prepare_data()\n","\n","print(\"Numner of records:  \",len(Preprocess.train))\n","print(\"Numner of corresponding Labels:  \",len(Preprocess.labels))\n"],"execution_count":22,"outputs":[{"output_type":"stream","text":["Numner of records:   55025\n","Numner of corresponding Labels:   0\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"MfwN_9ZctYCu","executionInfo":{"status":"ok","timestamp":1619698808424,"user_tz":-120,"elapsed":25169,"user":{"displayName":"Sara Fattouh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjGrdvZo2x_8K8abskRubIgMK3SdtS4cG-aQE01nA=s64","userId":"09360594903764573322"}},"outputId":"33a63a74-4145-4528-b6d6-7f2ef559aba1"},"source":["new_train =  Preprocess.train[Preprocess.train['Description'] != \" \"]\n","Preprocess.train = new_train\n","Preprocess.labels = new_train['labels'].values\n","\n","print(\"Numner of records without Null Description:  \",len(Preprocess.train))\n","print(\"Numner of corresponding Labels:  \",len(Preprocess.labels))"],"execution_count":23,"outputs":[{"output_type":"stream","text":["Numner of records without Null Description:   55025\n","Numner of corresponding Labels:   55025\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":0},"id":"mvJTsYxaDrnR","executionInfo":{"status":"ok","timestamp":1619698808425,"user_tz":-120,"elapsed":25161,"user":{"displayName":"Sara Fattouh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjGrdvZo2x_8K8abskRubIgMK3SdtS4cG-aQE01nA=s64","userId":"09360594903764573322"}},"outputId":"cccb43f2-e414-4b49-ba6e-9287227a8e33"},"source":["Preprocess.train\n"],"execution_count":24,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Unnamed: 0</th>\n","      <th>Unnamed: 0.1</th>\n","      <th>Integer_id</th>\n","      <th>Title</th>\n","      <th>Description</th>\n","      <th>Image_id</th>\n","      <th>Product_id</th>\n","      <th>Prdtypecode</th>\n","      <th>labels</th>\n","      <th>product</th>\n","      <th>title_len</th>\n","      <th>desc_len</th>\n","      <th>title_desc_len</th>\n","      <th>title_desc</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0</td>\n","      <td>2</td>\n","      <td>2</td>\n","      <td>Grand Stylet Ergonomique Bleu Gamepad Nintendo...</td>\n","      <td>PILOT STYLE Touch Pen de marque Speedlink est ...</td>\n","      <td>938777978</td>\n","      <td>201115110</td>\n","      <td>50</td>\n","      <td>2</td>\n","      <td>Entertainment</td>\n","      <td>12</td>\n","      <td>109</td>\n","      <td>121</td>\n","      <td>Grand Stylet Ergonomique Bleu Gamepad Nintendo...</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>1</td>\n","      <td>4</td>\n","      <td>4</td>\n","      <td>La Guerre Des Tuques</td>\n","      <td>Luc a des idées de grandeur. Il veut organiser...</td>\n","      <td>1077757786</td>\n","      <td>278535884</td>\n","      <td>2705</td>\n","      <td>4</td>\n","      <td>Books</td>\n","      <td>4</td>\n","      <td>34</td>\n","      <td>38</td>\n","      <td>La Guerre Des Tuques Luc a des id&amp;eacute;es de...</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>2</td>\n","      <td>7</td>\n","      <td>7</td>\n","      <td>Conquérant Sept Cahier Couverture Polypro 240 ...</td>\n","      <td>CONQUERANT CLASSIQUE Cahier 240 x 320 mm seyès...</td>\n","      <td>999581347</td>\n","      <td>344240059</td>\n","      <td>2522</td>\n","      <td>5</td>\n","      <td>Books</td>\n","      <td>14</td>\n","      <td>18</td>\n","      <td>32</td>\n","      <td>Conquérant Sept Cahier Couverture Polypro 240 ...</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>3</td>\n","      <td>9</td>\n","      <td>9</td>\n","      <td>Tente Pliante V3s5-Pro Pvc Blanc - 3 X 4m50 - ...</td>\n","      <td>Tente pliante V3S5 Pro PVC 500 gr/m² - 3 x 4m5...</td>\n","      <td>1245644185</td>\n","      <td>3793572222</td>\n","      <td>2582</td>\n","      <td>6</td>\n","      <td>Household</td>\n","      <td>19</td>\n","      <td>293</td>\n","      <td>312</td>\n","      <td>Tente Pliante V3s5-Pro Pvc Blanc - 3 X 4m50 - ...</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>4</td>\n","      <td>10</td>\n","      <td>10</td>\n","      <td>Eames Inspired Sxw Chair - Pink - Black</td>\n","      <td>The timeless DSW seat can now be paired with m...</td>\n","      <td>1111840281</td>\n","      <td>1915836983</td>\n","      <td>1560</td>\n","      <td>7</td>\n","      <td>Household</td>\n","      <td>8</td>\n","      <td>94</td>\n","      <td>102</td>\n","      <td>Eames Inspired Sxw Chair - Pink - Black The ti...</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>55020</th>\n","      <td>55111</td>\n","      <td>84908</td>\n","      <td>84908</td>\n","      <td>Dimmerable Usb Led Lampe De Bureau Réglable Ch...</td>\n","      <td>Nom de la marque:oobestAmpoules incluses:OuiCe...</td>\n","      <td>1313620762</td>\n","      <td>4198481300</td>\n","      <td>2060</td>\n","      <td>17</td>\n","      <td>Household</td>\n","      <td>18</td>\n","      <td>37</td>\n","      <td>55</td>\n","      <td>Dimmerable Usb Led Lampe De Bureau Réglable Ch...</td>\n","    </tr>\n","    <tr>\n","      <th>55021</th>\n","      <td>55112</td>\n","      <td>84909</td>\n","      <td>84909</td>\n","      <td>espa - kit complet de nage à contre courant 39...</td>\n","      <td>espa espa - kit complet de nage à contre coura...</td>\n","      <td>1043841028</td>\n","      <td>853455937</td>\n","      <td>2583</td>\n","      <td>12</td>\n","      <td>Household</td>\n","      <td>17</td>\n","      <td>173</td>\n","      <td>190</td>\n","      <td>espa - kit complet de nage à contre courant 39...</td>\n","    </tr>\n","    <tr>\n","      <th>55022</th>\n","      <td>55113</td>\n","      <td>84910</td>\n","      <td>84910</td>\n","      <td>Vêtements Pour Animaux Mode Style Chiens Rayé ...</td>\n","      <td>le t - shirt rayé mode chiens  petits chiots v...</td>\n","      <td>1158527239</td>\n","      <td>2699568414</td>\n","      <td>2220</td>\n","      <td>22</td>\n","      <td>Household</td>\n","      <td>12</td>\n","      <td>168</td>\n","      <td>180</td>\n","      <td>Vêtements Pour Animaux Mode Style Chiens Rayé ...</td>\n","    </tr>\n","    <tr>\n","      <th>55023</th>\n","      <td>55114</td>\n","      <td>84912</td>\n","      <td>84912</td>\n","      <td>Kit piscine acier NEVADA déco pierre Ø 3.50m x...</td>\n","      <td>Description complète :Kit piscine hors-sol Toi...</td>\n","      <td>1188462883</td>\n","      <td>3065095706</td>\n","      <td>2583</td>\n","      <td>12</td>\n","      <td>Household</td>\n","      <td>10</td>\n","      <td>190</td>\n","      <td>200</td>\n","      <td>Kit piscine acier NEVADA déco pierre Ø 3.50m x...</td>\n","    </tr>\n","    <tr>\n","      <th>55024</th>\n","      <td>55115</td>\n","      <td>84914</td>\n","      <td>84914</td>\n","      <td>Table Basse Bois De Récupération Massif Base B...</td>\n","      <td>Cette table basse a un design unique et consti...</td>\n","      <td>1267353403</td>\n","      <td>3942400296</td>\n","      <td>1560</td>\n","      <td>7</td>\n","      <td>Household</td>\n","      <td>9</td>\n","      <td>262</td>\n","      <td>271</td>\n","      <td>Table Basse Bois De Récupération Massif Base B...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>55025 rows × 14 columns</p>\n","</div>"],"text/plain":["       Unnamed: 0  ...                                         title_desc\n","0               0  ...  Grand Stylet Ergonomique Bleu Gamepad Nintendo...\n","1               1  ...  La Guerre Des Tuques Luc a des id&eacute;es de...\n","2               2  ...  Conquérant Sept Cahier Couverture Polypro 240 ...\n","3               3  ...  Tente Pliante V3s5-Pro Pvc Blanc - 3 X 4m50 - ...\n","4               4  ...  Eames Inspired Sxw Chair - Pink - Black The ti...\n","...           ...  ...                                                ...\n","55020       55111  ...  Dimmerable Usb Led Lampe De Bureau Réglable Ch...\n","55021       55112  ...  espa - kit complet de nage à contre courant 39...\n","55022       55113  ...  Vêtements Pour Animaux Mode Style Chiens Rayé ...\n","55023       55114  ...  Kit piscine acier NEVADA déco pierre Ø 3.50m x...\n","55024       55115  ...  Table Basse Bois De Récupération Massif Base B...\n","\n","[55025 rows x 14 columns]"]},"metadata":{"tags":[]},"execution_count":24}]},{"cell_type":"markdown","metadata":{"id":"E-fmFSVmORg9"},"source":["# 2.3. Stripping Records Description\n","\n","Remove HTML tags from the Desciption field using the BeautifulSoap library."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"SFpx_FWEsTuz","executionInfo":{"status":"ok","timestamp":1619698826287,"user_tz":-120,"elapsed":43013,"user":{"displayName":"Sara Fattouh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjGrdvZo2x_8K8abskRubIgMK3SdtS4cG-aQE01nA=s64","userId":"09360594903764573322"}},"outputId":"9cf9b9d5-8f0d-4fb1-8c16-581bcb84d148"},"source":["from bs4 import BeautifulSoup\n","Preprocess.train['Description'] = [BeautifulSoup(text).get_text() for text in  Preprocess.train['Description'] ]\n","Preprocess.train['Title'] = [BeautifulSoup(text).get_text() for text in  Preprocess.train['Title'] ]"],"execution_count":25,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/bs4/__init__.py:336: UserWarning: \"https://placehold.it/100x70\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n","  ' that document to Beautiful Soup.' % decoded_markup\n","/usr/local/lib/python3.7/dist-packages/bs4/__init__.py:336: UserWarning: \"https://www.pro-bems.com/IMAGES/images_1/FIGJJCT0000117/m/FIGJJCT0000117_5.jpg\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n","  ' that document to Beautiful Soup.' % decoded_markup\n","/usr/local/lib/python3.7/dist-packages/bs4/__init__.py:273: UserWarning: \"b'.'\" looks like a filename, not markup. You should probably open this file and pass the filehandle into Beautiful Soup.\n","  ' Beautiful Soup.' % markup)\n","/usr/local/lib/python3.7/dist-packages/bs4/__init__.py:336: UserWarning: \"https://www.pro-bems.com/IMAGES/images_1/FIG83X17001/m/FIG83X17001_5.jpg\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n","  ' that document to Beautiful Soup.' % decoded_markup\n","/usr/local/lib/python3.7/dist-packages/bs4/__init__.py:336: UserWarning: \"https://www.pro-bems.com/IMAGES/images/BOOKPNLIGMAG19/m/BOOKPNLIGMAG19_5.jpg\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n","  ' that document to Beautiful Soup.' % decoded_markup\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"BZDL4R3Stxwf","executionInfo":{"status":"ok","timestamp":1619698826293,"user_tz":-120,"elapsed":43010,"user":{"displayName":"Sara Fattouh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjGrdvZo2x_8K8abskRubIgMK3SdtS4cG-aQE01nA=s64","userId":"09360594903764573322"}},"outputId":"37f824bc-fd6f-46ad-d37e-f858d4cf1200"},"source":["print(len(Preprocess.train))"],"execution_count":26,"outputs":[{"output_type":"stream","text":["55025\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":0},"id":"GCgeyPh7t6GS","executionInfo":{"status":"ok","timestamp":1619698826294,"user_tz":-120,"elapsed":43000,"user":{"displayName":"Sara Fattouh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjGrdvZo2x_8K8abskRubIgMK3SdtS4cG-aQE01nA=s64","userId":"09360594903764573322"}},"outputId":"5e13b7c8-4fa4-4956-f924-2e71733b8c89"},"source":["Preprocess.train"],"execution_count":27,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Unnamed: 0</th>\n","      <th>Unnamed: 0.1</th>\n","      <th>Integer_id</th>\n","      <th>Title</th>\n","      <th>Description</th>\n","      <th>Image_id</th>\n","      <th>Product_id</th>\n","      <th>Prdtypecode</th>\n","      <th>labels</th>\n","      <th>product</th>\n","      <th>title_len</th>\n","      <th>desc_len</th>\n","      <th>title_desc_len</th>\n","      <th>title_desc</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0</td>\n","      <td>2</td>\n","      <td>2</td>\n","      <td>Grand Stylet Ergonomique Bleu Gamepad Nintendo...</td>\n","      <td>PILOT STYLE Touch Pen de marque Speedlink est ...</td>\n","      <td>938777978</td>\n","      <td>201115110</td>\n","      <td>50</td>\n","      <td>2</td>\n","      <td>Entertainment</td>\n","      <td>12</td>\n","      <td>109</td>\n","      <td>121</td>\n","      <td>Grand Stylet Ergonomique Bleu Gamepad Nintendo...</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>1</td>\n","      <td>4</td>\n","      <td>4</td>\n","      <td>La Guerre Des Tuques</td>\n","      <td>Luc a des idées de grandeur. Il veut organiser...</td>\n","      <td>1077757786</td>\n","      <td>278535884</td>\n","      <td>2705</td>\n","      <td>4</td>\n","      <td>Books</td>\n","      <td>4</td>\n","      <td>34</td>\n","      <td>38</td>\n","      <td>La Guerre Des Tuques Luc a des id&amp;eacute;es de...</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>2</td>\n","      <td>7</td>\n","      <td>7</td>\n","      <td>Conquérant Sept Cahier Couverture Polypro 240 ...</td>\n","      <td>CONQUERANT CLASSIQUE Cahier 240 x 320 mm seyès...</td>\n","      <td>999581347</td>\n","      <td>344240059</td>\n","      <td>2522</td>\n","      <td>5</td>\n","      <td>Books</td>\n","      <td>14</td>\n","      <td>18</td>\n","      <td>32</td>\n","      <td>Conquérant Sept Cahier Couverture Polypro 240 ...</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>3</td>\n","      <td>9</td>\n","      <td>9</td>\n","      <td>Tente Pliante V3s5-Pro Pvc Blanc - 3 X 4m50 - ...</td>\n","      <td>Tente pliante V3S5 Pro PVC 500 gr/m² - 3 x 4m5...</td>\n","      <td>1245644185</td>\n","      <td>3793572222</td>\n","      <td>2582</td>\n","      <td>6</td>\n","      <td>Household</td>\n","      <td>19</td>\n","      <td>293</td>\n","      <td>312</td>\n","      <td>Tente Pliante V3s5-Pro Pvc Blanc - 3 X 4m50 - ...</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>4</td>\n","      <td>10</td>\n","      <td>10</td>\n","      <td>Eames Inspired Sxw Chair - Pink - Black</td>\n","      <td>The timeless DSW seat can now be paired with m...</td>\n","      <td>1111840281</td>\n","      <td>1915836983</td>\n","      <td>1560</td>\n","      <td>7</td>\n","      <td>Household</td>\n","      <td>8</td>\n","      <td>94</td>\n","      <td>102</td>\n","      <td>Eames Inspired Sxw Chair - Pink - Black The ti...</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>55020</th>\n","      <td>55111</td>\n","      <td>84908</td>\n","      <td>84908</td>\n","      <td>Dimmerable Usb Led Lampe De Bureau Réglable Ch...</td>\n","      <td>Nom de la marque:oobestAmpoules incluses:OuiCe...</td>\n","      <td>1313620762</td>\n","      <td>4198481300</td>\n","      <td>2060</td>\n","      <td>17</td>\n","      <td>Household</td>\n","      <td>18</td>\n","      <td>37</td>\n","      <td>55</td>\n","      <td>Dimmerable Usb Led Lampe De Bureau Réglable Ch...</td>\n","    </tr>\n","    <tr>\n","      <th>55021</th>\n","      <td>55112</td>\n","      <td>84909</td>\n","      <td>84909</td>\n","      <td>espa - kit complet de nage à contre courant 39...</td>\n","      <td>espa espa - kit complet de nage à contre coura...</td>\n","      <td>1043841028</td>\n","      <td>853455937</td>\n","      <td>2583</td>\n","      <td>12</td>\n","      <td>Household</td>\n","      <td>17</td>\n","      <td>173</td>\n","      <td>190</td>\n","      <td>espa - kit complet de nage à contre courant 39...</td>\n","    </tr>\n","    <tr>\n","      <th>55022</th>\n","      <td>55113</td>\n","      <td>84910</td>\n","      <td>84910</td>\n","      <td>Vêtements Pour Animaux Mode Style Chiens Rayé ...</td>\n","      <td>le t - shirt rayé mode chiens  petits chiots v...</td>\n","      <td>1158527239</td>\n","      <td>2699568414</td>\n","      <td>2220</td>\n","      <td>22</td>\n","      <td>Household</td>\n","      <td>12</td>\n","      <td>168</td>\n","      <td>180</td>\n","      <td>Vêtements Pour Animaux Mode Style Chiens Rayé ...</td>\n","    </tr>\n","    <tr>\n","      <th>55023</th>\n","      <td>55114</td>\n","      <td>84912</td>\n","      <td>84912</td>\n","      <td>Kit piscine acier NEVADA déco pierre Ø 3.50m x...</td>\n","      <td>Description complète :Kit piscine hors-sol Toi...</td>\n","      <td>1188462883</td>\n","      <td>3065095706</td>\n","      <td>2583</td>\n","      <td>12</td>\n","      <td>Household</td>\n","      <td>10</td>\n","      <td>190</td>\n","      <td>200</td>\n","      <td>Kit piscine acier NEVADA déco pierre Ø 3.50m x...</td>\n","    </tr>\n","    <tr>\n","      <th>55024</th>\n","      <td>55115</td>\n","      <td>84914</td>\n","      <td>84914</td>\n","      <td>Table Basse Bois De Récupération Massif Base B...</td>\n","      <td>Cette table basse a un design unique et consti...</td>\n","      <td>1267353403</td>\n","      <td>3942400296</td>\n","      <td>1560</td>\n","      <td>7</td>\n","      <td>Household</td>\n","      <td>9</td>\n","      <td>262</td>\n","      <td>271</td>\n","      <td>Table Basse Bois De Récupération Massif Base B...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>55025 rows × 14 columns</p>\n","</div>"],"text/plain":["       Unnamed: 0  ...                                         title_desc\n","0               0  ...  Grand Stylet Ergonomique Bleu Gamepad Nintendo...\n","1               1  ...  La Guerre Des Tuques Luc a des id&eacute;es de...\n","2               2  ...  Conquérant Sept Cahier Couverture Polypro 240 ...\n","3               3  ...  Tente Pliante V3s5-Pro Pvc Blanc - 3 X 4m50 - ...\n","4               4  ...  Eames Inspired Sxw Chair - Pink - Black The ti...\n","...           ...  ...                                                ...\n","55020       55111  ...  Dimmerable Usb Led Lampe De Bureau Réglable Ch...\n","55021       55112  ...  espa - kit complet de nage à contre courant 39...\n","55022       55113  ...  Vêtements Pour Animaux Mode Style Chiens Rayé ...\n","55023       55114  ...  Kit piscine acier NEVADA déco pierre Ø 3.50m x...\n","55024       55115  ...  Table Basse Bois De Récupération Massif Base B...\n","\n","[55025 rows x 14 columns]"]},"metadata":{"tags":[]},"execution_count":27}]},{"cell_type":"code","metadata":{"id":"Slwz1OAjub_4","executionInfo":{"status":"ok","timestamp":1619698826296,"user_tz":-120,"elapsed":42999,"user":{"displayName":"Sara Fattouh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjGrdvZo2x_8K8abskRubIgMK3SdtS4cG-aQE01nA=s64","userId":"09360594903764573322"}}},"source":["text_col = 'Description'\n","max_len = 256\n","\n","Preprocess.get_sentences(text_col,True) #here we are gettig the labels\n","sentences = Preprocess.sentences\n","labels = Preprocess.labels"],"execution_count":28,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"X5rgrhFO25Q6","executionInfo":{"status":"ok","timestamp":1619698826297,"user_tz":-120,"elapsed":42991,"user":{"displayName":"Sara Fattouh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjGrdvZo2x_8K8abskRubIgMK3SdtS4cG-aQE01nA=s64","userId":"09360594903764573322"}},"outputId":"b499b9c2-0c38-4584-eef2-ff8fd0d64feb"},"source":["sentences"],"execution_count":29,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([\"PILOT STYLE Touch Pen de marque Speedlink est 1 stylet ergonomique pour GamePad Nintendo Wii U. Pour un confort optimal et une précision maximale sur le GamePad de la Wii U: ce grand stylet hautement ergonomique est non seulement parfaitement adapté à votre main mais aussi très élégant. Il est livré avec un support qui se fixe sans adhésif à l'arrière du GamePad  Caractéristiques: Modèle: Speedlink PILOT STYLE Touch Pen Couleur: Bleu Ref. Fabricant: SL-3468-BE Compatibilité: GamePad Nintendo Wii U Forme particulièrement ergonomique excellente tenue en main Pointe à revêtement longue durée conçue pour ne pas abîmer l'écran tactile En bonus : Support inclu pour GamePad \",\n","       \"Luc a des idées de grandeur. Il veut organiser un jeu de guerre de boules de neige et s'arranger pour en être le vainqueur incontesté. Mais Sophie s'en mêle et chambarde tous ses plans...\",\n","       'CONQUERANT CLASSIQUE Cahier 240 x 320 mm seyès incolorecouverture en Polypro 96 pages agrafé papier de 90 g/m2(400006764)',\n","       ...,\n","       'le t - shirt rayé mode chiens  petits chiots vêtementsnote: veuillez comparer le détail tailles avec toi avant d acheter.!!utiliser les mêmesles vêtements de comparer avec la taille.description:100% brand new la qualité élevéequantité: 1matériel: cotonmotif: levotre chien est plus élégante coolparfait pour la marche  joggingattention: comme différents ordinateurs afficher les couleurs différemment  la couleur de la poste peut varier légèrement d images ci - dessus  merci pour votre compréhension.toutes les dimensions sont mesurées à la main  il y a peut - être 2-3cm déviations  merci pour ta compréhensiontaille des détails:taille: scou: 24 cm / 9 h 45  dos: 23cm / 9.06  bust: 34cm / 13.39  taille: m cou: 28 / 11.02  dos: isbn / 10 63  bust: 40 cm / 15.75taille: lcou: 34cm / 13.39  retour: 31cm / 12.20  buste: 44 / 17.32 potaille: xlcou: 38 cm / 14.96  : 37cm / 14.57 pobust: 51cm / 20.08  taille: xxlcou: 42cm / 16.54 po: 41cm / 16.14  buste: 60 cm / 23.62 poforfait comprend:le t - shirt 1pcs pet',\n","       'Description complète :Kit piscine hors-sol Toi PIEDRA GRIS ronde Ø 3.50m hauteur 0.90m. Parois acier liner 30/100eme uni bleu revêtement breveté exclusif imitant la pierre échelle profilés en PVC. Kit piscine complet.Caractéristiques détaillées :- Forme : Ronde- Type : Kit piscine acier hors-sol- Dimensions extérieures : Ø3.50 x 0.90m- Surface installation : 3.60m x 3.60m- Hauteur avec margelle : 0.90m- Utilisation : Hors-sol- Capacité : 8m3- Kit complet : Oui- Liner : Uni bleu 30/100e avec fixation overlap- Largeur des margelles : Sans- Structure : Parois acier anti-corrosion- Epaisseur des parois : Acier 35/100eme- Jambes de force : Sans jambes de forces apparentes- Revêtement extérieur : Parois laquées avec décoration pierre \"Stone Effect\"- Type de filtration : Filtration à cartouche- Débit : 2m³ / heure- Pompe : 46W- Garantie structure : 2 ans- Garantie liner : 2 ans- Garantie filtration : 2 ans- Echelle : Symétrique acier 2 marches- Notice de montage : Oui- Livraison : 1 palette- Garantie : 2 ans',\n","       \"Cette table basse a un design unique et constituera un ajout intemporel à votre maison. Son dessus de table en bois massif est idéal pour ranger vos boissons panier de fruits ou objets décoratifs et sa base en acier solide ajoute à la robustesse de la table d'appoint. La table basse est faite de bois de récupération massif provenant de solives de planchers et de poutres de soutien de vieux bâtiments en cours de démolition et peut être composée de différents types de bois comme le Sesham (bois de rose) le pin le teck le hêtre le chêne le cèdre le bois de manguier l'acacia etc. Cela signifie que le bois de récupération conserve les caractéristiques de ces différents types de bois. Le bois récupéré est déjà vieilli patiné et séché de sorte qu'il ne rétrécit pas ne se plie pas et n'a pas besoin d'une finition. Chaque étape du processus est réalisée avec le plus grand soin que ce soit le ponçage la peinture ou le laquage. Les belles fibres de bois rendent chaque meuble unique et légèrement différent du suivant. Les signes d'usure et la structure fibreuse visible donnent à chaque pièce son histoire et un aspect unique. L'article est déjà assemblé ; aucun assemblage n'est requis. Remarque importante : les couleurs varient d'un meuble à l'autre rendant chacune de nos tables basses unique la livraison est aléatoire. Couleur de base : BlancMatériau : dessus de table en bois massif de récupération + base en acierDimensions : 60 x 33 cm (Diam. x H)Produit poncé peint et laquéAucun assemblage requis\"],\n","      dtype=object)"]},"metadata":{"tags":[]},"execution_count":29}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"VLFKhuaCjLZH","executionInfo":{"status":"ok","timestamp":1619698826298,"user_tz":-120,"elapsed":42984,"user":{"displayName":"Sara Fattouh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjGrdvZo2x_8K8abskRubIgMK3SdtS4cG-aQE01nA=s64","userId":"09360594903764573322"}},"outputId":"0ab56d7e-c07c-4111-8d60-03ff75912981"},"source":["print(len(Preprocess.train))\n","print(\"Numner of corresponding Labels:  \",len(Preprocess.labels))"],"execution_count":30,"outputs":[{"output_type":"stream","text":["55025\n","Numner of corresponding Labels:   55025\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"geLZwoV4vBgp","executionInfo":{"status":"ok","timestamp":1619698826299,"user_tz":-120,"elapsed":42983,"user":{"displayName":"Sara Fattouh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjGrdvZo2x_8K8abskRubIgMK3SdtS4cG-aQE01nA=s64","userId":"09360594903764573322"}}},"source":["# Preprocess.train.to_csv('MyTraining.csv')"],"execution_count":31,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"rkfQMxR7LbZW"},"source":["# 3. Tokenization & Input Formatting\n","Transform our dataset into the format that BERT can be trained on."]},{"cell_type":"markdown","metadata":{"id":"VkufSK_fOvdq"},"source":["# 3.1. BERT Tokenizer\n","\n","To feed our text to BERT, it must be split into tokens, and then these tokens must be mapped to their index in the tokenizer vocabulary.\n","\n","The tokenization must be performed by the tokenizer included within BERT"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":132,"referenced_widgets":["b3c0be48df834878a7d8861d152fdf4b","5468c6ea55e2453d8115d79cbb5be05d","339bc25b7bb64246a87531597c9f4f0c","06139fcdd8c6436db3cb698fab8ed276","73396b62c3be4d71aaff0be18061fd2d","18368b847ae74455ad1d49304f7cbc35","8bc35773f3a34de0b891c5bd655d594b","0033b6ab2bea41608a312fde6a2fdf5c","eccfaeeddd6e4069a0532375c923f663","9d7fe9c8cd8d48b0b31ebd697e0c0c18","5776f32da7314543a1446a66c505a3a4","3a5eda55fba9421a96e38b9f70760b6f","b0ea351dddaf4656a04d612af7e6b763","c13aa24c2a6345b3bc5619d774af23bd","05c39f17dfcb43ed93f171d75a1f8339","ac0d26427a014ba4968fac4a4427d3d2"]},"id":"i3ljYsil4E8y","executionInfo":{"status":"ok","timestamp":1619698828620,"user_tz":-120,"elapsed":45295,"user":{"displayName":"Sara Fattouh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjGrdvZo2x_8K8abskRubIgMK3SdtS4cG-aQE01nA=s64","userId":"09360594903764573322"}},"outputId":"72d01fc4-c0cb-4da5-b7be-865cecd5bd8b"},"source":["from transformers import CamembertConfig, CamembertTokenizer, CamembertModel, CamembertForSequenceClassification, AdamW\n","#from transformers.modeling_roberta import RobertaClassificationHead\n","print('Using Camembert')\n","modelname = 'camembert-base'\n","tokenizer = CamembertTokenizer.from_pretrained(modelname, do_lowercase=False)"],"execution_count":32,"outputs":[{"output_type":"stream","text":["Using Camembert\n"],"name":"stdout"},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"b3c0be48df834878a7d8861d152fdf4b","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=0.0, description='Downloading', max=810912.0, style=ProgressStyle(descripti…"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["\n"],"name":"stdout"},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"eccfaeeddd6e4069a0532375c923f663","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=0.0, description='Downloading', max=1395301.0, style=ProgressStyle(descript…"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"0KJf2pHgLTt_"},"source":["# 3.2. Required Formatting\n","We are required to give it a number of pieces of information\n","\n","We need to:\n","\n","1.   Add special tokens to the start and end of each sentence.\n","2.   Pad & truncate all sentences to a single constant length.\n","3. Explicitly differentiate real tokens from padding tokens with the “attention mask”.\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"qAJZvJjsQMbJ"},"source":["# 3.3. Tokenize Dataset\n","\n","We will use \"encode_plus\":\n","\n","returns a dictionary containing the encoded sequence or sequence pair and additional information:\n","the mask for sequence classification and the overflowing elements if a max_length is specified."]},{"cell_type":"code","metadata":{"id":"7ZPWDpSH-WsQ","executionInfo":{"status":"ok","timestamp":1619698828621,"user_tz":-120,"elapsed":45295,"user":{"displayName":"Sara Fattouh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjGrdvZo2x_8K8abskRubIgMK3SdtS4cG-aQE01nA=s64","userId":"09360594903764573322"}}},"source":["#function to prepare input for model training\n","def prep_input(sentences,labels, max_len):\n","    input_ids = []\n","    attention_masks = []\n","\n","    # For every sentence...\n","    for sent in sentences: \n","        # `encode_plus` will:\n","        #   (1) Tokenize the sentence.\n","        #   (2) Prepend the `[CLS]` token to the start.\n","        #   (3) Append the `[SEP]` token to the end.\n","        #   (4) Map tokens to their IDs.\n","        #   (5) Pad or truncate the sentence to `max_length`\n","        #   (6) Create attention masks for [PAD] tokens.\n","        encoded_dict = tokenizer.encode_plus(\n","                            sent,                      # Sentence to encode.\n","                            add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n","                            max_length = max_len,      # Pad & truncate all sentences.\n","                            pad_to_max_length = True,\n","                            return_attention_mask = True, # Construct attn. masks.\n","                            return_tensors = 'pt',        # Return pytorch tensors.\n","                       )\n","\n","        # Add the encoded sentence to the list.    \n","        input_ids.append(encoded_dict['input_ids'])\n","\n","        # And its attention mask (simply differentiates padding from non-padding).\n","        attention_masks.append(encoded_dict['attention_mask'])\n","\n","    # Convert the lists into tensors.\n","    input_ids = torch.cat(input_ids, dim=0)\n","    attention_masks = torch.cat(attention_masks, dim=0)\n","    \n","    if labels is not None:\n","        labels = torch.tensor(labels)\n","        return input_ids,attention_masks,labels\n","    else:\n","        return input_ids,attention_masks"],"execution_count":33,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"TzMGwdS6Pred"},"source":["**Apply Tokenizer on a single sentence.**\n","\n","\n","\n"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"FcB3PBDJ-ePR","executionInfo":{"status":"ok","timestamp":1619698872628,"user_tz":-120,"elapsed":89294,"user":{"displayName":"Sara Fattouh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjGrdvZo2x_8K8abskRubIgMK3SdtS4cG-aQE01nA=s64","userId":"09360594903764573322"}},"outputId":"71ce0379-2ed7-49c7-e2bd-25c9feab3d5c"},"source":["input_ids, attention_masks, labels = prep_input(sentences,labels, max_len=max_len) #max_len=256, impact training and evaluation speed\n","print('Original: ', sentences[4])\n","\n","#map tokens to their index in the tokenizer vocabulary\n","print('Token IDs:', input_ids[4])\n","\n","#explicitly differentiate real tokens from padding tokens with the “attention mask”. \"0\" or \"1\"\n","print('Attention Masks',attention_masks[4]) "],"execution_count":34,"outputs":[{"output_type":"stream","text":["Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"],"name":"stderr"},{"output_type":"stream","text":["Original:  The timeless DSW seat can now be paired with more types of legs! These wood legs look like a natural fit for the seat and will make a safe choice for any interior. This chair will be perfect  in offices kitchens and dining rooms.  Ideal for : kitchen Living room office waiting room or receptionAvailable Colours : white red blue baby blue black yellow orange olive dark orange chocolate light green warm grey beige light grey blue ocean pink fushia green tree graphite tarmac and royal blueOther Features : avalaible with or without amrests.\n","Token IDs: tensor([    5,   908, 12889, 14156, 28828,    48,   622,  3910,    49,  3168,\n","         2446,  4627,   204,  1466, 10697,  1756,   786,    16, 12014,   152,\n","          908,    10,    35,    21,  8563,    16, 12014,  3770, 16396,    33,\n","           21, 22997,  2039,  1782,   808,    48,   622,  1168, 14233, 18018,\n","           33,    77,  3270,  8626,  4850,  1782,    33,  2224,  1361, 14102,\n","            9, 17526,  5572, 14233,  2446, 20918,   378,  7821,    10,  4031,\n","        10005,    10,  1168,    18,   236,   402,    21, 10376,    10,     9,\n","          551,   234,   341,  1782,    43,  4031, 10005, 29598,    21, 10376,\n","         7821,  2902,   199,   402,    21, 10376,  1632,   343,  8633, 11260,\n","         6711,   651,  2875,  8143,    43, 28199,  6803,  4777,  2026, 11553,\n","         4777,  2026,  4341,   102,   647,  6685,  5529,    21,  6760,    18,\n","        10530,  5529,  2216,    35, 20192, 24524, 15540,   215,  2473,  2842,\n","        13461, 20192,  2473,  2842,  4777,  2026,  1888,   291,   364,  7379,\n","          496,  8704, 15411, 24524,  7469,    35,    21, 11262,  1737,  8602,\n","        16095,  1168,  9363,  4777,  2026,   585,  5036,  5012,  5136,    10,\n","           43,  8962,    55,  4700,  1466,  1632,  1466,  2645,  4098,    81,\n","           41,    10,     9,     6,     1,     1,     1,     1,     1,     1,\n","            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n","            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n","            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n","            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n","            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n","            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n","            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n","            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n","            1,     1,     1,     1,     1,     1])\n","Attention Masks tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0,\n","        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"9CdNwGmwK_O9"},"source":["# 3.4. Training & Validation Split\n","\n","Divide up our training randomly select ..% as a  validation set off of the training set.\n"]},{"cell_type":"code","metadata":{"id":"Q8Nx3qpmox-J","executionInfo":{"status":"ok","timestamp":1619698873134,"user_tz":-120,"elapsed":89798,"user":{"displayName":"Sara Fattouh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjGrdvZo2x_8K8abskRubIgMK3SdtS4cG-aQE01nA=s64","userId":"09360594903764573322"}}},"source":["from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n","\n","val_size = 0.1\n","\n","tr_inputs, val_inputs, tr_labels, val_labels = train_test_split(input_ids, labels, stratify=labels, random_state=2020,\n","                                                                test_size=val_size)\n","\n","tr_masks, val_masks, u,v =   train_test_split(attention_masks, labels, stratify=labels, random_state=2020,\n","                                              test_size=val_size)\n","\n","\n","\n","\n","train_dataset = torch.utils.data.TensorDataset(tr_inputs, tr_masks, tr_labels)\n","\n","val_dataset= torch.utils.data.TensorDataset(val_inputs, val_masks, val_labels)\n"," \n","train_sampler = RandomSampler(train_dataset) \n","valid_sampler = SequentialSampler(val_dataset)"],"execution_count":35,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"k1yPiShBKbHP"},"source":["We’ll also create an **iterator** for our dataset using the torch DataLoader class. This helps save on memory during training because unlike a for loop, with an iterator the entire dataset does not need to be loaded into memory.\n","\n","**Dataloaders** allow for batch delivery of the dataset into the architecture for training. \n","They are critical for ensuring a streamlined flow of data."]},{"cell_type":"code","metadata":{"id":"F9i9zl0lKX2U","executionInfo":{"status":"ok","timestamp":1619698873135,"user_tz":-120,"elapsed":89797,"user":{"displayName":"Sara Fattouh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjGrdvZo2x_8K8abskRubIgMK3SdtS4cG-aQE01nA=s64","userId":"09360594903764573322"}}},"source":["# The DataLoader needs to know our batch size for training, so we specify it \n","# here. For fine-tuning BERT on a specific task, the authors recommend a batch \n","# size of 16 or 32.\n","\n","batch_size = 32\n","\n","# Create the DataLoaders for our training and validation sets.\n","# We'll take training samples in random order. \n","train_dataloader = DataLoader(\n","            train_dataset,  # The training samples.\n","            sampler = train_sampler, # Select batches randomly\n","            batch_size = batch_size # Trains with this batch size.\n","        )\n","\n","# For validation the order doesn't matter, so we'll just read them sequentially.\n","validation_dataloader = DataLoader(\n","            val_dataset, # The validation samples.\n","            sampler = valid_sampler, # Pull out batches sequentially.\n","            batch_size = batch_size # Evaluate with this batch size.\n","        )\n","\n","\n","# test_dataloader = DataLoader(\n","#             test_dataset,  # The training samples.\n","#             sampler = test_sampler, # Select batches randomly\n","#             batch_size = batch_size # Trains with this batch size.\n","#         )"],"execution_count":36,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"FIr2h48YLDzA"},"source":["# 4. Train Our Classification Model\n","Now that our input data is properly formatted, it’s time to fine tune the BERT model."]},{"cell_type":"markdown","metadata":{"id":"an3i1Q3mRbjt"},"source":["# 4.1. BertForSequenceClassification\n","\n","Thankfully, the huggingface pytorch implementation includes a set of interfaces designed for a variety of NLP tasks. Though these interfaces are all built on top of a trained BERT model, each has different top layers and output types designed to accomodate their specific NLP task.\n","\n","We first want to modify the pre-trained BERT model to give outputs for classification, and then we want to continue training the model on our dataset until that the entire model, end-to-end, is well-suited for our task.\n","\n","**BertForSequenceClassification** is one of the current of classes provided for fine-tuning.\n","\n","This is the normal BERT model with an added single linear layer on top for classification that we will use as a sentence classifier. As we feed input data, the entire pre-trained BERT model and the additional untrained classification layer is trained on our specific task.\n","\n","- Not to forget that Camembet model inherits RobertaModel"]},{"cell_type":"code","metadata":{"id":"9jkAzT_7k6a-","executionInfo":{"status":"ok","timestamp":1619698873136,"user_tz":-120,"elapsed":89796,"user":{"displayName":"Sara Fattouh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjGrdvZo2x_8K8abskRubIgMK3SdtS4cG-aQE01nA=s64","userId":"09360594903764573322"}}},"source":["class vec_output_CamembertForSequenceClassification(CamembertModel):\n","    config_class = CamembertConfig\n","\n","    def __init__(self, config):\n","        super().__init__(config)\n","        self.num_labels = config.num_labels\n","\n","        self.roberta = CamembertModel(config)\n","        self.dense = nn.Linear(256*config.hidden_size, config.hidden_size)\n","        self.dropout = nn.Dropout(0.1)\n","        self.out_proj = nn.Linear(config.hidden_size, config.num_labels)\n","        self.init_weights()\n","\n","\n","    def forward(\n","        self,\n","        input_ids=None,\n","        attention_mask=None,\n","        token_type_ids=None,\n","        position_ids=None,\n","        head_mask=None,\n","        inputs_embeds=None,\n","        labels=None,\n","        output_attentions=None,\n","        output_hidden_states=None,\n","    ):\n","        outputs = self.roberta(\n","            input_ids,\n","            attention_mask=attention_mask,\n","            token_type_ids=token_type_ids,\n","            position_ids=position_ids,\n","            head_mask=head_mask,\n","            inputs_embeds=inputs_embeds,\n","#             output_attentions=output_attentions,\n","#             output_hidden_states=output_hidden_states,\n","        )\n","        sequence_output = outputs[0] #(B,256,768)\n","        x = sequence_output.view(sequence_output.shape[0], 256*768)\n","#         x = sequence_output[:, 0, :]  # take <s> token (equiv. to [CLS])-> #(B,768) Image -> (B,2048)\n","        x = self.dense(x)  # 768 -> 768\n","        feat= torch.tanh(x) \n","        logits = self.out_proj(feat) # 768 -> 27\n","        outputs = (logits,) + outputs[2:]\n","\n","        return outputs,feat  # (loss), logits, (hidden_states), (attentions)"],"execution_count":37,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000,"referenced_widgets":["35985184ff31491cb463b9844d2bad6b","5dcb25da607949e9b39fe6791116990c","46d0bc4295aa4e93b0d546e8db59d05f","66d8ba741be84d0fb533778bb741b6c9","a6208841eb1a4f81a5ce9147b892d641","397b2671ba094ef3b0cdab5dd0a3e125","0b4847b68f294e2c96c0eb054b9aa36a","c40f1569af1647b1994967b502768a2b","a490b51178ee4cc5bdeecbf8d21e6487","d3b697ecbd794e1db05a6d64a6a28cfb","f55b93a86b3e4d57bd6dccf4bb56c3cc","47effa7283234911b7fbda7ab6f74ec7","3f91f490afda4ea0ab0e30b25c17c4c2","9511691c3c63403dbce5b1d167f3b184","e8aee6e517354b219890423b189c256d","b747db2106454f85a9440b257e4d014e"]},"id":"2P50QV8QoP0M","executionInfo":{"status":"ok","timestamp":1619698896204,"user_tz":-120,"elapsed":112856,"user":{"displayName":"Sara Fattouh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjGrdvZo2x_8K8abskRubIgMK3SdtS4cG-aQE01nA=s64","userId":"09360594903764573322"}},"outputId":"d7d8e86e-f3cc-4b35-9f60-ca618b67bae8"},"source":["num_classes = 27 \n","\n","model = vec_output_CamembertForSequenceClassification.from_pretrained(\n","    modelname,                                    # Use the 12-layer BERT model, with an uncased vocab.\n","    num_labels = num_classes,                     # The number of output labels -- use 2 for binary classification.                                      # You can increase this for multi-class tasks.                                         \n","    output_attentions = False,                    # Whether the model returns attentions weights.\n","    output_hidden_states = False,                 # Whether the model returns all hidden-states.\n",")\n","\n","model.cuda()  # Tell pytorch to run this model on the GPU."],"execution_count":38,"outputs":[{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"35985184ff31491cb463b9844d2bad6b","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=0.0, description='Downloading', max=508.0, style=ProgressStyle(description_…"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["\n"],"name":"stdout"},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"a490b51178ee4cc5bdeecbf8d21e6487","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=0.0, description='Downloading', max=445032417.0, style=ProgressStyle(descri…"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["\n"],"name":"stdout"},{"output_type":"stream","text":["Some weights of the model checkpoint at camembert-base were not used when initializing vec_output_CamembertForSequenceClassification: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight']\n","- This IS expected if you are initializing vec_output_CamembertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing vec_output_CamembertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of vec_output_CamembertForSequenceClassification were not initialized from the model checkpoint at camembert-base and are newly initialized: ['embeddings.word_embeddings.weight', 'embeddings.position_embeddings.weight', 'embeddings.token_type_embeddings.weight', 'embeddings.LayerNorm.weight', 'embeddings.LayerNorm.bias', 'encoder.layer.0.attention.self.query.weight', 'encoder.layer.0.attention.self.query.bias', 'encoder.layer.0.attention.self.key.weight', 'encoder.layer.0.attention.self.key.bias', 'encoder.layer.0.attention.self.value.weight', 'encoder.layer.0.attention.self.value.bias', 'encoder.layer.0.attention.output.dense.weight', 'encoder.layer.0.attention.output.dense.bias', 'encoder.layer.0.attention.output.LayerNorm.weight', 'encoder.layer.0.attention.output.LayerNorm.bias', 'encoder.layer.0.intermediate.dense.weight', 'encoder.layer.0.intermediate.dense.bias', 'encoder.layer.0.output.dense.weight', 'encoder.layer.0.output.dense.bias', 'encoder.layer.0.output.LayerNorm.weight', 'encoder.layer.0.output.LayerNorm.bias', 'encoder.layer.1.attention.self.query.weight', 'encoder.layer.1.attention.self.query.bias', 'encoder.layer.1.attention.self.key.weight', 'encoder.layer.1.attention.self.key.bias', 'encoder.layer.1.attention.self.value.weight', 'encoder.layer.1.attention.self.value.bias', 'encoder.layer.1.attention.output.dense.weight', 'encoder.layer.1.attention.output.dense.bias', 'encoder.layer.1.attention.output.LayerNorm.weight', 'encoder.layer.1.attention.output.LayerNorm.bias', 'encoder.layer.1.intermediate.dense.weight', 'encoder.layer.1.intermediate.dense.bias', 'encoder.layer.1.output.dense.weight', 'encoder.layer.1.output.dense.bias', 'encoder.layer.1.output.LayerNorm.weight', 'encoder.layer.1.output.LayerNorm.bias', 'encoder.layer.2.attention.self.query.weight', 'encoder.layer.2.attention.self.query.bias', 'encoder.layer.2.attention.self.key.weight', 'encoder.layer.2.attention.self.key.bias', 'encoder.layer.2.attention.self.value.weight', 'encoder.layer.2.attention.self.value.bias', 'encoder.layer.2.attention.output.dense.weight', 'encoder.layer.2.attention.output.dense.bias', 'encoder.layer.2.attention.output.LayerNorm.weight', 'encoder.layer.2.attention.output.LayerNorm.bias', 'encoder.layer.2.intermediate.dense.weight', 'encoder.layer.2.intermediate.dense.bias', 'encoder.layer.2.output.dense.weight', 'encoder.layer.2.output.dense.bias', 'encoder.layer.2.output.LayerNorm.weight', 'encoder.layer.2.output.LayerNorm.bias', 'encoder.layer.3.attention.self.query.weight', 'encoder.layer.3.attention.self.query.bias', 'encoder.layer.3.attention.self.key.weight', 'encoder.layer.3.attention.self.key.bias', 'encoder.layer.3.attention.self.value.weight', 'encoder.layer.3.attention.self.value.bias', 'encoder.layer.3.attention.output.dense.weight', 'encoder.layer.3.attention.output.dense.bias', 'encoder.layer.3.attention.output.LayerNorm.weight', 'encoder.layer.3.attention.output.LayerNorm.bias', 'encoder.layer.3.intermediate.dense.weight', 'encoder.layer.3.intermediate.dense.bias', 'encoder.layer.3.output.dense.weight', 'encoder.layer.3.output.dense.bias', 'encoder.layer.3.output.LayerNorm.weight', 'encoder.layer.3.output.LayerNorm.bias', 'encoder.layer.4.attention.self.query.weight', 'encoder.layer.4.attention.self.query.bias', 'encoder.layer.4.attention.self.key.weight', 'encoder.layer.4.attention.self.key.bias', 'encoder.layer.4.attention.self.value.weight', 'encoder.layer.4.attention.self.value.bias', 'encoder.layer.4.attention.output.dense.weight', 'encoder.layer.4.attention.output.dense.bias', 'encoder.layer.4.attention.output.LayerNorm.weight', 'encoder.layer.4.attention.output.LayerNorm.bias', 'encoder.layer.4.intermediate.dense.weight', 'encoder.layer.4.intermediate.dense.bias', 'encoder.layer.4.output.dense.weight', 'encoder.layer.4.output.dense.bias', 'encoder.layer.4.output.LayerNorm.weight', 'encoder.layer.4.output.LayerNorm.bias', 'encoder.layer.5.attention.self.query.weight', 'encoder.layer.5.attention.self.query.bias', 'encoder.layer.5.attention.self.key.weight', 'encoder.layer.5.attention.self.key.bias', 'encoder.layer.5.attention.self.value.weight', 'encoder.layer.5.attention.self.value.bias', 'encoder.layer.5.attention.output.dense.weight', 'encoder.layer.5.attention.output.dense.bias', 'encoder.layer.5.attention.output.LayerNorm.weight', 'encoder.layer.5.attention.output.LayerNorm.bias', 'encoder.layer.5.intermediate.dense.weight', 'encoder.layer.5.intermediate.dense.bias', 'encoder.layer.5.output.dense.weight', 'encoder.layer.5.output.dense.bias', 'encoder.layer.5.output.LayerNorm.weight', 'encoder.layer.5.output.LayerNorm.bias', 'encoder.layer.6.attention.self.query.weight', 'encoder.layer.6.attention.self.query.bias', 'encoder.layer.6.attention.self.key.weight', 'encoder.layer.6.attention.self.key.bias', 'encoder.layer.6.attention.self.value.weight', 'encoder.layer.6.attention.self.value.bias', 'encoder.layer.6.attention.output.dense.weight', 'encoder.layer.6.attention.output.dense.bias', 'encoder.layer.6.attention.output.LayerNorm.weight', 'encoder.layer.6.attention.output.LayerNorm.bias', 'encoder.layer.6.intermediate.dense.weight', 'encoder.layer.6.intermediate.dense.bias', 'encoder.layer.6.output.dense.weight', 'encoder.layer.6.output.dense.bias', 'encoder.layer.6.output.LayerNorm.weight', 'encoder.layer.6.output.LayerNorm.bias', 'encoder.layer.7.attention.self.query.weight', 'encoder.layer.7.attention.self.query.bias', 'encoder.layer.7.attention.self.key.weight', 'encoder.layer.7.attention.self.key.bias', 'encoder.layer.7.attention.self.value.weight', 'encoder.layer.7.attention.self.value.bias', 'encoder.layer.7.attention.output.dense.weight', 'encoder.layer.7.attention.output.dense.bias', 'encoder.layer.7.attention.output.LayerNorm.weight', 'encoder.layer.7.attention.output.LayerNorm.bias', 'encoder.layer.7.intermediate.dense.weight', 'encoder.layer.7.intermediate.dense.bias', 'encoder.layer.7.output.dense.weight', 'encoder.layer.7.output.dense.bias', 'encoder.layer.7.output.LayerNorm.weight', 'encoder.layer.7.output.LayerNorm.bias', 'encoder.layer.8.attention.self.query.weight', 'encoder.layer.8.attention.self.query.bias', 'encoder.layer.8.attention.self.key.weight', 'encoder.layer.8.attention.self.key.bias', 'encoder.layer.8.attention.self.value.weight', 'encoder.layer.8.attention.self.value.bias', 'encoder.layer.8.attention.output.dense.weight', 'encoder.layer.8.attention.output.dense.bias', 'encoder.layer.8.attention.output.LayerNorm.weight', 'encoder.layer.8.attention.output.LayerNorm.bias', 'encoder.layer.8.intermediate.dense.weight', 'encoder.layer.8.intermediate.dense.bias', 'encoder.layer.8.output.dense.weight', 'encoder.layer.8.output.dense.bias', 'encoder.layer.8.output.LayerNorm.weight', 'encoder.layer.8.output.LayerNorm.bias', 'encoder.layer.9.attention.self.query.weight', 'encoder.layer.9.attention.self.query.bias', 'encoder.layer.9.attention.self.key.weight', 'encoder.layer.9.attention.self.key.bias', 'encoder.layer.9.attention.self.value.weight', 'encoder.layer.9.attention.self.value.bias', 'encoder.layer.9.attention.output.dense.weight', 'encoder.layer.9.attention.output.dense.bias', 'encoder.layer.9.attention.output.LayerNorm.weight', 'encoder.layer.9.attention.output.LayerNorm.bias', 'encoder.layer.9.intermediate.dense.weight', 'encoder.layer.9.intermediate.dense.bias', 'encoder.layer.9.output.dense.weight', 'encoder.layer.9.output.dense.bias', 'encoder.layer.9.output.LayerNorm.weight', 'encoder.layer.9.output.LayerNorm.bias', 'encoder.layer.10.attention.self.query.weight', 'encoder.layer.10.attention.self.query.bias', 'encoder.layer.10.attention.self.key.weight', 'encoder.layer.10.attention.self.key.bias', 'encoder.layer.10.attention.self.value.weight', 'encoder.layer.10.attention.self.value.bias', 'encoder.layer.10.attention.output.dense.weight', 'encoder.layer.10.attention.output.dense.bias', 'encoder.layer.10.attention.output.LayerNorm.weight', 'encoder.layer.10.attention.output.LayerNorm.bias', 'encoder.layer.10.intermediate.dense.weight', 'encoder.layer.10.intermediate.dense.bias', 'encoder.layer.10.output.dense.weight', 'encoder.layer.10.output.dense.bias', 'encoder.layer.10.output.LayerNorm.weight', 'encoder.layer.10.output.LayerNorm.bias', 'encoder.layer.11.attention.self.query.weight', 'encoder.layer.11.attention.self.query.bias', 'encoder.layer.11.attention.self.key.weight', 'encoder.layer.11.attention.self.key.bias', 'encoder.layer.11.attention.self.value.weight', 'encoder.layer.11.attention.self.value.bias', 'encoder.layer.11.attention.output.dense.weight', 'encoder.layer.11.attention.output.dense.bias', 'encoder.layer.11.attention.output.LayerNorm.weight', 'encoder.layer.11.attention.output.LayerNorm.bias', 'encoder.layer.11.intermediate.dense.weight', 'encoder.layer.11.intermediate.dense.bias', 'encoder.layer.11.output.dense.weight', 'encoder.layer.11.output.dense.bias', 'encoder.layer.11.output.LayerNorm.weight', 'encoder.layer.11.output.LayerNorm.bias', 'pooler.dense.weight', 'pooler.dense.bias', 'dense.weight', 'dense.bias', 'out_proj.weight', 'out_proj.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"],"name":"stderr"},{"output_type":"execute_result","data":{"text/plain":["vec_output_CamembertForSequenceClassification(\n","  (embeddings): RobertaEmbeddings(\n","    (word_embeddings): Embedding(32005, 768, padding_idx=1)\n","    (position_embeddings): Embedding(514, 768, padding_idx=1)\n","    (token_type_embeddings): Embedding(1, 768)\n","    (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","    (dropout): Dropout(p=0.1, inplace=False)\n","  )\n","  (encoder): RobertaEncoder(\n","    (layer): ModuleList(\n","      (0): RobertaLayer(\n","        (attention): RobertaAttention(\n","          (self): RobertaSelfAttention(\n","            (query): Linear(in_features=768, out_features=768, bias=True)\n","            (key): Linear(in_features=768, out_features=768, bias=True)\n","            (value): Linear(in_features=768, out_features=768, bias=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (output): RobertaSelfOutput(\n","            (dense): Linear(in_features=768, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (intermediate): RobertaIntermediate(\n","          (dense): Linear(in_features=768, out_features=3072, bias=True)\n","        )\n","        (output): RobertaOutput(\n","          (dense): Linear(in_features=3072, out_features=768, bias=True)\n","          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","      )\n","      (1): RobertaLayer(\n","        (attention): RobertaAttention(\n","          (self): RobertaSelfAttention(\n","            (query): Linear(in_features=768, out_features=768, bias=True)\n","            (key): Linear(in_features=768, out_features=768, bias=True)\n","            (value): Linear(in_features=768, out_features=768, bias=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (output): RobertaSelfOutput(\n","            (dense): Linear(in_features=768, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (intermediate): RobertaIntermediate(\n","          (dense): Linear(in_features=768, out_features=3072, bias=True)\n","        )\n","        (output): RobertaOutput(\n","          (dense): Linear(in_features=3072, out_features=768, bias=True)\n","          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","      )\n","      (2): RobertaLayer(\n","        (attention): RobertaAttention(\n","          (self): RobertaSelfAttention(\n","            (query): Linear(in_features=768, out_features=768, bias=True)\n","            (key): Linear(in_features=768, out_features=768, bias=True)\n","            (value): Linear(in_features=768, out_features=768, bias=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (output): RobertaSelfOutput(\n","            (dense): Linear(in_features=768, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (intermediate): RobertaIntermediate(\n","          (dense): Linear(in_features=768, out_features=3072, bias=True)\n","        )\n","        (output): RobertaOutput(\n","          (dense): Linear(in_features=3072, out_features=768, bias=True)\n","          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","      )\n","      (3): RobertaLayer(\n","        (attention): RobertaAttention(\n","          (self): RobertaSelfAttention(\n","            (query): Linear(in_features=768, out_features=768, bias=True)\n","            (key): Linear(in_features=768, out_features=768, bias=True)\n","            (value): Linear(in_features=768, out_features=768, bias=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (output): RobertaSelfOutput(\n","            (dense): Linear(in_features=768, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (intermediate): RobertaIntermediate(\n","          (dense): Linear(in_features=768, out_features=3072, bias=True)\n","        )\n","        (output): RobertaOutput(\n","          (dense): Linear(in_features=3072, out_features=768, bias=True)\n","          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","      )\n","      (4): RobertaLayer(\n","        (attention): RobertaAttention(\n","          (self): RobertaSelfAttention(\n","            (query): Linear(in_features=768, out_features=768, bias=True)\n","            (key): Linear(in_features=768, out_features=768, bias=True)\n","            (value): Linear(in_features=768, out_features=768, bias=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (output): RobertaSelfOutput(\n","            (dense): Linear(in_features=768, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (intermediate): RobertaIntermediate(\n","          (dense): Linear(in_features=768, out_features=3072, bias=True)\n","        )\n","        (output): RobertaOutput(\n","          (dense): Linear(in_features=3072, out_features=768, bias=True)\n","          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","      )\n","      (5): RobertaLayer(\n","        (attention): RobertaAttention(\n","          (self): RobertaSelfAttention(\n","            (query): Linear(in_features=768, out_features=768, bias=True)\n","            (key): Linear(in_features=768, out_features=768, bias=True)\n","            (value): Linear(in_features=768, out_features=768, bias=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (output): RobertaSelfOutput(\n","            (dense): Linear(in_features=768, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (intermediate): RobertaIntermediate(\n","          (dense): Linear(in_features=768, out_features=3072, bias=True)\n","        )\n","        (output): RobertaOutput(\n","          (dense): Linear(in_features=3072, out_features=768, bias=True)\n","          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","      )\n","      (6): RobertaLayer(\n","        (attention): RobertaAttention(\n","          (self): RobertaSelfAttention(\n","            (query): Linear(in_features=768, out_features=768, bias=True)\n","            (key): Linear(in_features=768, out_features=768, bias=True)\n","            (value): Linear(in_features=768, out_features=768, bias=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (output): RobertaSelfOutput(\n","            (dense): Linear(in_features=768, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (intermediate): RobertaIntermediate(\n","          (dense): Linear(in_features=768, out_features=3072, bias=True)\n","        )\n","        (output): RobertaOutput(\n","          (dense): Linear(in_features=3072, out_features=768, bias=True)\n","          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","      )\n","      (7): RobertaLayer(\n","        (attention): RobertaAttention(\n","          (self): RobertaSelfAttention(\n","            (query): Linear(in_features=768, out_features=768, bias=True)\n","            (key): Linear(in_features=768, out_features=768, bias=True)\n","            (value): Linear(in_features=768, out_features=768, bias=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (output): RobertaSelfOutput(\n","            (dense): Linear(in_features=768, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (intermediate): RobertaIntermediate(\n","          (dense): Linear(in_features=768, out_features=3072, bias=True)\n","        )\n","        (output): RobertaOutput(\n","          (dense): Linear(in_features=3072, out_features=768, bias=True)\n","          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","      )\n","      (8): RobertaLayer(\n","        (attention): RobertaAttention(\n","          (self): RobertaSelfAttention(\n","            (query): Linear(in_features=768, out_features=768, bias=True)\n","            (key): Linear(in_features=768, out_features=768, bias=True)\n","            (value): Linear(in_features=768, out_features=768, bias=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (output): RobertaSelfOutput(\n","            (dense): Linear(in_features=768, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (intermediate): RobertaIntermediate(\n","          (dense): Linear(in_features=768, out_features=3072, bias=True)\n","        )\n","        (output): RobertaOutput(\n","          (dense): Linear(in_features=3072, out_features=768, bias=True)\n","          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","      )\n","      (9): RobertaLayer(\n","        (attention): RobertaAttention(\n","          (self): RobertaSelfAttention(\n","            (query): Linear(in_features=768, out_features=768, bias=True)\n","            (key): Linear(in_features=768, out_features=768, bias=True)\n","            (value): Linear(in_features=768, out_features=768, bias=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (output): RobertaSelfOutput(\n","            (dense): Linear(in_features=768, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (intermediate): RobertaIntermediate(\n","          (dense): Linear(in_features=768, out_features=3072, bias=True)\n","        )\n","        (output): RobertaOutput(\n","          (dense): Linear(in_features=3072, out_features=768, bias=True)\n","          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","      )\n","      (10): RobertaLayer(\n","        (attention): RobertaAttention(\n","          (self): RobertaSelfAttention(\n","            (query): Linear(in_features=768, out_features=768, bias=True)\n","            (key): Linear(in_features=768, out_features=768, bias=True)\n","            (value): Linear(in_features=768, out_features=768, bias=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (output): RobertaSelfOutput(\n","            (dense): Linear(in_features=768, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (intermediate): RobertaIntermediate(\n","          (dense): Linear(in_features=768, out_features=3072, bias=True)\n","        )\n","        (output): RobertaOutput(\n","          (dense): Linear(in_features=3072, out_features=768, bias=True)\n","          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","      )\n","      (11): RobertaLayer(\n","        (attention): RobertaAttention(\n","          (self): RobertaSelfAttention(\n","            (query): Linear(in_features=768, out_features=768, bias=True)\n","            (key): Linear(in_features=768, out_features=768, bias=True)\n","            (value): Linear(in_features=768, out_features=768, bias=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (output): RobertaSelfOutput(\n","            (dense): Linear(in_features=768, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (intermediate): RobertaIntermediate(\n","          (dense): Linear(in_features=768, out_features=3072, bias=True)\n","        )\n","        (output): RobertaOutput(\n","          (dense): Linear(in_features=3072, out_features=768, bias=True)\n","          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","      )\n","    )\n","  )\n","  (pooler): RobertaPooler(\n","    (dense): Linear(in_features=768, out_features=768, bias=True)\n","    (activation): Tanh()\n","  )\n","  (roberta): CamembertModel(\n","    (embeddings): RobertaEmbeddings(\n","      (word_embeddings): Embedding(32005, 768, padding_idx=1)\n","      (position_embeddings): Embedding(514, 768, padding_idx=1)\n","      (token_type_embeddings): Embedding(1, 768)\n","      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","      (dropout): Dropout(p=0.1, inplace=False)\n","    )\n","    (encoder): RobertaEncoder(\n","      (layer): ModuleList(\n","        (0): RobertaLayer(\n","          (attention): RobertaAttention(\n","            (self): RobertaSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): RobertaSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): RobertaIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): RobertaOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (1): RobertaLayer(\n","          (attention): RobertaAttention(\n","            (self): RobertaSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): RobertaSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): RobertaIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): RobertaOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (2): RobertaLayer(\n","          (attention): RobertaAttention(\n","            (self): RobertaSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): RobertaSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): RobertaIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): RobertaOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (3): RobertaLayer(\n","          (attention): RobertaAttention(\n","            (self): RobertaSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): RobertaSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): RobertaIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): RobertaOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (4): RobertaLayer(\n","          (attention): RobertaAttention(\n","            (self): RobertaSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): RobertaSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): RobertaIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): RobertaOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (5): RobertaLayer(\n","          (attention): RobertaAttention(\n","            (self): RobertaSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): RobertaSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): RobertaIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): RobertaOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (6): RobertaLayer(\n","          (attention): RobertaAttention(\n","            (self): RobertaSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): RobertaSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): RobertaIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): RobertaOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (7): RobertaLayer(\n","          (attention): RobertaAttention(\n","            (self): RobertaSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): RobertaSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): RobertaIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): RobertaOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (8): RobertaLayer(\n","          (attention): RobertaAttention(\n","            (self): RobertaSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): RobertaSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): RobertaIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): RobertaOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (9): RobertaLayer(\n","          (attention): RobertaAttention(\n","            (self): RobertaSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): RobertaSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): RobertaIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): RobertaOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (10): RobertaLayer(\n","          (attention): RobertaAttention(\n","            (self): RobertaSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): RobertaSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): RobertaIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): RobertaOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (11): RobertaLayer(\n","          (attention): RobertaAttention(\n","            (self): RobertaSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): RobertaSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): RobertaIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): RobertaOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","    )\n","    (pooler): RobertaPooler(\n","      (dense): Linear(in_features=768, out_features=768, bias=True)\n","      (activation): Tanh()\n","    )\n","  )\n","  (dense): Linear(in_features=196608, out_features=768, bias=True)\n","  (dropout): Dropout(p=0.1, inplace=False)\n","  (out_proj): Linear(in_features=768, out_features=27, bias=True)\n",")"]},"metadata":{"tags":[]},"execution_count":38}]},{"cell_type":"markdown","metadata":{"id":"K1qsv9PIQ7Wd"},"source":[" **Define a helper function for calculating accuracy.**"]},{"cell_type":"code","metadata":{"id":"iRhlKlfPo9bx","executionInfo":{"status":"ok","timestamp":1619698896205,"user_tz":-120,"elapsed":112854,"user":{"displayName":"Sara Fattouh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjGrdvZo2x_8K8abskRubIgMK3SdtS4cG-aQE01nA=s64","userId":"09360594903764573322"}}},"source":["# Function to calculate the accuracy of our predictions vs labels\n","def flat_accuracy(preds, labels):\n","    pred_flat = np.argmax(preds, axis=1).flatten()\n","    labels_flat = labels.flatten()\n","    return np.sum(pred_flat == labels_flat) / len(labels_flat)"],"execution_count":39,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"bMgtCqPWRKY2"},"source":["**Define a helper function for formatting elapsed times as hh:mm:ss**"]},{"cell_type":"code","metadata":{"id":"0kVwm0_3RMO4","executionInfo":{"status":"ok","timestamp":1619698896205,"user_tz":-120,"elapsed":112852,"user":{"displayName":"Sara Fattouh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjGrdvZo2x_8K8abskRubIgMK3SdtS4cG-aQE01nA=s64","userId":"09360594903764573322"}}},"source":["import time\n","import datetime\n","\n","def format_time(elapsed):\n","    '''\n","    Takes a time in seconds and returns a string hh:mm:ss\n","    '''\n","    # Round to the nearest second.\n","    elapsed_rounded = int(round((elapsed)))\n","    \n","    # Format as hh:mm:ss\n","    return str(datetime.timedelta(seconds=elapsed_rounded))"],"execution_count":40,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"YGH8gKNsMwOD"},"source":["# 4.2. Optimizer & Learning Rate Scheduler\n","\n","Now that we have our model loaded we need to grab the training hyperparameters from within the stored model.\n","\n","For the purposes of fine-tuning, the authors recommend choosing from the following values (from Appendix A.3 of the BERT paper):\n","\n","*  Batch size: 16, 32\n","*   Learning rate (Adam): 5e-5, 3e-5, 2e-5\n","\n","For fine-tuning BERT, we should use **transformers.AdamW** instead of Pytorch's version of it.\n","The epsilon parameter eps = 1e-8 is “a very small number to prevent any division by zero in the implementation”\n","\n","\n","\n","\n","\n"]},{"cell_type":"code","metadata":{"id":"-A-OPvnTOD9U","executionInfo":{"status":"ok","timestamp":1619698896206,"user_tz":-120,"elapsed":112851,"user":{"displayName":"Sara Fattouh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjGrdvZo2x_8K8abskRubIgMK3SdtS4cG-aQE01nA=s64","userId":"09360594903764573322"}}},"source":["# Note: AdamW is a class from the huggingface library \n","# I believe the 'W' stands for 'Weight Decay fix\"\n","\n","optimizer = AdamW(model.parameters(),\n","                  lr = 2e-5, # args.learning_rate - default is 5e-5, our notebook had 2e-5\n","                  eps = 1e-8 # args.adam_epsilon  - default is 1e-8.\n","                )"],"execution_count":41,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"iUQfbnkDZ5yN"},"source":["Also, we should use a **warmup scheduler** as suggested in the paper, so the scheduler is created using get_linear_scheduler_with_warmup function from transformers package.\n","\n","It create a schedule with a learning rate that decreases linearly from the initial lr set in the optimizer to 0, after a warmup period during which it increases linearly from 0 to the initial lr set in the optimizer."]},{"cell_type":"code","metadata":{"id":"0VA3mBBNOUT_","executionInfo":{"status":"ok","timestamp":1619698896206,"user_tz":-120,"elapsed":112850,"user":{"displayName":"Sara Fattouh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjGrdvZo2x_8K8abskRubIgMK3SdtS4cG-aQE01nA=s64","userId":"09360594903764573322"}}},"source":["from transformers import get_linear_schedule_with_warmup\n","\n","# Number of training epochs. The BERT authors recommend between 2 and 4. \n","epochs = 10\n","\n","# Total number of training steps is [number of batches] x [number of epochs]. \n","# (Note that this is not the same as the number of training samples).\n","\n","total_steps = len(train_dataloader) * epochs\n","\n","# Create the learning rate scheduler.\n","scheduler = get_linear_schedule_with_warmup(optimizer, \n","                                            num_warmup_steps = 0, # Default value in run_glue.py\n","                                            num_training_steps = total_steps)"],"execution_count":42,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"k4iexXQ7O4KW"},"source":["# 4.3. Training Loop\n","\n","Below is our training loop. Fundamentally for each pass in the loop we have a trianing phase and a validation phase.\n","\n","**Training:**\n","\n","\n","*   Unpack our data inputs and labels\n","*   Load data onto the GPU for acceleration\n","*   Clear out the gradients calculated in the previous pass\n","     - In pytorch the gradients accumulate by default, unless you  explicitly clear them out.\n","*   Forward pass (feed input data through the network)\n","*   Backward pass (backpropagation)\n","*   Tell the network to update parameters with optimizer.step()\n","*   Track variables for monitoring progress\n","\n","\n","\n","\n","\n","\n","\n","\n"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"tLwv08ldkCrY","executionInfo":{"status":"ok","timestamp":1619698896206,"user_tz":-120,"elapsed":112842,"user":{"displayName":"Sara Fattouh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjGrdvZo2x_8K8abskRubIgMK3SdtS4cG-aQE01nA=s64","userId":"09360594903764573322"}},"outputId":"31c46718-e19b-4c99-e490-a1ba745157a1"},"source":["import torch\n","import gc\n","\n","torch.cuda.empty_cache() #variables no longer referenced will be freed\n","gc.collect()\n","\n"],"execution_count":43,"outputs":[{"output_type":"execute_result","data":{"text/plain":["425"]},"metadata":{"tags":[]},"execution_count":43}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Hrl04w0BpcKv","outputId":"8637e66b-b29d-46b3-df53-cb88a1629665"},"source":["from sklearn.metrics import f1_score\n","import numpy as np\n","import time \n","import torch.optim as optim\n","import torch.nn as nn\n","import matplotlib.pyplot as plt\n","\n","\n","loss_criterion = nn.CrossEntropyLoss()\n","\n","\n","# This training code is based on the `run_glue.py` script here:\n","# https://github.com/huggingface/transformers/blob/5bfcd0485ece086ebcbed2d008813037968a9e58/examples/run_glue.py#L128\n","\n","# Set the seed value all over the place to make this reproducible.\n","seed_val = 42\n","\n","\n","np.random.seed(seed_val)\n","torch.manual_seed(seed_val)\n","torch.cuda.manual_seed_all(seed_val)\n","\n","# We'll store a number of quantities such as training and validation loss, \n","# validation accuracy, and timings.\n","training_stats = []\n","\n","# ----------------------------\n"," # Tracking variables \n","\n","\n","train_loss_values = []\n","\n","val_loss_values = []\n","logits_values =[]\n","\n","total_train_accuracy = 0\n","avg_train_accuracy = 0\n","\n","train_accuracy_values = []\n","val_accuracy_values = []\n","predictions=[]\n","true_labels=[]\n","\n","#-----------------------------\n","\n","\n","\n","\n","# Measure the total training time for the whole run.\n","total_t0 = time.time()\n","device = torch.device(\"cuda:0\")\n","\n","# For each epoch...\n","for epoch_i in range(0, epochs):\n","    \n","    # ========================================\n","    #               Training\n","    # ========================================\n","    \n","    # Perform one full pass over the training set.\n","\n","    print(\"\")\n","    print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n","    print('Training...')\n","    \n","    #tr and val\n","    vec_output_tr = []\n","    vec_output_val =[]\n","\n","    # Measure how long the training epoch takes.\n","    t0 = time.time()\n","\n","    # Reset the total loss for this epoch.\n","    total_train_loss = 0\n","\n","    # Put the model into training mode. Don't be mislead--the call to \n","    # `train` just changes the *mode*, it doesn't *perform* the training.\n","    # `dropout` and `batchnorm` layers behave differently during training\n","    # vs. test (source: https://stackoverflow.com/questions/51433378/what-does-model-train-do-in-pytorch)\n","\n","\n","    best_f1 = 0\n","    model.train()   # It just sets the mode.\n","                    # tell the model that you are training it. So effectively layers like dropout, batchnorm etc.\n","                    # which behave differently on the train and test procedures \n","                    # know what is going on and hence can behave accordingly.\n","                    # you can call model.train(mode = False) or model.eval() for testing\n","                    \n","\n","    # For each batch of training data...\n","    for step, batch in enumerate(train_dataloader):\n","\n","      # Progress update every 40 batches.\n","        if step % 40 == 0 and not step == 0:\n","            # Calculate elapsed time in minutes.\n","            elapsed = format_time(time.time() - t0)\n","            \n","            # Report progress.\n","            print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(train_dataloader), elapsed))\n","        \n","        # Unpack this training batch from our dataloader. \n","        \n","        # As we unpack the batch, we'll also copy each tensor to the GPU using the \n","        # `to` method.\n","        #\n","        # `batch` contains three pytorch tensors:\n","        #   [0]: input ids \n","        #   [1]: attention masks\n","        #   [2]: labels \n","        b_input_ids  = batch[0].to(device)\n","        b_input_mask = batch[1].to(device)\n","        b_labels     = batch[2].to(device)\n","\n","        # clear previous gradients\n","        model.zero_grad()        \n","\n","        # In PyTorch, we need to set the gradients to zero before starting to do backpropragation  \n","        # PyTorch accumulates the gradients on subsequent backward passes.\n","        # Else the gradient would point in some other direction than the intended direction towards the minimum\n","        \n","        \n","        # pass a batch of inputs through the model \n","        # logits: outputs prior to activation\n","        logits,vec = model( b_input_ids,    \n","                            token_type_ids = None, \n","                            attention_mask = b_input_mask\n","                          )\n","        \n","        # get model output\n","        # take output zero because that's the [CLS] token, used for classification\n","        logits = logits[0]\n","        \n","        # calculate loss (batch output, ground truth batch labels)\n","        loss = loss_criterion(logits, b_labels)\n","        \n","        #saving the features_tr\n","        # Convert Pytorch Tensor to Numpy Array \n","        vec = vec.detach().cpu().numpy()\n","        vec_output_tr.extend(vec)\n","        \n","        # Accumulate the training loss over all of the batches so that we can\n","        # calculate the average loss at the end. `loss` is a Tensor containing a\n","        # single value; the `.item()` function just returns the Python value \n","        # from the tensor.\n","        total_train_loss += loss.item()\n","\n","        #-------------------------------------------------------\n","\n","        # Move logits and labels to CPU\n","        logits = logits.detach().cpu().numpy()\n","\n","        # Move logits and labels to CPU\n","        predicted_labels=np.argmax(logits,axis=1)\n","        predictions.extend(predicted_labels)\n","        label_ids = b_labels.to('cpu').numpy()\n","        true_labels.extend(label_ids)\n","\n","        total_train_accuracy += flat_accuracy(logits, label_ids)\n","\n","\n","        #-------------------------------------------------------\n","\n","        # Perform a backward pass to calculate the gradients.\n","        loss.backward()\n","\n","        # Clip the norm of the gradients to 1.0.\n","        # This is to help prevent the \"exploding gradients\" problem.\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n","\n","        # Update parameters and take a step using the computed gradient.\n","        # The optimizer dictates the \"update rule\"--how the parameters are\n","        # modified based on their gradients, the learning rate, etc.\n","        optimizer.step()\n","\n","        # Update the learning rate.\n","        scheduler.step()\n","        \n","  #----------------------------------------------\n","    avg_train_accuracy = total_train_accuracy / len(train_dataloader)\n","\n","    print(\"\")\n","    print(\"Training Accuracy: {}\".format(avg_train_accuracy))\n","    train_accuracy_values.append(avg_train_accuracy)\n","    avg_train_loss = total_train_loss / len(train_dataloader)  \n","    train_loss_values.append(avg_train_loss) \n","\n","#########################################################################\n","        \n","      \n","    # Calculate the average loss over all of the batches.\n","    avg_train_loss = total_train_loss / len(train_dataloader)            \n","    \n","    # Measure how long this epoch took.\n","    training_time = format_time(time.time() - t0)\n","\n","    print(\"\")\n","    print(\"  Average training loss: {} \".format(avg_train_loss))\n","    print(\"  Training epcoh took: {:} \".format(training_time))\n","    print(\"  Learning rate: \", optimizer.param_groups[0][\"lr\"])\n","        \n","    # ========================================\n","    #               Validation\n","    # ========================================\n","    # After the completion of each training epoch, measure our performance on\n","    # our validation set.\n","\n","    print(\"\")\n","    print(\"Running Validation...\")\n","\n","    t0 = time.time()\n","\n","    # Put the model in evaluation mode--the dropout layers behave differently\n","    # during evaluation.\n","    model.eval()\n","\n","    # Tracking variables \n","    total_eval_accuracy = 0\n","    total_eval_loss = 0\n","    nb_eval_steps = 0\n","    predictions = []\n","    true_labels = []\n","    \n","\n","    # Evaluate data for one epoch\n","    for batch in validation_dataloader:\n","        \n","        # Unpack this training batch from our dataloader. \n","        #\n","        # As we unpack the batch, we'll also copy each tensor to the GPU using \n","        # the `to` method.\n","        #\n","        # `batch` contains three pytorch tensors:\n","        #   [0]: input ids \n","        #   [1]: attention masks\n","        #   [2]: labels \n","        b_input_ids = batch[0].to(device)\n","        b_input_mask = batch[1].to(device)\n","        b_labels = batch[2].to(device)\n","        \n","        # Tell pytorch not to bother with constructing the compute graph during\n","        # the forward pass, since this is only needed for backprop (training).\n","        with torch.no_grad():        \n","\n","            # Forward pass, calculate logit predictions.\n","            # token_type_ids is the same as the \"segment ids\", which \n","            # differentiates sentence 1 and 2 in 2-sentence tasks.\n","            # The documentation for this `model` function is here: \n","            # https://huggingface.co/transformers/v2.2.0/model_doc/bert.html#transformers.BertForSequenceClassification\n","            # Get the \"logits\" output by the model. The \"logits\" are the output\n","            # values prior to applying an activation function like the softmax.\n","            logits,vec = model(b_input_ids, \n","                           token_type_ids = None, \n","                           attention_mask = b_input_mask\n","                           )\n","            \n","        #new\n","        logits = logits[0]\n","        \n","        #defining the val loss\n","        loss = loss_criterion(logits, b_labels)\n","        \n","        \n","        # Accumulate the validation loss.\n","        total_eval_loss += loss.item()\n","\n","        # Move logits and labels to CPU\n","        logits = logits.detach().cpu().numpy()\n","\n","        # Move logits and labels to CPU\n","        predicted_labels=np.argmax(logits,axis=1)\n","        predictions.extend(predicted_labels)\n","        label_ids = b_labels.to('cpu').numpy()\n","        true_labels.extend(label_ids)\n","        \n","        #saving the features_tr\n","        vec = vec.detach().cpu().numpy()\n","        vec_output_val.extend(vec)\n","        \n","\n","         ##########################################################################\n","\n","        logits_values.append(predicted_labels)\n","\n","        ##########################################################################\n","\n","        # Calculate the accuracy for this batch of test sentences, and\n","        # accumulate it over all batches.\n","        total_eval_accuracy += flat_accuracy(logits, label_ids)\n","        \n","\n","    # Report the final accuracy for this validation run.\n","    avg_val_accuracy = total_eval_accuracy / len(validation_dataloader)\n","    #--------------------------------\n","    val_accuracy_values.append(avg_val_accuracy)\n","    #--------------------------------\n","\n","    print(\"  Accuracy: {}\".format(avg_val_accuracy))\n","\n","    # Calculate the average loss over all of the batches.\n","    avg_val_loss = total_eval_loss / len(validation_dataloader)\n","\n","    #-----------------------------\n","    val_loss_values.append(avg_val_loss)\n","    \n","    # Measure how long the validation run took.\n","    validation_time = format_time(time.time() - t0)\n","    \n","    print(\"  Validation Loss: {}\".format(avg_val_loss))\n","    print(\"  Validation took: {:}\".format(validation_time))\n","    print(\"Validation F1-Score: {}\".format(f1_score(true_labels,predictions,average='macro')))\n","    curr_f1=f1_score(true_labels,predictions,average='macro')\n","    if curr_f1 > best_f1:\n","        best_f1=curr_f1\n","        torch.save(model.state_dict(), '/content/drive/My Drive/Rakuten/models/90_10_Hirachical_CamemBERT_description.pt')\n","        np.save('CamemBERT_best_vec_train_model_train.npy',vec_output_tr)\n","        np.save('CamemBERT_best_vec_val.npy',vec_output_val)\n","        \n","    # Record all statistics from this epoch.\n","#     training_stats.append(\n","#         {\n","#             'epoch': epoch_i + 1,\n","#             'Training Loss': avg_train_loss,\n","#             'Valid. Loss': avg_val_loss,\n","#             'Valid. Accur.': avg_val_accuracy,\n","#             'Training Time': training_time,\n","#             'Validation Time': validation_time\n","#         }\n","#     )\n","\n","print(\"\")\n","print(\"Training complete!\")\n","\n","print(\"Total training took {:} (h:mm:ss)\".format(format_time(time.time()-total_t0)))\n","\n","print()\n","\n","plt.plot(np.array(train_loss_values), 'r', label='train loss')\n","plt.plot(np.array(val_loss_values), 'b', label='val loss'  )\n","plt.legend()\n","plt.title('Loss Curve')\n","plt.show()\n","\n","print()\n","\n","plt.plot(np.array(train_accuracy_values), 'r', label='train accuracy')\n","plt.plot(np.array(val_accuracy_values), 'b', label='val accuracy'  )\n","plt.legend()\n","plt.title('Train Curve')\n","plt.show()\n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["\n","======== Epoch 1 / 10 ========\n","Training...\n","  Batch    40  of  1,548.    Elapsed: 0:00:32.\n","  Batch    80  of  1,548.    Elapsed: 0:01:04.\n","  Batch   120  of  1,548.    Elapsed: 0:01:36.\n","  Batch   160  of  1,548.    Elapsed: 0:02:08.\n","  Batch   200  of  1,548.    Elapsed: 0:02:40.\n","  Batch   240  of  1,548.    Elapsed: 0:03:12.\n","  Batch   280  of  1,548.    Elapsed: 0:03:43.\n","  Batch   320  of  1,548.    Elapsed: 0:04:15.\n","  Batch   360  of  1,548.    Elapsed: 0:04:47.\n","  Batch   400  of  1,548.    Elapsed: 0:05:19.\n","  Batch   440  of  1,548.    Elapsed: 0:05:51.\n","  Batch   480  of  1,548.    Elapsed: 0:06:23.\n","  Batch   520  of  1,548.    Elapsed: 0:06:55.\n","  Batch   560  of  1,548.    Elapsed: 0:07:27.\n","  Batch   600  of  1,548.    Elapsed: 0:07:59.\n","  Batch   640  of  1,548.    Elapsed: 0:08:31.\n","  Batch   680  of  1,548.    Elapsed: 0:09:02.\n","  Batch   720  of  1,548.    Elapsed: 0:09:34.\n","  Batch   760  of  1,548.    Elapsed: 0:10:06.\n","  Batch   800  of  1,548.    Elapsed: 0:10:38.\n","  Batch   840  of  1,548.    Elapsed: 0:11:10.\n","  Batch   880  of  1,548.    Elapsed: 0:11:42.\n","  Batch   920  of  1,548.    Elapsed: 0:12:14.\n","  Batch   960  of  1,548.    Elapsed: 0:12:46.\n","  Batch 1,000  of  1,548.    Elapsed: 0:13:18.\n","  Batch 1,040  of  1,548.    Elapsed: 0:13:49.\n","  Batch 1,080  of  1,548.    Elapsed: 0:14:21.\n","  Batch 1,120  of  1,548.    Elapsed: 0:14:53.\n","  Batch 1,160  of  1,548.    Elapsed: 0:15:25.\n","  Batch 1,200  of  1,548.    Elapsed: 0:15:57.\n","  Batch 1,240  of  1,548.    Elapsed: 0:16:29.\n","  Batch 1,280  of  1,548.    Elapsed: 0:17:01.\n","  Batch 1,320  of  1,548.    Elapsed: 0:17:33.\n","  Batch 1,360  of  1,548.    Elapsed: 0:18:05.\n","  Batch 1,400  of  1,548.    Elapsed: 0:18:37.\n","  Batch 1,440  of  1,548.    Elapsed: 0:19:09.\n","  Batch 1,480  of  1,548.    Elapsed: 0:19:40.\n","  Batch 1,520  of  1,548.    Elapsed: 0:20:12.\n","\n","Training Accuracy: 0.7805770887166236\n","\n","  Average training loss: 0.7271345324275259 \n","  Training epcoh took: 0:20:34 \n","  Learning rate:  1.8e-05\n","\n","Running Validation...\n","  Accuracy: 0.8351814516129032\n","  Validation Loss: 0.5278217393125213\n","  Validation took: 0:00:45\n","Validation F1-Score: 0.7799000589972309\n","\n","======== Epoch 2 / 10 ========\n","Training...\n","  Batch    40  of  1,548.    Elapsed: 0:00:32.\n","  Batch    80  of  1,548.    Elapsed: 0:01:04.\n","  Batch   120  of  1,548.    Elapsed: 0:01:36.\n","  Batch   160  of  1,548.    Elapsed: 0:02:08.\n","  Batch   200  of  1,548.    Elapsed: 0:02:40.\n","  Batch   240  of  1,548.    Elapsed: 0:03:12.\n","  Batch   280  of  1,548.    Elapsed: 0:03:44.\n","  Batch   320  of  1,548.    Elapsed: 0:04:16.\n","  Batch   360  of  1,548.    Elapsed: 0:04:48.\n","  Batch   400  of  1,548.    Elapsed: 0:05:19.\n","  Batch   440  of  1,548.    Elapsed: 0:05:51.\n","  Batch   480  of  1,548.    Elapsed: 0:06:23.\n","  Batch   520  of  1,548.    Elapsed: 0:06:55.\n","  Batch   560  of  1,548.    Elapsed: 0:07:27.\n","  Batch   600  of  1,548.    Elapsed: 0:07:59.\n","  Batch   640  of  1,548.    Elapsed: 0:08:31.\n","  Batch   680  of  1,548.    Elapsed: 0:09:03.\n","  Batch   720  of  1,548.    Elapsed: 0:09:35.\n","  Batch   760  of  1,548.    Elapsed: 0:10:07.\n","  Batch   800  of  1,548.    Elapsed: 0:10:39.\n","  Batch   840  of  1,548.    Elapsed: 0:11:11.\n","  Batch   880  of  1,548.    Elapsed: 0:11:43.\n","  Batch   920  of  1,548.    Elapsed: 0:12:15.\n","  Batch   960  of  1,548.    Elapsed: 0:12:47.\n","  Batch 1,000  of  1,548.    Elapsed: 0:13:19.\n","  Batch 1,040  of  1,548.    Elapsed: 0:13:51.\n","  Batch 1,080  of  1,548.    Elapsed: 0:14:22.\n","  Batch 1,120  of  1,548.    Elapsed: 0:14:54.\n","  Batch 1,160  of  1,548.    Elapsed: 0:15:26.\n","  Batch 1,200  of  1,548.    Elapsed: 0:15:58.\n","  Batch 1,240  of  1,548.    Elapsed: 0:16:30.\n","  Batch 1,280  of  1,548.    Elapsed: 0:17:02.\n","  Batch 1,320  of  1,548.    Elapsed: 0:17:34.\n","  Batch 1,360  of  1,548.    Elapsed: 0:18:06.\n","  Batch 1,400  of  1,548.    Elapsed: 0:18:38.\n","  Batch 1,440  of  1,548.    Elapsed: 0:19:10.\n","  Batch 1,480  of  1,548.    Elapsed: 0:19:42.\n","  Batch 1,520  of  1,548.    Elapsed: 0:20:14.\n","\n","Training Accuracy: 1.643866189348263\n","\n","  Average training loss: 0.42247573156824253 \n","  Training epcoh took: 0:20:36 \n","  Learning rate:  1.6000000000000003e-05\n","\n","Running Validation...\n","  Accuracy: 0.8524299043510878\n","  Validation Loss: 0.46629721506737\n","  Validation took: 0:00:45\n","Validation F1-Score: 0.8115738683184676\n","\n","======== Epoch 3 / 10 ========\n","Training...\n","  Batch    40  of  1,548.    Elapsed: 0:00:32.\n","  Batch    80  of  1,548.    Elapsed: 0:01:04.\n","  Batch   120  of  1,548.    Elapsed: 0:01:36.\n","  Batch   160  of  1,548.    Elapsed: 0:02:08.\n","  Batch   200  of  1,548.    Elapsed: 0:02:40.\n","  Batch   240  of  1,548.    Elapsed: 0:03:12.\n","  Batch   280  of  1,548.    Elapsed: 0:03:44.\n","  Batch   320  of  1,548.    Elapsed: 0:04:16.\n","  Batch   360  of  1,548.    Elapsed: 0:04:48.\n","  Batch   400  of  1,548.    Elapsed: 0:05:20.\n","  Batch   440  of  1,548.    Elapsed: 0:05:51.\n","  Batch   480  of  1,548.    Elapsed: 0:06:23.\n","  Batch   520  of  1,548.    Elapsed: 0:06:55.\n","  Batch   560  of  1,548.    Elapsed: 0:07:27.\n","  Batch   600  of  1,548.    Elapsed: 0:07:59.\n","  Batch   640  of  1,548.    Elapsed: 0:08:31.\n","  Batch   680  of  1,548.    Elapsed: 0:09:03.\n","  Batch   720  of  1,548.    Elapsed: 0:09:35.\n","  Batch   760  of  1,548.    Elapsed: 0:10:07.\n","  Batch   800  of  1,548.    Elapsed: 0:10:39.\n","  Batch   840  of  1,548.    Elapsed: 0:11:11.\n","  Batch   880  of  1,548.    Elapsed: 0:11:43.\n","  Batch   920  of  1,548.    Elapsed: 0:12:15.\n","  Batch   960  of  1,548.    Elapsed: 0:12:47.\n","  Batch 1,000  of  1,548.    Elapsed: 0:13:19.\n","  Batch 1,040  of  1,548.    Elapsed: 0:13:51.\n","  Batch 1,080  of  1,548.    Elapsed: 0:14:23.\n","  Batch 1,120  of  1,548.    Elapsed: 0:14:55.\n","  Batch 1,160  of  1,548.    Elapsed: 0:15:27.\n","  Batch 1,200  of  1,548.    Elapsed: 0:15:59.\n","  Batch 1,240  of  1,548.    Elapsed: 0:16:30.\n","  Batch 1,280  of  1,548.    Elapsed: 0:17:03.\n","  Batch 1,320  of  1,548.    Elapsed: 0:17:35.\n","  Batch 1,360  of  1,548.    Elapsed: 0:18:07.\n","  Batch 1,400  of  1,548.    Elapsed: 0:18:38.\n","  Batch 1,440  of  1,548.    Elapsed: 0:19:10.\n","  Batch 1,480  of  1,548.    Elapsed: 0:19:42.\n","  Batch 1,520  of  1,548.    Elapsed: 0:20:14.\n","\n","Training Accuracy: 2.5404352390180875\n","\n","  Average training loss: 0.3096447039361879 \n","  Training epcoh took: 0:20:36 \n","  Learning rate:  1.4e-05\n","\n","Running Validation...\n","  Accuracy: 0.8613383814703677\n","  Validation Loss: 0.44886123111774756\n","  Validation took: 0:00:45\n","Validation F1-Score: 0.8378579011216786\n","\n","======== Epoch 4 / 10 ========\n","Training...\n","  Batch    40  of  1,548.    Elapsed: 0:00:32.\n","  Batch    80  of  1,548.    Elapsed: 0:01:04.\n","  Batch   120  of  1,548.    Elapsed: 0:01:36.\n","  Batch   160  of  1,548.    Elapsed: 0:02:08.\n","  Batch   200  of  1,548.    Elapsed: 0:02:40.\n","  Batch   240  of  1,548.    Elapsed: 0:03:12.\n","  Batch   280  of  1,548.    Elapsed: 0:03:44.\n","  Batch   320  of  1,548.    Elapsed: 0:04:16.\n","  Batch   360  of  1,548.    Elapsed: 0:04:48.\n","  Batch   400  of  1,548.    Elapsed: 0:05:20.\n","  Batch   440  of  1,548.    Elapsed: 0:05:52.\n","  Batch   480  of  1,548.    Elapsed: 0:06:24.\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"1DzrlGLs2xsY"},"source":["# Saving and loading the model\n","\n","torch.load: Uses pickle’s unpickling facilities to deserialize pickled object files to memory."]},{"cell_type":"code","metadata":{"id":"cdxzacSvK8bl"},"source":["pwd"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"PZNDYDA9K0vW"},"source":["# model_path = '/content/drive/My Drive/Rakuten/models/90_10_Hirachical_CamemBERT_description.pt'"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"TnKuLBv42ZIS"},"source":["# #save the trained model for prediction\n","\n","# model_path = '/CamemBERT_best_model.pt'                # .pt for checkpointing models in pickle format\n","# torch.save(model.state_dict(), model_path)   # by default uses python pickle to save the objects and some metadata"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"5A2aQDAuRrjZ"},"source":["# device = torch.device(\"cuda:0\")"],"execution_count":null,"outputs":[]}]}