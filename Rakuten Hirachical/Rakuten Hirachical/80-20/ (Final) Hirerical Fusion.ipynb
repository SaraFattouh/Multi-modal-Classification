{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":" (Final) Hirerical Fusion.ipynb","provenance":[{"file_id":"10HG_Jj6e-CNcUe_Ng3H6hAbd0crPZTLT","timestamp":1618388020139},{"file_id":"1Iwp1sNEfG6ly73_MK_bi3EUEZAC05-lQ","timestamp":1617026135135},{"file_id":"1OCoA3nYHVEcUF5r3Sxaq9PuAqT1U9Lx7","timestamp":1616632198364},{"file_id":"1GEPelG8M4pwx_llUYLlmUO6jfjnUgMPl","timestamp":1616587813712},{"file_id":"1Px7sog6jBh8jhBFgrHAKeqt1maPE48jM","timestamp":1614749164976}],"collapsed_sections":[],"toc_visible":true,"machine_shape":"hm"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU","widgets":{"application/vnd.jupyter.widget-state+json":{"cabeec34a36c4f45a9ee5dae0f344186":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_d0be15b7b1a84c5891a15b38f673aa30","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_665c783098c54946a68dd1e4daa64c61","IPY_MODEL_cf761948037d44d7aeec271b2ecebe6c"]}},"d0be15b7b1a84c5891a15b38f673aa30":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"665c783098c54946a68dd1e4daa64c61":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_131fe6ad9c4c4b2a964cc0ffae5f6199","_dom_classes":[],"description":"100%","_model_name":"FloatProgressModel","bar_style":"success","max":84916,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":84916,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_742e142743214f2f85316901d688c94c"}},"cf761948037d44d7aeec271b2ecebe6c":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_feba2b1f6ee14130b1e882693b5a2c89","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 84916/84916 [00:01&lt;00:00, 70748.36it/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_d87f5739eaf14f48b12cc8ae06296a8c"}},"131fe6ad9c4c4b2a964cc0ffae5f6199":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"742e142743214f2f85316901d688c94c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"feba2b1f6ee14130b1e882693b5a2c89":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"d87f5739eaf14f48b12cc8ae06296a8c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"527051bdbaae4a71a17acc3b3700ab43":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_6750b4fa0a4b45a9a517f407fce9293e","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_7f6b1e1562864e728e6f7f2efadcacbb","IPY_MODEL_37eaab311a824af8aaa0a837d26efde9"]}},"6750b4fa0a4b45a9a517f407fce9293e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"7f6b1e1562864e728e6f7f2efadcacbb":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_d63535211fb043b3a0742fa570533540","_dom_classes":[],"description":"100%","_model_name":"FloatProgressModel","bar_style":"success","max":84916,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":84916,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_6e5921b7e5df4b818734d85b65f299d1"}},"37eaab311a824af8aaa0a837d26efde9":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_7bd5c7d33ea540e28a7a99fd48f2f313","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 84916/84916 [00:00&lt;00:00, 110688.23it/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_2996beb47f4448c184956b780f43d830"}},"d63535211fb043b3a0742fa570533540":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"6e5921b7e5df4b818734d85b65f299d1":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"7bd5c7d33ea540e28a7a99fd48f2f313":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"2996beb47f4448c184956b780f43d830":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"84b2f906c4cc440b97611e847d7ace45":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_942936be5c344665a886b27dcfce8e72","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_af4bc2fceceb43548fcbb8d593ddb531","IPY_MODEL_43eeccaeaf894bf3a144f35768333a3c"]}},"942936be5c344665a886b27dcfce8e72":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"af4bc2fceceb43548fcbb8d593ddb531":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_8fd78d7c990b4d74a1d62099f939e4f1","_dom_classes":[],"description":"100%","_model_name":"FloatProgressModel","bar_style":"success","max":84916,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":84916,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_ba0bad8f31434350b339f73a44edd2cf"}},"43eeccaeaf894bf3a144f35768333a3c":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_6b36d7322203489f8ffd52b3b0f805e9","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 84916/84916 [2:09:14&lt;00:00, 10.95it/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_e727404e940b4f6fbf93b5d6d0ae9951"}},"8fd78d7c990b4d74a1d62099f939e4f1":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"ba0bad8f31434350b339f73a44edd2cf":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"6b36d7322203489f8ffd52b3b0f805e9":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"e727404e940b4f6fbf93b5d6d0ae9951":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}}}}},"cells":[{"cell_type":"markdown","metadata":{"id":"1fCPKaFCgw23"},"source":["# Multimodal Calssification on Rakuten France Dataset\n","# Multi Modal Addition Fusion"]},{"cell_type":"code","metadata":{"id":"feRlWWySuDCt","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1619173778322,"user_tz":-120,"elapsed":1380,"user":{"displayName":"Sara Fattouh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjGrdvZo2x_8K8abskRubIgMK3SdtS4cG-aQE01nA=s64","userId":"09360594903764573322"}},"outputId":"486b1cda-3a22-4563-8da9-900ad53efcef"},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"BZouy12lm_Xv","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1619173779436,"user_tz":-120,"elapsed":2477,"user":{"displayName":"Sara Fattouh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjGrdvZo2x_8K8abskRubIgMK3SdtS4cG-aQE01nA=s64","userId":"09360594903764573322"}},"outputId":"9b03dc7a-0b93-4cd2-8433-9dc8c23f5c27"},"source":["!ls"],"execution_count":2,"outputs":[{"output_type":"stream","text":["drive  Rakuten\tsample_data\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"oG6LhxvoLTKt","executionInfo":{"status":"ok","timestamp":1619173779437,"user_tz":-120,"elapsed":2467,"user":{"displayName":"Sara Fattouh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjGrdvZo2x_8K8abskRubIgMK3SdtS4cG-aQE01nA=s64","userId":"09360594903764573322"}}},"source":["# pwd"],"execution_count":3,"outputs":[]},{"cell_type":"code","metadata":{"id":"BhBoZ1r7tBSd","executionInfo":{"status":"ok","timestamp":1619173779438,"user_tz":-120,"elapsed":2462,"user":{"displayName":"Sara Fattouh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjGrdvZo2x_8K8abskRubIgMK3SdtS4cG-aQE01nA=s64","userId":"09360594903764573322"}}},"source":["# mkdir Rakuten"],"execution_count":4,"outputs":[]},{"cell_type":"code","metadata":{"id":"rtpdRXBytE0X","executionInfo":{"status":"ok","timestamp":1619173779438,"user_tz":-120,"elapsed":2457,"user":{"displayName":"Sara Fattouh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjGrdvZo2x_8K8abskRubIgMK3SdtS4cG-aQE01nA=s64","userId":"09360594903764573322"}}},"source":["# cd './Rakuten'"],"execution_count":5,"outputs":[]},{"cell_type":"code","metadata":{"id":"Im900y65tWcT","executionInfo":{"status":"ok","timestamp":1619173779439,"user_tz":-120,"elapsed":2452,"user":{"displayName":"Sara Fattouh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjGrdvZo2x_8K8abskRubIgMK3SdtS4cG-aQE01nA=s64","userId":"09360594903764573322"}}},"source":["# mkdir models data "],"execution_count":6,"outputs":[]},{"cell_type":"code","metadata":{"id":"Jzp_vuw1MuNS","executionInfo":{"status":"ok","timestamp":1619173779439,"user_tz":-120,"elapsed":2443,"user":{"displayName":"Sara Fattouh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjGrdvZo2x_8K8abskRubIgMK3SdtS4cG-aQE01nA=s64","userId":"09360594903764573322"}}},"source":["# pwd"],"execution_count":7,"outputs":[]},{"cell_type":"code","metadata":{"id":"pxUWT_9hzEVv","executionInfo":{"status":"ok","timestamp":1619173779440,"user_tz":-120,"elapsed":2439,"user":{"displayName":"Sara Fattouh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjGrdvZo2x_8K8abskRubIgMK3SdtS4cG-aQE01nA=s64","userId":"09360594903764573322"}}},"source":["# cp '/content/drive/My Drive/Rakuten/models/CamemBERT_best_model_split_title.pt' models"],"execution_count":8,"outputs":[]},{"cell_type":"code","metadata":{"id":"IwsYn7UatdVV","executionInfo":{"status":"ok","timestamp":1619173779440,"user_tz":-120,"elapsed":2433,"user":{"displayName":"Sara Fattouh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjGrdvZo2x_8K8abskRubIgMK3SdtS4cG-aQE01nA=s64","userId":"09360594903764573322"}}},"source":["# cp '/content/drive/My Drive/Rakuten/models/CamemBERT_best_model_split_description.pt' models\n"],"execution_count":9,"outputs":[]},{"cell_type":"code","metadata":{"id":"tFVZ-GX6SDCp","executionInfo":{"status":"ok","timestamp":1619173779441,"user_tz":-120,"elapsed":2429,"user":{"displayName":"Sara Fattouh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjGrdvZo2x_8K8abskRubIgMK3SdtS4cG-aQE01nA=s64","userId":"09360594903764573322"}}},"source":["# cp '/content/drive/My Drive/Rakuten/models/FlauBERT_best_model_split_description.pt' models"],"execution_count":10,"outputs":[]},{"cell_type":"code","metadata":{"id":"OrrpaqRTui48","executionInfo":{"status":"ok","timestamp":1619173779442,"user_tz":-120,"elapsed":2425,"user":{"displayName":"Sara Fattouh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjGrdvZo2x_8K8abskRubIgMK3SdtS4cG-aQE01nA=s64","userId":"09360594903764573322"}}},"source":["# cp '/content/drive/My Drive/Rakuten/models/FlauBERT_best_model_split_title.pt' models\n"],"execution_count":11,"outputs":[]},{"cell_type":"code","metadata":{"id":"XEeo6IJHukVz","executionInfo":{"status":"ok","timestamp":1619173779442,"user_tz":-120,"elapsed":2419,"user":{"displayName":"Sara Fattouh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjGrdvZo2x_8K8abskRubIgMK3SdtS4cG-aQE01nA=s64","userId":"09360594903764573322"}}},"source":["# cp '/content/drive/My Drive/Rakuten/models/Final_RESNet_model.pt' models"],"execution_count":12,"outputs":[]},{"cell_type":"code","metadata":{"id":"NEebtBL70gi6","executionInfo":{"status":"ok","timestamp":1619173779443,"user_tz":-120,"elapsed":2414,"user":{"displayName":"Sara Fattouh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjGrdvZo2x_8K8abskRubIgMK3SdtS4cG-aQE01nA=s64","userId":"09360594903764573322"}}},"source":["# pwd"],"execution_count":13,"outputs":[]},{"cell_type":"code","metadata":{"id":"4kTwB_kSusK2","executionInfo":{"status":"ok","timestamp":1619173779444,"user_tz":-120,"elapsed":2409,"user":{"displayName":"Sara Fattouh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjGrdvZo2x_8K8abskRubIgMK3SdtS4cG-aQE01nA=s64","userId":"09360594903764573322"}}},"source":["# !cp  '/content/drive/My Drive/Rakuten/image.zip' './'"],"execution_count":14,"outputs":[]},{"cell_type":"code","metadata":{"id":"7iY2GDPw1MZh","executionInfo":{"status":"ok","timestamp":1619173779444,"user_tz":-120,"elapsed":2404,"user":{"displayName":"Sara Fattouh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjGrdvZo2x_8K8abskRubIgMK3SdtS4cG-aQE01nA=s64","userId":"09360594903764573322"}}},"source":["# ls -la"],"execution_count":15,"outputs":[]},{"cell_type":"code","metadata":{"id":"JhdiQHVFvSj3","executionInfo":{"status":"ok","timestamp":1619173779445,"user_tz":-120,"elapsed":2397,"user":{"displayName":"Sara Fattouh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjGrdvZo2x_8K8abskRubIgMK3SdtS4cG-aQE01nA=s64","userId":"09360594903764573322"}}},"source":["# !unzip  ./image.zip"],"execution_count":16,"outputs":[]},{"cell_type":"code","metadata":{"id":"uWx79doRO-pp","colab":{"base_uri":"https://localhost:8080/","height":35},"executionInfo":{"status":"ok","timestamp":1619173779445,"user_tz":-120,"elapsed":2387,"user":{"displayName":"Sara Fattouh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjGrdvZo2x_8K8abskRubIgMK3SdtS4cG-aQE01nA=s64","userId":"09360594903764573322"}},"outputId":"0b27d906-76e8-4656-ae98-13932ed4d095"},"source":["pwd"],"execution_count":17,"outputs":[{"output_type":"execute_result","data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'/content'"]},"metadata":{"tags":[]},"execution_count":17}]},{"cell_type":"code","metadata":{"id":"v1kbpdFQwrir","executionInfo":{"status":"ok","timestamp":1619173779446,"user_tz":-120,"elapsed":2376,"user":{"displayName":"Sara Fattouh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjGrdvZo2x_8K8abskRubIgMK3SdtS4cG-aQE01nA=s64","userId":"09360594903764573322"}}},"source":["#  !cp '/content/drive/My Drive/Rakuten/data/NewTest.csv' data \n","#  !cp '/content/drive/My Drive/Rakuten/data/NewTraining.csv' data \n","#  !cp '/content/drive/My Drive/Rakuten/data/catalog_english_taxonomy.tsv' data \n","#  !cp '/content/drive/My Drive/Rakuten/data/Y_train.tsv' data \n","#  !cp '/content/drive/My Drive/Rakuten/data/X_train.tsv' data"],"execution_count":18,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"id":"USx_EdDXMeU4","executionInfo":{"status":"ok","timestamp":1619173779447,"user_tz":-120,"elapsed":2366,"user":{"displayName":"Sara Fattouh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjGrdvZo2x_8K8abskRubIgMK3SdtS4cG-aQE01nA=s64","userId":"09360594903764573322"}},"outputId":"c2ebaeaf-711c-422b-acec-8b7edb6e14bc"},"source":["pwd"],"execution_count":19,"outputs":[{"output_type":"execute_result","data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'/content'"]},"metadata":{"tags":[]},"execution_count":19}]},{"cell_type":"markdown","metadata":{"id":"LXpLoRd1h5xu"},"source":["# 1. Setup"]},{"cell_type":"markdown","metadata":{"id":"FIGO5jehh8-2"},"source":["# 1.1 Using Colab GPU for Training\n","\n","Since we’ll be training a large neural network it’s best to take advantage of the free GPUs and TPUs that Google offers (in this case we’ll attach a GPU), otherwise training will take a very long time.\n","\n","A GPU can be added by going to the menu and selecting:\n","\n","Edit 🡒 Notebook Settings 🡒 Hardware accelerator 🡒 (GPU)"]},{"cell_type":"code","metadata":{"id":"-hgdtmoPf2al","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1619173781602,"user_tz":-120,"elapsed":4506,"user":{"displayName":"Sara Fattouh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjGrdvZo2x_8K8abskRubIgMK3SdtS4cG-aQE01nA=s64","userId":"09360594903764573322"}},"outputId":"08451e20-029c-4982-d686-05c184abefe2"},"source":["import os, time, datetime\n","import numpy as np\n","import pandas as pd\n","from tqdm.notebook import tqdm\n","import random\n","import logging\n","tqdm.pandas()\n","import seaborn as sns\n","from sklearn.model_selection import train_test_split\n","from google.colab import drive\n","\n","#NN Packages\n","import torch\n","import torch.nn as nn\n","from torch.utils.data import TensorDataset, random_split,DataLoader, RandomSampler, SequentialSampler\n","\n","logger = logging.getLogger(__name__)"],"execution_count":20,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/tqdm/std.py:658: FutureWarning: The Panel class is removed from pandas. Accessing it from the top-level namespace will also be removed in the next version\n","  from pandas import Panel\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"kEwI57OheXSi","executionInfo":{"status":"ok","timestamp":1619173781605,"user_tz":-120,"elapsed":4498,"user":{"displayName":"Sara Fattouh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjGrdvZo2x_8K8abskRubIgMK3SdtS4cG-aQE01nA=s64","userId":"09360594903764573322"}}},"source":["torch.manual_seed(123)\n","torch.cuda.manual_seed(123)\n","torch.backends.cudnn.enabled=False\n","torch.backends.cudnn.deterministic=True"],"execution_count":21,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7-DN_gCoh_pG","executionInfo":{"status":"ok","timestamp":1619173781605,"user_tz":-120,"elapsed":4489,"user":{"displayName":"Sara Fattouh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjGrdvZo2x_8K8abskRubIgMK3SdtS4cG-aQE01nA=s64","userId":"09360594903764573322"}},"outputId":"eb94d8cc-c713-4bb1-a4b8-d63c78a3abd3"},"source":["if torch.cuda.is_available():    \n","\n","    # Tell PyTorch to use the GPU.    \n","    device = torch.device(\"cuda\")\n","\n","    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n","\n","    print('We will use the GPU:', torch.cuda.get_device_name(0))\n","\n","# If not...\n","else:\n","    print('No GPU available, using the CPU instead.')\n","    device = torch.device(\"cpu\")"],"execution_count":22,"outputs":[{"output_type":"stream","text":["There are 1 GPU(s) available.\n","We will use the GPU: Tesla V100-SXM2-16GB\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"8dnUWVTZM8Wc","executionInfo":{"status":"ok","timestamp":1619173781605,"user_tz":-120,"elapsed":4474,"user":{"displayName":"Sara Fattouh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjGrdvZo2x_8K8abskRubIgMK3SdtS4cG-aQE01nA=s64","userId":"09360594903764573322"}},"outputId":"ee034db3-1355-4589-dbe7-cf2e61f04b5f"},"source":["gpu_info = !nvidia-smi\n","gpu_info = '\\n'.join(gpu_info)\n","if gpu_info.find('failed') >= 0:\n","  print('Select the Runtime > \"Change runtime type\" menu to enable a GPU accelerator, ')\n","  print('and then re-execute this cell.')\n","else:\n","  print(gpu_info)"],"execution_count":23,"outputs":[{"output_type":"stream","text":["Fri Apr 23 10:29:40 2021       \n","+-----------------------------------------------------------------------------+\n","| NVIDIA-SMI 465.19.01    Driver Version: 460.32.03    CUDA Version: 11.2     |\n","|-------------------------------+----------------------+----------------------+\n","| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n","| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n","|                               |                      |               MIG M. |\n","|===============================+======================+======================|\n","|   0  Tesla V100-SXM2...  Off  | 00000000:00:04.0 Off |                    0 |\n","| N/A   33C    P0    24W / 300W |      2MiB / 16160MiB |      0%      Default |\n","|                               |                      |                  N/A |\n","+-------------------------------+----------------------+----------------------+\n","                                                                               \n","+-----------------------------------------------------------------------------+\n","| Processes:                                                                  |\n","|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n","|        ID   ID                                                   Usage      |\n","|=============================================================================|\n","|  No running processes found                                                 |\n","+-----------------------------------------------------------------------------+\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"tibnb-j2mVPo"},"source":["# 1.2. Installing the Hugging Face Library - Image Pretrained Models\n","\n","Install the transformers package from **Hugging Face** which will give us a pytorch interface for working with BERT. This library contains interfaces for other pretrained language models.\n","\n","We’ve selected the pytorch interface because it strikes a nice balance between the high-level APIs (which are easy to use but don’t provide insight into how things work) and tensorflow code (which contains lots of details but often sidetracks us into lessons about tensorflow, when the purpose here is BERT!).\n","\n","At the moment, the Hugging Face library seems to be the most widely accepted and powerful pytorch interface for working with BERT. In addition to supporting a variety of different pre-trained transformer models, the library also includes pre-built modifications of these models suited to your specific task.\n","E.g \"BertForSequenceClassification\" that we will be using.\n","\n","The goal  of the **pretrainedmodels** is to:\n","\n","-  help to reproduce transfer learning setups\n","\n","-  access pretrained ConvNets with a unique interface/API inspired by torchvision."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"j_umLuXpmYwk","executionInfo":{"status":"ok","timestamp":1619173789067,"user_tz":-120,"elapsed":11921,"user":{"displayName":"Sara Fattouh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjGrdvZo2x_8K8abskRubIgMK3SdtS4cG-aQE01nA=s64","userId":"09360594903764573322"}},"outputId":"738ada37-c210-4959-b689-384ea9545be6"},"source":["!pip install transformers\n","!pip install sentencepiece\n","!pip install pretrainedmodels"],"execution_count":24,"outputs":[{"output_type":"stream","text":["Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (4.5.1)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.0.12)\n","Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n","Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers) (0.0.45)\n","Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.10.2)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.41.1)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n","Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from transformers) (3.10.1)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers) (20.9)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2020.12.5)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n","Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.0.1)\n","Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n","Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.7.4.3)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.4.1)\n","Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers) (2.4.7)\n","Requirement already satisfied: sentencepiece in /usr/local/lib/python3.7/dist-packages (0.1.95)\n","Requirement already satisfied: pretrainedmodels in /usr/local/lib/python3.7/dist-packages (0.7.4)\n","Requirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (from pretrainedmodels) (1.8.1+cu101)\n","Requirement already satisfied: munch in /usr/local/lib/python3.7/dist-packages (from pretrainedmodels) (2.5.0)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from pretrainedmodels) (4.41.1)\n","Requirement already satisfied: torchvision in /usr/local/lib/python3.7/dist-packages (from pretrainedmodels) (0.9.1+cu101)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torch->pretrainedmodels) (1.19.5)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch->pretrainedmodels) (3.7.4.3)\n","Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from munch->pretrainedmodels) (1.15.0)\n","Requirement already satisfied: pillow>=4.1.1 in /usr/local/lib/python3.7/dist-packages (from torchvision->pretrainedmodels) (7.1.2)\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"LLbB2aTdigvz"},"source":["# 2. Dataset Loading and Preprocessing"]},{"cell_type":"markdown","metadata":{"id":"WDHFkRyDjxpP"},"source":["# 2.1 Dataset Loading"]},{"cell_type":"code","metadata":{"id":"wJ0jmI1xxQ7j","executionInfo":{"status":"ok","timestamp":1619173789068,"user_tz":-120,"elapsed":11911,"user":{"displayName":"Sara Fattouh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjGrdvZo2x_8K8abskRubIgMK3SdtS4cG-aQE01nA=s64","userId":"09360594903764573322"}}},"source":["text_data_path = '/content/Rakuten/data'\n","image_data_path = '/content/Rakuten/image' "],"execution_count":25,"outputs":[]},{"cell_type":"code","metadata":{"id":"9QzA9onxAnhv","executionInfo":{"status":"ok","timestamp":1619173789068,"user_tz":-120,"elapsed":11905,"user":{"displayName":"Sara Fattouh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjGrdvZo2x_8K8abskRubIgMK3SdtS4cG-aQE01nA=s64","userId":"09360594903764573322"}}},"source":["class SigirPreprocess():\n","\n","  def __init__(self, text_data_path):\n","    \n","        self.text_data_path = text_data_path\n","        self.train = None # Merged X_train and Y_train\n","        self.dict_code_to_id = {}\n","        self.dict_id_to_code = {}\n","        self.list_tags = {} #unique type code\n","        self.sentences = []\n","        self.labels = []\n","        self.text_col = None\n","        self.X_test = None\n","\n","  def prepare_data(self):\n","        \n","        # #loading the Merged, preprocessed text data and test data\n","        # train = pd.read_csv(self.text_data_path+\"/NewTraining.csv\")\n","        # # new_train =  train[train['Description'] != \" \"]\n","        # # new_train = new_train[new_train['Description'].notna()]\n","        # self.train = train\n","\n","        catalog_eng= pd.read_csv(text_data_path+\"/catalog_english_taxonomy.tsv\",sep=\"\\t\")\n","        X_train= pd.read_csv(text_data_path+\"/X_train.tsv\",sep=\"\\t\")\n","        Y_train= pd.read_csv(text_data_path+\"/Y_train.tsv\",sep=\"\\t\")\n","        \n","        self.list_tags = list(Y_train['Prdtypecode'].unique())\n","        for i,tag in enumerate(self.list_tags):\n","            self.dict_code_to_id[tag] = i \n","            self.dict_id_to_code[i]=tag\n","        print(self.dict_code_to_id)\n","            \n","        Y_train['labels']=Y_train['Prdtypecode'].map(self.dict_code_to_id)\n","        train=pd.merge(left=X_train,right=Y_train,\n","               how='left',left_on=['Integer_id','Image_id','Product_id'],\n","               right_on=['Integer_id','Image_id','Product_id'])\n","        prod_map=pd.Series(catalog_eng['Top level category'].values,\n","                           index=catalog_eng['Prdtypecode']).to_dict()\n","\n","        train['product'] = train['Prdtypecode'].map(prod_map)\n","        train['title_len']=train['Title'].progress_apply(lambda x : len(x.split()) if pd.notna(x) else 0)\n","        train['desc_len']=train['Description'].progress_apply(lambda x : len(x.split()) if pd.notna(x) else 0)\n","        train['title_desc_len']=train['title_len'] + train['desc_len']\n","        train.loc[train['Description'].isnull(), 'Description'] = \" \"\n","        train['title_desc'] = train['Title'] + \" \" + train['Description']\n","        \n","        self.train = train\n","\n","        \n","  def get_sentences(self, text_col, remove_null_rows=True):\n","\n","       #get values of a specific column\n","        self.text_col = text_col        \n","\n","        new_train = self.train.copy()  \n","        self.sentences = new_train[text_col].values\n","        self.labels = new_train['labels'].values\n","\n","\n","  def prepare_test(self, text_col):\n","    \n","        X_test = pd.read_csv(self.text_data_path + \"/NewTest.csv\")\n","        self.X_test = X_test\n","        X_test['title_desc'] = X_test['Title'] + \" \" + X_test['Description']\n","        self.test_sentences = X_test[text_col].values\n","        return self.test_sentences\n","        "],"execution_count":26,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"8triqwglkYZO"},"source":["# 2.2 Drop Records With No Description"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":152,"referenced_widgets":["cabeec34a36c4f45a9ee5dae0f344186","d0be15b7b1a84c5891a15b38f673aa30","665c783098c54946a68dd1e4daa64c61","cf761948037d44d7aeec271b2ecebe6c","131fe6ad9c4c4b2a964cc0ffae5f6199","742e142743214f2f85316901d688c94c","feba2b1f6ee14130b1e882693b5a2c89","d87f5739eaf14f48b12cc8ae06296a8c","527051bdbaae4a71a17acc3b3700ab43","6750b4fa0a4b45a9a517f407fce9293e","7f6b1e1562864e728e6f7f2efadcacbb","37eaab311a824af8aaa0a837d26efde9","d63535211fb043b3a0742fa570533540","6e5921b7e5df4b818734d85b65f299d1","7bd5c7d33ea540e28a7a99fd48f2f313","2996beb47f4448c184956b780f43d830"]},"id":"Q_BYtGYwkSPk","executionInfo":{"status":"ok","timestamp":1619173791004,"user_tz":-120,"elapsed":13831,"user":{"displayName":"Sara Fattouh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjGrdvZo2x_8K8abskRubIgMK3SdtS4cG-aQE01nA=s64","userId":"09360594903764573322"}},"outputId":"9e017618-c5db-48e9-9cee-b71b9770a0d1"},"source":["#Load train and test data (test for specific column)\n","\n","text_col = 'title_desc'\n","\n","max_len = 256\n","\n","num_classes = 27\n","\n","Preprocess = SigirPreprocess(text_data_path)\n","\n","Preprocess.prepare_data()\n","train = Preprocess.train\n","# print(\"Trian:  \", len(Preprocess.train))\n","\n","\n","Preprocess.get_sentences(text_col)\n","# print(\"Labels: \", len(Preprocess.labels))\n","\n","# X_test = Preprocess.prepare_test(text_col)\n","# print(\"Test:   \", len(Preprocess.X_test))"],"execution_count":27,"outputs":[{"output_type":"stream","text":["{10: 0, 2280: 1, 50: 2, 1280: 3, 2705: 4, 2522: 5, 2582: 6, 1560: 7, 1281: 8, 1920: 9, 2403: 10, 1140: 11, 2583: 12, 1180: 13, 1300: 14, 2462: 15, 1160: 16, 2060: 17, 40: 18, 60: 19, 1320: 20, 1302: 21, 2220: 22, 2905: 23, 2585: 24, 1940: 25, 1301: 26}\n"],"name":"stdout"},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"cabeec34a36c4f45a9ee5dae0f344186","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=0.0, max=84916.0), HTML(value='')))"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["\n"],"name":"stdout"},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"527051bdbaae4a71a17acc3b3700ab43","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=0.0, max=84916.0), HTML(value='')))"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"D1MDFu47PX6-","executionInfo":{"status":"ok","timestamp":1619173840905,"user_tz":-120,"elapsed":63717,"user":{"displayName":"Sara Fattouh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjGrdvZo2x_8K8abskRubIgMK3SdtS4cG-aQE01nA=s64","userId":"09360594903764573322"}},"outputId":"7afaa2c1-ff01-4705-e9da-f3b14aabfc8a"},"source":["from bs4 import BeautifulSoup\n","Preprocess.train['Description'] = [BeautifulSoup(text).get_text() for text in  Preprocess.train['Description'] ]\n","Preprocess.train['Title'] = [BeautifulSoup(text).get_text() for text in  Preprocess.train['Title'] ]\n","Preprocess.train['title_desc'] = [BeautifulSoup(text).get_text() for text in  Preprocess.train['title_desc'] ]\n"],"execution_count":28,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/bs4/__init__.py:336: UserWarning: \"https://placehold.it/100x70\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n","  ' that document to Beautiful Soup.' % decoded_markup\n","/usr/local/lib/python3.7/dist-packages/bs4/__init__.py:336: UserWarning: \"https://www.pro-bems.com/IMAGES/images_1/FIGJJCT0000117/m/FIGJJCT0000117_5.jpg\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n","  ' that document to Beautiful Soup.' % decoded_markup\n","/usr/local/lib/python3.7/dist-packages/bs4/__init__.py:273: UserWarning: \"b'.'\" looks like a filename, not markup. You should probably open this file and pass the filehandle into Beautiful Soup.\n","  ' Beautiful Soup.' % markup)\n","/usr/local/lib/python3.7/dist-packages/bs4/__init__.py:336: UserWarning: \"https://www.pro-bems.com/IMAGES/images_1/FIG83X17001/m/FIG83X17001_5.jpg\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n","  ' that document to Beautiful Soup.' % decoded_markup\n","/usr/local/lib/python3.7/dist-packages/bs4/__init__.py:336: UserWarning: \"https://www.pro-bems.com/IMAGES/images/BOOKPNLIGMAG19/m/BOOKPNLIGMAG19_5.jpg\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n","  ' that document to Beautiful Soup.' % decoded_markup\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":741},"id":"P9jF_ASbMB2A","executionInfo":{"status":"ok","timestamp":1619173840906,"user_tz":-120,"elapsed":63704,"user":{"displayName":"Sara Fattouh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjGrdvZo2x_8K8abskRubIgMK3SdtS4cG-aQE01nA=s64","userId":"09360594903764573322"}},"outputId":"519174d7-925c-4882-fef4-b2a8b3878a7c"},"source":["from collections import Counter\n","import matplotlib.pyplot as plt\n","\n","\n","counter = Counter(Preprocess.labels)\n","for k,v in counter.items():\n","\tper = v / len(Preprocess.labels) * 100\n","\tprint('Class=%d, n=%d (%.3f%%)' % (k, v, per))\n","\n","print(\"\")\n","# plot the distribution\n","plt.bar(counter.keys(), counter.values())\n","plt.show()"],"execution_count":29,"outputs":[{"output_type":"stream","text":["Class=0, n=3116 (3.670%)\n","Class=1, n=4760 (5.606%)\n","Class=2, n=1681 (1.980%)\n","Class=3, n=4870 (5.735%)\n","Class=4, n=2761 (3.251%)\n","Class=5, n=4989 (5.875%)\n","Class=6, n=2589 (3.049%)\n","Class=7, n=5073 (5.974%)\n","Class=8, n=2070 (2.438%)\n","Class=9, n=4303 (5.067%)\n","Class=10, n=4774 (5.622%)\n","Class=11, n=2671 (3.145%)\n","Class=12, n=10209 (12.022%)\n","Class=13, n=764 (0.900%)\n","Class=14, n=5045 (5.941%)\n","Class=15, n=1421 (1.673%)\n","Class=16, n=3953 (4.655%)\n","Class=17, n=4993 (5.880%)\n","Class=18, n=2508 (2.954%)\n","Class=19, n=832 (0.980%)\n","Class=20, n=3241 (3.817%)\n","Class=21, n=2491 (2.933%)\n","Class=22, n=824 (0.970%)\n","Class=23, n=872 (1.027%)\n","Class=24, n=2496 (2.939%)\n","Class=25, n=803 (0.946%)\n","Class=26, n=807 (0.950%)\n","\n"],"name":"stdout"},{"output_type":"display_data","data":{"image/png":"iVBORw0KGgoAAAANSUhEUgAAAYMAAAD4CAYAAAAO9oqkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAPdUlEQVR4nO3df6zddX3H8edrVPyBU4rcNKztdrvZuKDJJrsBFo0hdisFl5UlSiCL3JluXbK66bJkov/UqCS4OFGSSdLZbsU4kKAbzWRjDWDc/gC5FcKvjvUGwbYp9GoRdUZd9b0/zqfz0N1Lufec3nPuuc9HcnO+3/f38/3ez4cvva/7/Zzv+d5UFZKk5e3nBt0BSdLgGQaSJMNAkmQYSJIwDCRJwIpBd2Chzj333BofHx90NyRpydi3b9+3qmpstm1LNgzGx8eZmpoadDckaclI8vRc25wmkiQZBpIkw0CShGEgScIwkCRhGEiSMAwkSRgGkiQMA0kSS/gTyNLpMn7tl0/Z5qnr37EIPZEWzymvDJLsSnI0yaNdtXOS7E1yoL2ubPUkuTHJdJKHk1zQtc9ka38gyWRX/TeSPNL2uTFJ+j1ISdKLeynTRH8PbDqpdi1wd1WtB+5u6wCXAevb11bgJuiEB7AduAi4ENh+IkBamz/q2u/k7yVJOs1OGQZV9VXg2EnlzcDutrwbuKKrfnN13AecneQ84FJgb1Udq6rngL3AprbtNVV1X3X+GPPNXceSJC2Shb6BvKqqjrTlZ4BVbXk1cLCr3aFWe7H6oVnqs0qyNclUkqmZmZkFdl2SdLKe7yZqv9FXH/ryUr7XjqqaqKqJsbFZH8ktSVqAhYbBs22Kh/Z6tNUPA2u72q1ptRerr5mlLklaRAsNgz3AiTuCJoE7uurXtLuKLgaeb9NJdwEbk6xsbxxvBO5q276b5OJ2F9E1XceSJC2SU37OIMktwCXAuUkO0bkr6HrgtiRbgKeBK1vzO4HLgWngB8B7AKrqWJKPAg+0dh+pqhNvSv8JnTuWXgn8S/uSJC2iU4ZBVV09x6YNs7QtYNscx9kF7JqlPgW86VT9kCSdPj6OQpJkGEiSDANJEoaBJAnDQJKEYSBJwjCQJGEYSJIwDCRJGAaSJAwDSRKGgSQJw0CShGEgScIwkCRhGEiSMAwkSRgGkiQMA0kShoEkCcNAkoRhIEnCMJAkYRhIkjAMJEkYBpIkDANJEoaBJAnDQJKEYSBJwjCQJNFjGCT58ySPJXk0yS1JXpFkXZL7k0wn+UKSM1vbl7f16bZ9vOs4H2z1J5Jc2tuQJEnzteAwSLIa+DNgoqreBJwBXAV8HLihql4PPAdsabtsAZ5r9RtaO5Kc3/Z7I7AJ+EySMxbaL0nS/PU6TbQCeGWSFcCrgCPA24Hb2/bdwBVteXNbp23fkCStfmtV/aiqvgFMAxf22C9J0jwsOAyq6jDwCeCbdELgeWAf8J2qOt6aHQJWt+XVwMG27/HW/nXd9Vn2kSQtgl6miVbS+a1+HfALwFl0pnlOmyRbk0wlmZqZmTmd30qSlpVepol+C/hGVc1U1f8AXwLeApzdpo0A1gCH2/JhYC1A2/5a4Nvd9Vn2eYGq2lFVE1U1MTY21kPXJUndegmDbwIXJ3lVm/vfADwO3Au8s7WZBO5oy3vaOm37PVVVrX5Vu9toHbAe+FoP/ZIkzdOKUzeZXVXdn+R24OvAceBBYAfwZeDWJB9rtZ1tl53A55JMA8fo3EFEVT2W5DY6QXIc2FZVP1lovyRJ87fgMACoqu3A9pPKTzLL3UBV9UPgXXMc5zrgul76IklaOD+BLEkyDCRJhoEkCcNAkoRhIEnCMJAkYRhIkjAMJEkYBpIkDANJEoaBJAnDQJKEYSBJwjCQJGEYSJIwDCRJGAaSJAwDSRKGgSQJw0CShGEgScIwkCRhGEiSMAwkSRgGkiQMA0kShoEkCcNAkoRhIEnCMJAkYRhIkjAMJEn0GAZJzk5ye5L/TLI/yW8mOSfJ3iQH2uvK1jZJbkwyneThJBd0HWeytT+QZLLXQUmS5qfXK4NPA/9aVb8K/BqwH7gWuLuq1gN3t3WAy4D17WsrcBNAknOA7cBFwIXA9hMBIklaHAsOgySvBd4G7ASoqh9X1XeAzcDu1mw3cEVb3gzcXB33AWcnOQ+4FNhbVceq6jlgL7Bpof2SJM1fL1cG64AZ4O+SPJjks0nOAlZV1ZHW5hlgVVteDRzs2v9Qq81V/3+SbE0ylWRqZmamh65Lkrr1EgYrgAuAm6rqzcB/87MpIQCqqoDq4Xu8QFXtqKqJqpoYGxvr12EladnrJQwOAYeq6v62fjudcHi2Tf/QXo+27YeBtV37r2m1ueqSpEWy4DCoqmeAg0ne0EobgMeBPcCJO4ImgTva8h7gmnZX0cXA82066S5gY5KV7Y3jja0mSVokK3rc/0+Bzyc5E3gSeA+dgLktyRbgaeDK1vZO4HJgGvhBa0tVHUvyUeCB1u4jVXWsx35JkuahpzCoqoeAiVk2bZilbQHb5jjOLmBXL32RJC2cn0CWJBkGkiTDQJKEYSBJwjCQJGEYSJIwDCRJGAaSJAwDSRKGgSQJw0CShGEgScIwkCTR+yOstcyMX/vlU7Z56vp3LEJPlib/+2lYeWUgSfLKYFT5G6hejP9/6GReGUiSvDIYJH87kzQsvDKQJHllcCr+9j6cPC9Sf3llIEkyDCRJhoEkCcNAkoRhIEnCMJAkYRhIkjAMJEkYBpIk/ASyhoifKpYGxysDSZJhIEnqQxgkOSPJg0n+ua2vS3J/kukkX0hyZqu/vK1Pt+3jXcf4YKs/keTSXvskSZqfflwZvA/Y37X+ceCGqno98BywpdW3AM+1+g2tHUnOB64C3ghsAj6T5Iw+9EuS9BL1FAZJ1gDvAD7b1gO8Hbi9NdkNXNGWN7d12vYNrf1m4Naq+lFVfQOYBi7spV+SpPnp9crgU8BfAj9t668DvlNVx9v6IWB1W14NHARo259v7f+vPss+L5Bka5KpJFMzMzM9dl2SdMKCby1N8jvA0aral+SS/nVpblW1A9gBMDExUYvxPaWlwltz1YtePmfwFuB3k1wOvAJ4DfBp4OwkK9pv/2uAw639YWAtcCjJCuC1wLe76id07yNJWgQLniaqqg9W1ZqqGqfzBvA9VfX7wL3AO1uzSeCOtrynrdO231NV1epXtbuN1gHrga8ttF+SpPk7HZ9A/gBwa5KPAQ8CO1t9J/C5JNPAMToBQlU9luQ24HHgOLCtqn5yGvolSZpDX8Kgqr4CfKUtP8ksdwNV1Q+Bd82x/3XAdf3oiyRp/vwEsiTJMJAkGQaSJAwDSRKGgSQJw0CShGEgScIwkCRhGEiSOD2Po5Ckl8QnrQ4PrwwkSYaBJGmZThN5aSpJL+SVgSTJMJAkGQaSJAwDSRKGgSQJw0CSxDK9tXQp8nZYSaeTVwaSJK8MtDx4ZSW9OMNA/qCU5DSRJMkrA0mnMCxXjvbj9PLKQJJkGEiSDANJEoaBJAnDQJKEYSBJoocwSLI2yb1JHk/yWJL3tfo5SfYmOdBeV7Z6ktyYZDrJw0ku6DrWZGt/IMlk78OSJM1HL1cGx4G/qKrzgYuBbUnOB64F7q6q9cDdbR3gMmB9+9oK3ASd8AC2AxcBFwLbTwSIJGlxLPhDZ1V1BDjSlr+XZD+wGtgMXNKa7Qa+Anyg1W+uqgLuS3J2kvNa271VdQwgyV5gE3DLQvum4TGqH9CRRk1f3jNIMg68GbgfWNWCAuAZYFVbXg0c7NrtUKvNVZ/t+2xNMpVkamZmph9dlyTRhzBI8mrgi8D7q+q73dvaVUD1+j26jrejqiaqamJsbKxfh5WkZa+nMEjyMjpB8Pmq+lIrP9umf2ivR1v9MLC2a/c1rTZXXZK0SHq5myjATmB/VX2ya9Me4MQdQZPAHV31a9pdRRcDz7fppLuAjUlWtjeON7aaJGmR9PLU0rcA7wYeSfJQq30IuB64LckW4GngyrbtTuByYBr4AfAegKo6luSjwAOt3UdOvJksSVocvdxN9B9A5ti8YZb2BWyb41i7gF0L7YskqTd+AlmS5B+36Tfvq5e0FHllIEkyDCRJhoEkCcNAkoRvIEtDy5sRtJi8MpAkGQaSJMNAkoRhIEnCMJAkYRhIkjAMJEkYBpIkDANJEoaBJAkfRyFJPvoDw0BSnw3LD9Zh6cd8DLLPhoHUg5fyjxeG74eOFs9SCSXfM5AkGQaSJMNAkoRhIEnCMJAkYRhIkjAMJEkYBpIkDANJEoaBJAnDQJKEYSBJYojCIMmmJE8kmU5y7aD7I0nLyVCEQZIzgL8BLgPOB65Ocv5geyVJy8dQhAFwITBdVU9W1Y+BW4HNA+6TJC0bqapB94Ek7wQ2VdUftvV3AxdV1XtParcV2NpW3wA80acunAt8q0/HGlaOcTSM+hhHfXww2DH+UlWNzbZhSf1xm6raAezo93GTTFXVRL+PO0wc42gY9TGO+vhgeMc4LNNEh4G1XetrWk2StAiGJQweANYnWZfkTOAqYM+A+yRJy8ZQTBNV1fEk7wXuAs4AdlXVY4vYhb5PPQ0hxzgaRn2Moz4+GNIxDsUbyJKkwRqWaSJJ0gAZBpIkw2A5PAYjyVNJHknyUJKpQfenH5LsSnI0yaNdtXOS7E1yoL2uHGQfezHH+D6c5HA7jw8luXyQfexVkrVJ7k3yeJLHkryv1UfpPM41xqE7l8v6PYP2GIz/An4bOETnrqarq+rxgXasz5I8BUxU1ch8mCfJ24DvAzdX1Zta7a+AY1V1fQv2lVX1gUH2c6HmGN+Hge9X1ScG2bd+SXIecF5VfT3JzwP7gCuAP2B0zuNcY7ySITuXy/3KwMdgLFFV9VXg2EnlzcDutrybzj+6JWmO8Y2UqjpSVV9vy98D9gOrGa3zONcYh85yD4PVwMGu9UMM6YnqUQH/lmRfe6THqFpVVUfa8jPAqkF25jR5b5KH2zTSkp0+OVmSceDNwP2M6Hk8aYwwZOdyuYfBcvHWqrqAzlNht7UpiJFWnfnPUZsDvQn4FeDXgSPAXw+2O/2R5NXAF4H3V9V3u7eNynmcZYxDdy6Xexgsi8dgVNXh9noU+Ec602Oj6Nk2R3tirvbogPvTV1X1bFX9pKp+CvwtI3Aek7yMzg/Jz1fVl1p5pM7jbGMcxnO53MNg5B+DkeSs9sYVSc4CNgKPvvheS9YeYLItTwJ3DLAvfXfiB2Tzeyzx85gkwE5gf1V9smvTyJzHucY4jOdyWd9NBNBu6foUP3sMxnUD7lJfJfllOlcD0Hn8yD+MwhiT3AJcQudxwM8C24F/Am4DfhF4Griyqpbkm7BzjO8SOtMKBTwF/HHX3PqSk+StwL8DjwA/beUP0ZlTH5XzONcYr2bIzuWyDwNJktNEkiQMA0kShoEkCcNAkoRhIEnCMJAkYRhIkoD/BZk1yKpFpmdjAAAAAElFTkSuQmCC\n","text/plain":["<Figure size 432x288 with 1 Axes>"]},"metadata":{"tags":[],"needs_background":"light"}}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"qguSL6pMHQve","executionInfo":{"status":"ok","timestamp":1619173840907,"user_tz":-120,"elapsed":63690,"user":{"displayName":"Sara Fattouh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjGrdvZo2x_8K8abskRubIgMK3SdtS4cG-aQE01nA=s64","userId":"09360594903764573322"}},"outputId":"7cd4bde8-9e23-44f7-9a6b-b30b6820e336"},"source":["print(Preprocess.train['Title'].isnull().sum())\n","\n","print(Preprocess.train['Description'].isnull().sum())\n","\n","print(Preprocess.train['Image_id'].isnull().sum())\n","\n","print(Preprocess.train['Product_id'].isnull().sum())\n","\n","print(Preprocess.train['Prdtypecode'].isnull().sum())\n","\n","print(Preprocess.train['labels'].isnull().sum())\n","\n","print(Preprocess.train['product'].isnull().sum()) #top level category\n","\n","print(Preprocess.train['title_desc'].isnull().sum())\n","\n","\n"],"execution_count":30,"outputs":[{"output_type":"stream","text":["0\n","0\n","0\n","0\n","0\n","0\n","0\n","0\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9gGj4NDbINdl","executionInfo":{"status":"ok","timestamp":1619173840908,"user_tz":-120,"elapsed":63678,"user":{"displayName":"Sara Fattouh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjGrdvZo2x_8K8abskRubIgMK3SdtS4cG-aQE01nA=s64","userId":"09360594903764573322"}},"outputId":"2a85aab9-3380-4a22-f205-be2f7c5d8d71"},"source":["Preprocess.train.isnull().values.any()"],"execution_count":31,"outputs":[{"output_type":"execute_result","data":{"text/plain":["False"]},"metadata":{"tags":[]},"execution_count":31}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"8797SZsJIVnP","executionInfo":{"status":"ok","timestamp":1619173841339,"user_tz":-120,"elapsed":64095,"user":{"displayName":"Sara Fattouh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjGrdvZo2x_8K8abskRubIgMK3SdtS4cG-aQE01nA=s64","userId":"09360594903764573322"}},"outputId":"c07ed0f3-004d-4e34-a91e-1e855cd114ee"},"source":["Preprocess.train.isnull().sum().sum()"],"execution_count":32,"outputs":[{"output_type":"execute_result","data":{"text/plain":["0"]},"metadata":{"tags":[]},"execution_count":32}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":946},"id":"jBCwmgodH5wL","executionInfo":{"status":"ok","timestamp":1619173841340,"user_tz":-120,"elapsed":64081,"user":{"displayName":"Sara Fattouh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjGrdvZo2x_8K8abskRubIgMK3SdtS4cG-aQE01nA=s64","userId":"09360594903764573322"}},"outputId":"d81ea004-875d-4894-b75d-f6aebb7013aa"},"source":["Preprocess.train"],"execution_count":33,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Integer_id</th>\n","      <th>Title</th>\n","      <th>Description</th>\n","      <th>Image_id</th>\n","      <th>Product_id</th>\n","      <th>Prdtypecode</th>\n","      <th>labels</th>\n","      <th>product</th>\n","      <th>title_len</th>\n","      <th>desc_len</th>\n","      <th>title_desc_len</th>\n","      <th>title_desc</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0</td>\n","      <td>Olivia: Personalisiertes Notizbuch / 150 Seite...</td>\n","      <td></td>\n","      <td>1263597046</td>\n","      <td>3804725264</td>\n","      <td>10</td>\n","      <td>0</td>\n","      <td>Books</td>\n","      <td>14</td>\n","      <td>0</td>\n","      <td>14</td>\n","      <td>Olivia: Personalisiertes Notizbuch / 150 Seite...</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>1</td>\n","      <td>Journal Des Arts (Le) N° 133 Du 28/09/2001 - L...</td>\n","      <td></td>\n","      <td>1008141237</td>\n","      <td>436067568</td>\n","      <td>2280</td>\n","      <td>1</td>\n","      <td>Books</td>\n","      <td>39</td>\n","      <td>0</td>\n","      <td>39</td>\n","      <td>Journal Des Arts (Le) N° 133 Du 28/09/2001 - L...</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>2</td>\n","      <td>Grand Stylet Ergonomique Bleu Gamepad Nintendo...</td>\n","      <td>PILOT STYLE Touch Pen de marque Speedlink est ...</td>\n","      <td>938777978</td>\n","      <td>201115110</td>\n","      <td>50</td>\n","      <td>2</td>\n","      <td>Entertainment</td>\n","      <td>12</td>\n","      <td>109</td>\n","      <td>121</td>\n","      <td>Grand Stylet Ergonomique Bleu Gamepad Nintendo...</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>3</td>\n","      <td>Peluche Donald - Europe - Disneyland 2000 (Mar...</td>\n","      <td></td>\n","      <td>457047496</td>\n","      <td>50418756</td>\n","      <td>1280</td>\n","      <td>3</td>\n","      <td>Child</td>\n","      <td>10</td>\n","      <td>0</td>\n","      <td>10</td>\n","      <td>Peluche Donald - Europe - Disneyland 2000 (Mar...</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>4</td>\n","      <td>La Guerre Des Tuques</td>\n","      <td>Luc a des idées de grandeur. Il veut organiser...</td>\n","      <td>1077757786</td>\n","      <td>278535884</td>\n","      <td>2705</td>\n","      <td>4</td>\n","      <td>Books</td>\n","      <td>4</td>\n","      <td>34</td>\n","      <td>38</td>\n","      <td>La Guerre Des Tuques Luc a des idées de grande...</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>84911</th>\n","      <td>84911</td>\n","      <td>The Sims [ Import Anglais ]</td>\n","      <td></td>\n","      <td>941495734</td>\n","      <td>206719094</td>\n","      <td>40</td>\n","      <td>18</td>\n","      <td>Entertainment</td>\n","      <td>6</td>\n","      <td>0</td>\n","      <td>6</td>\n","      <td>The Sims [ Import Anglais ]</td>\n","    </tr>\n","    <tr>\n","      <th>84912</th>\n","      <td>84912</td>\n","      <td>Kit piscine acier NEVADA déco pierre Ø 3.50m x...</td>\n","      <td>Description complète :Kit piscine hors-sol Toi...</td>\n","      <td>1188462883</td>\n","      <td>3065095706</td>\n","      <td>2583</td>\n","      <td>12</td>\n","      <td>Household</td>\n","      <td>10</td>\n","      <td>190</td>\n","      <td>200</td>\n","      <td>Kit piscine acier NEVADA déco pierre Ø 3.50m x...</td>\n","    </tr>\n","    <tr>\n","      <th>84913</th>\n","      <td>84913</td>\n","      <td>Journal Officiel De La Republique Francaise N°...</td>\n","      <td></td>\n","      <td>1009325617</td>\n","      <td>440707564</td>\n","      <td>2280</td>\n","      <td>1</td>\n","      <td>Books</td>\n","      <td>36</td>\n","      <td>0</td>\n","      <td>36</td>\n","      <td>Journal Officiel De La Republique Francaise N°...</td>\n","    </tr>\n","    <tr>\n","      <th>84914</th>\n","      <td>84914</td>\n","      <td>Table Basse Bois De Récupération Massif Base B...</td>\n","      <td>Cette table basse a un design unique et consti...</td>\n","      <td>1267353403</td>\n","      <td>3942400296</td>\n","      <td>1560</td>\n","      <td>7</td>\n","      <td>Household</td>\n","      <td>9</td>\n","      <td>262</td>\n","      <td>271</td>\n","      <td>Table Basse Bois De Récupération Massif Base B...</td>\n","    </tr>\n","    <tr>\n","      <th>84915</th>\n","      <td>84915</td>\n","      <td>Gomme De Collection 2 Gommes Pinguin Glace Ver...</td>\n","      <td></td>\n","      <td>684671297</td>\n","      <td>57203227</td>\n","      <td>2522</td>\n","      <td>5</td>\n","      <td>Books</td>\n","      <td>9</td>\n","      <td>0</td>\n","      <td>9</td>\n","      <td>Gomme De Collection 2 Gommes Pinguin Glace Ver...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>84916 rows × 12 columns</p>\n","</div>"],"text/plain":["       Integer_id  ...                                         title_desc\n","0               0  ...  Olivia: Personalisiertes Notizbuch / 150 Seite...\n","1               1  ...  Journal Des Arts (Le) N° 133 Du 28/09/2001 - L...\n","2               2  ...  Grand Stylet Ergonomique Bleu Gamepad Nintendo...\n","3               3  ...  Peluche Donald - Europe - Disneyland 2000 (Mar...\n","4               4  ...  La Guerre Des Tuques Luc a des idées de grande...\n","...           ...  ...                                                ...\n","84911       84911  ...                      The Sims [ Import Anglais ]  \n","84912       84912  ...  Kit piscine acier NEVADA déco pierre Ø 3.50m x...\n","84913       84913  ...  Journal Officiel De La Republique Francaise N°...\n","84914       84914  ...  Table Basse Bois De Récupération Massif Base B...\n","84915       84915  ...  Gomme De Collection 2 Gommes Pinguin Glace Ver...\n","\n","[84916 rows x 12 columns]"]},"metadata":{"tags":[]},"execution_count":33}]},{"cell_type":"markdown","metadata":{"id":"VAqoRpq9zMS4"},"source":["# View Tokenizer Input "]},{"cell_type":"code","metadata":{"id":"lhxfSqOKlR94","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1619173841341,"user_tz":-120,"elapsed":64065,"user":{"displayName":"Sara Fattouh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjGrdvZo2x_8K8abskRubIgMK3SdtS4cG-aQE01nA=s64","userId":"09360594903764573322"}},"outputId":"fd31fe52-9871-4ac5-b9ea-deceacd9897c"},"source":["Preprocess.get_sentences(text_col,True)\n","sentences = Preprocess.sentences\n","\n","labels = Preprocess.labels\n","print(sentences)\n"],"execution_count":34,"outputs":[{"output_type":"stream","text":["['Olivia: Personalisiertes Notizbuch / 150 Seiten / Punktraster / Ca Din A5 / Rosen-Design  '\n"," \"Journal Des Arts (Le) N° 133 Du 28/09/2001 - L'art Et Son Marche Salon D'art Asiatique A Paris - Jacques Barrere - Francois Perrier - La Reforme Des Ventes Aux Encheres Publiques - Le Sna Fete Ses Cent Ans.  \"\n"," \"Grand Stylet Ergonomique Bleu Gamepad Nintendo Wii U - Speedlink Pilot Style PILOT STYLE Touch Pen de marque Speedlink est 1 stylet ergonomique pour GamePad Nintendo Wii U. Pour un confort optimal et une précision maximale sur le GamePad de la Wii U: ce grand stylet hautement ergonomique est non seulement parfaitement adapté à votre main mais aussi très élégant. Il est livré avec un support qui se fixe sans adhésif à l'arrière du GamePad  Caractéristiques: Modèle: Speedlink PILOT STYLE Touch Pen Couleur: Bleu Ref. Fabricant: SL-3468-BE Compatibilité: GamePad Nintendo Wii U Forme particulièrement ergonomique excellente tenue en main Pointe à revêtement longue durée conçue pour ne pas abîmer l'écran tactile En bonus : Support inclu pour GamePad \"\n"," ...\n"," \"Journal Officiel De La Republique Francaise N° 46 Du 15/02/1871 - Changement D'adresses - Partie Officielle - Partie Non Officielle - Elections A L'assemblee Nationale - Ravitaillement De Paris - Nouvelles Etrangeres  -  Italie  -  Amerique.  \"\n"," \"Table Basse Bois De Récupération Massif Base Blanche 60x60x33cm Cette table basse a un design unique et constituera un ajout intemporel à votre maison. Son dessus de table en bois massif est idéal pour ranger vos boissons panier de fruits ou objets décoratifs et sa base en acier solide ajoute à la robustesse de la table d'appoint. La table basse est faite de bois de récupération massif provenant de solives de planchers et de poutres de soutien de vieux bâtiments en cours de démolition et peut être composée de différents types de bois comme le Sesham (bois de rose) le pin le teck le hêtre le chêne le cèdre le bois de manguier l'acacia etc. Cela signifie que le bois de récupération conserve les caractéristiques de ces différents types de bois. Le bois récupéré est déjà vieilli patiné et séché de sorte qu'il ne rétrécit pas ne se plie pas et n'a pas besoin d'une finition. Chaque étape du processus est réalisée avec le plus grand soin que ce soit le ponçage la peinture ou le laquage. Les belles fibres de bois rendent chaque meuble unique et légèrement différent du suivant. Les signes d'usure et la structure fibreuse visible donnent à chaque pièce son histoire et un aspect unique. L'article est déjà assemblé ; aucun assemblage n'est requis. Remarque importante : les couleurs varient d'un meuble à l'autre rendant chacune de nos tables basses unique la livraison est aléatoire. Couleur de base : BlancMatériau : dessus de table en bois massif de récupération + base en acierDimensions : 60 x 33 cm (Diam. x H)Produit poncé peint et laquéAucun assemblage requis\"\n"," 'Gomme De Collection 2 Gommes Pinguin Glace Vert Orange  ']\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"XXlPVw5U1pos","executionInfo":{"status":"ok","timestamp":1619173841341,"user_tz":-120,"elapsed":64052,"user":{"displayName":"Sara Fattouh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjGrdvZo2x_8K8abskRubIgMK3SdtS4cG-aQE01nA=s64","userId":"09360594903764573322"}},"outputId":"c61c5b02-a299-415a-ec0c-6f4a870c3da7"},"source":["print (type(sentences))\n","print()\n","# print(\"Total number of sentences:{}, labels:{}\".format(len(sentences), len(labels)))"],"execution_count":35,"outputs":[{"output_type":"stream","text":["<class 'numpy.ndarray'>\n","\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"xwZDh0jf9FRc"},"source":["# View Test Sentences"]},{"cell_type":"code","metadata":{"id":"Sjmz7l6rnefT","executionInfo":{"status":"ok","timestamp":1619173841342,"user_tz":-120,"elapsed":64041,"user":{"displayName":"Sara Fattouh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjGrdvZo2x_8K8abskRubIgMK3SdtS4cG-aQE01nA=s64","userId":"09360594903764573322"}}},"source":["# batch_size = 32  \n","\n","# test_sentences = Preprocess.test_sentences\n","\n","# X_test_phase1  = Preprocess.X_test"],"execution_count":36,"outputs":[]},{"cell_type":"code","metadata":{"id":"8QV2kcCt9Oz6","executionInfo":{"status":"ok","timestamp":1619173841342,"user_tz":-120,"elapsed":64034,"user":{"displayName":"Sara Fattouh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjGrdvZo2x_8K8abskRubIgMK3SdtS4cG-aQE01nA=s64","userId":"09360594903764573322"}}},"source":["# print(len(test_sentences))"],"execution_count":37,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"oVemBMlCk4xW"},"source":["**Helper Function**"]},{"cell_type":"code","metadata":{"id":"WV-Xbdjxk3Ii","executionInfo":{"status":"ok","timestamp":1619173841343,"user_tz":-120,"elapsed":64029,"user":{"displayName":"Sara Fattouh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjGrdvZo2x_8K8abskRubIgMK3SdtS4cG-aQE01nA=s64","userId":"09360594903764573322"}}},"source":["def format_time(elapsed):\n","    '''\n","    Takes a time in seconds and returns a string hh:mm:ss\n","    '''\n","    # Round to the nearest second.\n","    elapsed_rounded = int(round((elapsed)))\n","    \n","    # Format as hh:mm:ss\n","    return str(datetime.timedelta(seconds=elapsed_rounded))"],"execution_count":38,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ZIbKzQwxlrZm"},"source":["# 3. Tokenization & Input Formatting\n"," Transform our dataset into the format that BERT can be trained on."]},{"cell_type":"markdown","metadata":{"id":"Ud5qFfdmlwsk"},"source":["# 3.1. BERT Tokenizer\n","To feed our text to BERT, it must be split into tokens, and then these tokens must be mapped to their index in the tokenizer vocabulary.\n","\n","The tokenization must be performed by the tokenizer included within BERT"]},{"cell_type":"code","metadata":{"id":"AbEg-aB2l61a","executionInfo":{"status":"ok","timestamp":1619173845815,"user_tz":-120,"elapsed":68495,"user":{"displayName":"Sara Fattouh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjGrdvZo2x_8K8abskRubIgMK3SdtS4cG-aQE01nA=s64","userId":"09360594903764573322"}}},"source":["from transformers import XLMForSequenceClassification\n","from transformers import FlaubertModel, FlaubertTokenizer,FlaubertForSequenceClassification,AdamW, FlaubertConfig \n","\n","from torch.nn import Dropout,Conv1d, Linear\n","from transformers.modeling_utils import SequenceSummary\n","\n","#from transformers.modeling_roberta import RobertaClassificationHead\n"],"execution_count":39,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"0l3oZ1Tims3q"},"source":["# 3.2. Required Formatting\n","We are required to give it a number of pieces of information\n","\n","We need to:\n","\n","Add special tokens to the start and end of each sentence.\n","Pad & truncate all sentences to a single constant length.\n","Explicitly differentiate real tokens from padding tokens with the “attention mask”."]},{"cell_type":"markdown","metadata":{"id":"fZvQKKcxmu4L"},"source":["# 3.3. Tokenize Dataset\n","We will use \"encode_plus\":\n","\n","returns a dictionary containing the encoded sequence or sequence pair and additional information: the mask for sequence classification and the overflowing elements if a max_length is specified."]},{"cell_type":"code","metadata":{"id":"lSBvH82Amx_3","executionInfo":{"status":"ok","timestamp":1619173845816,"user_tz":-120,"elapsed":68491,"user":{"displayName":"Sara Fattouh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjGrdvZo2x_8K8abskRubIgMK3SdtS4cG-aQE01nA=s64","userId":"09360594903764573322"}}},"source":["def prep_input(sentences,labels, max_len,tokenizer):\n","    input_ids = []\n","    attention_masks = []\n","\n","    # For every sentence...\n","    for sent in sentences:\n","        # `encode_plus` will:\n","        #   (1) Tokenize the sentence.\n","        #   (2) Prepend the `[CLS]` token to the start.\n","        #   (3) Append the `[SEP]` token to the end.\n","        #   (4) Map tokens to their IDs.\n","        #   (5) Pad or truncate the sentence to `max_length`\n","        #   (6) Create attention masks for [PAD] tokens.\n","        encoded_dict = tokenizer.encode_plus(\n","                            sent,                           # Sentence to encode.\n","                            add_special_tokens = True,      # Add '[CLS]' and '[SEP]'\n","                            max_length = max_len,           # Pad & truncate all sentences.\n","                            pad_to_max_length = True,\n","                            return_attention_mask = True,   # Construct attn. masks.\n","                            return_tensors = 'pt',     # Return pytorch tensors.\n","                       )\n","\n","        # Add the encoded sentence to the list.    \n","        input_ids.append(encoded_dict['input_ids'])       # IDs of the the vocabularies in the Model's dictionary\n","\n","        # And its attention mask (simply differentiates padding from non-padding).\n","        attention_masks.append(encoded_dict['attention_mask'])\n","\n","    # Convert the lists into tensors. \n","    input_ids = torch.cat(input_ids, dim=0)             # Concatenates the given sequence of seq tensors in the given dimension. \n","                                                        # All tensors must  have the same shape \n","    attention_masks = torch.cat(attention_masks, dim=0)\n","\n","    if labels is not None:\n","        labels = torch.tensor(labels)\n","        return input_ids, attention_masks, labels\n","    else:\n","        return input_ids, attention_masks"],"execution_count":40,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"KVKdFMMmP8aT"},"source":["# 3.4. Importing Tokenizers and Input Preparation\n","\n","- Now it is time to import both Camembert and FlauBERT tokenizers from  pretained package and prepare the input using them. \n","\n","- Calling prep_input() for each model will result in the corresponding:\n","     \n","\n","1.   **input ids**\n","2.   **attention maks**\n","3.   **labels**\n","\n"," "]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-BYzJChjw5AG","executionInfo":{"status":"ok","timestamp":1619173849084,"user_tz":-120,"elapsed":71749,"user":{"displayName":"Sara Fattouh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjGrdvZo2x_8K8abskRubIgMK3SdtS4cG-aQE01nA=s64","userId":"09360594903764573322"}},"outputId":"0a1505ba-b323-4e0d-eb2e-483362ce716d"},"source":["from transformers import CamembertConfig, CamembertTokenizer, CamembertModel, CamembertForSequenceClassification, AdamW\n","from transformers import FlaubertModel, FlaubertTokenizer,FlaubertForSequenceClassification,AdamW, FlaubertConfig \n","\n","print('Using Camembert')\n","tokenizer_cam = CamembertTokenizer.from_pretrained('camembert-base', do_lowercase=False)\n","print('Using Flaubert')\n","tokenizer_flau = FlaubertTokenizer.from_pretrained('flaubert/flaubert_base_cased', do_lowercase=False)"],"execution_count":41,"outputs":[{"output_type":"stream","text":["Using Camembert\n","Using Flaubert\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"vAvaWiVEwq54","executionInfo":{"status":"ok","timestamp":1619173906632,"user_tz":-120,"elapsed":129283,"user":{"displayName":"Sara Fattouh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjGrdvZo2x_8K8abskRubIgMK3SdtS4cG-aQE01nA=s64","userId":"09360594903764573322"}},"outputId":"6ba20dc4-ac3a-4915-fded-008f9520459e"},"source":["input_ids_cam, attention_masks_cam, labels_cam = prep_input (sentences, labels, max_len, tokenizer_cam)\n"],"execution_count":42,"outputs":[{"output_type":"stream","text":["Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"o-4ArTutx-Cx","executionInfo":{"status":"ok","timestamp":1619174079769,"user_tz":-120,"elapsed":302407,"user":{"displayName":"Sara Fattouh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjGrdvZo2x_8K8abskRubIgMK3SdtS4cG-aQE01nA=s64","userId":"09360594903764573322"}},"outputId":"5c9fb012-981f-4319-d938-f6ce063e3561"},"source":["input_ids_flau, attention_masks_flau, labels_flau  = prep_input(sentences,labels, max_len,tokenizer_flau)"],"execution_count":43,"outputs":[{"output_type":"stream","text":["Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"],"name":"stderr"}]},{"cell_type":"markdown","metadata":{"id":"7XrGAY_zRX--"},"source":["# 3.5. Training & Validation Split\n","Divide up our training randomly select **10%** as a validation set off of the training set.\n","\n","While splitting, we used the following parameters:\n","\n","\n","1.   **stratify**: \n","in this context, stratification means that the train_test_split method returns training and test subsets that have the same proportions of class labels as the input dataset.\n","2.   **random_state**: \n","simply sets a seed to the random generator, so that your train-test splits are always deterministic. If you don't set a seed, it is different each time."]},{"cell_type":"code","metadata":{"id":"nZQ0mB1NyeWa","executionInfo":{"status":"ok","timestamp":1619174079770,"user_tz":-120,"elapsed":302398,"user":{"displayName":"Sara Fattouh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjGrdvZo2x_8K8abskRubIgMK3SdtS4cG-aQE01nA=s64","userId":"09360594903764573322"}}},"source":["# val_size = 0.15\n","#basic ---------------\n","\n","# tr_inputs_cam, val_inputs_cam, _,_ = train_test_split (input_ids_cam, labels_cam, stratify = labels_cam,    \n","#                                                             random_state=2020, test_size = val_size)\n","\n","# tr_masks_cam, val_masks_cam, _,_ =   train_test_split (attention_masks_cam, labels, stratify = labels,        # labels: Preprocess.labels\n","#                                                             random_state=2020, test_size = val_size)\n","\n","\n","# tr_inputs_flau, val_inputs_flau, _,_ = train_test_split (input_ids_flau, labels_flau, stratify=labels,\n","#                                                             random_state=2020, test_size = val_size)\n","\n","# tr_masks_flau, val_masks_flau, _,_   = train_test_split (attention_masks_flau, labels,stratify=labels_flau,  # labels: Preprocess.labels\n","#                                                             random_state=2020, test_size = val_size)"],"execution_count":44,"outputs":[]},{"cell_type":"code","metadata":{"id":"R9_fzCmV2JBI","executionInfo":{"status":"ok","timestamp":1619174079771,"user_tz":-120,"elapsed":302393,"user":{"displayName":"Sara Fattouh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjGrdvZo2x_8K8abskRubIgMK3SdtS4cG-aQE01nA=s64","userId":"09360594903764573322"}}},"source":["# tr_inputs, test_inputs_cam, tr_labels, test_labels_cam = train_test_split(input_ids_cam, labels_cam, stratify=labels_cam, random_state=2020,\n","#                                                                 test_size = 0.2)\n","\n","# tr_inputs_cam, val_inputs_cam, train_labels, val_labels = train_test_split(tr_inputs, tr_labels, stratify=tr_labels, random_state=2020,\n","#                                                                 test_size = 0.15)\n","\n","# tr_masks, test_masks_cam, tr_masks_labels, _ =   train_test_split(attention_masks_cam, labels, stratify=labels, random_state=2020,\n","#                                                                  test_size=0.2)\n","\n","# tr_masks_cam, val_masks_cam, u,v =   train_test_split(tr_masks, tr_masks_labels, stratify=tr_masks_labels, random_state=2020,\n","#                                                                 test_size=0.15 )\n","\n","\n","\n","\n","# tr_inputs, test_inputs_flau, tr_labels, test_labels_flau = train_test_split(input_ids_flau, labels_flau, stratify=labels_cam, random_state=2020,\n","#                                                                 test_size = 0.2)\n","\n","# tr_inputs_flau, val_inputs_flau, train_labels, val_labels = train_test_split(tr_inputs, tr_labels, stratify=tr_labels, random_state=2020,\n","#                                                                 test_size = 0.15)\n","\n","# tr_masks, test_masks_flau, tr_masks_labels, _ =   train_test_split(attention_masks_flau, labels, stratify=labels, random_state=2020,\n","#                                                                  test_size=0.2)\n","\n","# tr_masks_flau, val_masks_flau, u,v =   train_test_split(tr_masks, tr_masks_labels, stratify=tr_masks_labels, random_state=2020,\n","#                                                                 test_size=0.15 )"],"execution_count":45,"outputs":[]},{"cell_type":"code","metadata":{"id":"nn6cpPDt2_fW","executionInfo":{"status":"ok","timestamp":1619174080684,"user_tz":-120,"elapsed":303300,"user":{"displayName":"Sara Fattouh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjGrdvZo2x_8K8abskRubIgMK3SdtS4cG-aQE01nA=s64","userId":"09360594903764573322"}}},"source":["tr_inputs, val_inputs_cam, tr_labels, val_labels_cam = train_test_split(input_ids_cam, labels_cam, stratify=labels_cam, random_state=2020,\n","                                                                test_size = 0.2)\n","\n","tr_inputs_cam, test_inputs_cam, train_labels, test_labels = train_test_split(tr_inputs, tr_labels, stratify=tr_labels, random_state=2020,\n","                                                                test_size = 0.15)\n","\n","tr_masks, val_masks_cam, tr_masks_labels, _ =   train_test_split(attention_masks_cam, labels, stratify=labels, random_state=2020,\n","                                                                 test_size=0.2)\n","\n","tr_masks_cam, test_masks_cam, u,v =   train_test_split(tr_masks, tr_masks_labels, stratify=tr_masks_labels, random_state=2020,\n","                                                                test_size=0.15 )\n","\n","\n","\n","\n","tr_inputs, val_inputs_flau, tr_labels, val_labels_flau = train_test_split(input_ids_flau, labels_flau, stratify=labels_cam, random_state=2020,\n","                                                                test_size = 0.2)\n","\n","tr_inputs_flau, test_inputs_flau, train_labels, test_labels = train_test_split(tr_inputs, tr_labels, stratify=tr_labels, random_state=2020,\n","                                                                test_size = 0.15)\n","\n","tr_masks, val_masks_flau, tr_masks_labels, _ =   train_test_split(attention_masks_flau, labels, stratify=labels, random_state=2020,\n","                                                                 test_size=0.2)\n","\n","tr_masks_flau, test_masks_flau, u,v =   train_test_split(tr_masks, tr_masks_labels, stratify=tr_masks_labels, random_state=2020,\n","                                                                test_size=0.15 )"],"execution_count":46,"outputs":[]},{"cell_type":"code","metadata":{"id":"SfiE2yDIzuRO","executionInfo":{"status":"ok","timestamp":1619174082150,"user_tz":-120,"elapsed":304761,"user":{"displayName":"Sara Fattouh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjGrdvZo2x_8K8abskRubIgMK3SdtS4cG-aQE01nA=s64","userId":"09360594903764573322"}}},"source":["torch.save(tr_inputs_cam, \"tr_inputs_cam.pt\")\n","torch.save(val_inputs_cam, \"val_inputs_cam.pt\")\n","torch.save(tr_masks_cam, \"tr_masks_cam.pt\")\n","torch.save(val_masks_cam, \"val_masks_cam.pt\")\n","torch.save(test_inputs_cam, \"test_inputs_cam.pt\")\n","torch.save(test_masks_cam, \"test_masks_cam.pt\")\n","\n","\n","torch.save(tr_inputs_flau, \"tr_inputs_flau.pt\")\n","torch.save(val_inputs_flau, \"val_inputs_flau.pt\")\n","torch.save(tr_masks_flau, \"tr_masks_flau.pt\")\n","torch.save(val_masks_flau, \"val_masks_flau.pt\")\n","torch.save(test_inputs_flau, \"test_inputs_flau.pt\")\n","torch.save(test_masks_flau, \"test_masks_flau.pt\")\n","\n"],"execution_count":47,"outputs":[]},{"cell_type":"code","metadata":{"id":"mdkITHXh0Ahs","executionInfo":{"status":"ok","timestamp":1619174082151,"user_tz":-120,"elapsed":304754,"user":{"displayName":"Sara Fattouh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjGrdvZo2x_8K8abskRubIgMK3SdtS4cG-aQE01nA=s64","userId":"09360594903764573322"}}},"source":["text_input='./'\n","\n","tr_inputs_cam = torch.load(text_input + \"tr_inputs_cam.pt\")\n","val_inputs_cam = torch.load(text_input +\"val_inputs_cam.pt\")\n","tr_masks_cam = torch.load(text_input + \"tr_masks_cam.pt\")\n","val_masks_cam = torch.load(text_input + \"val_masks_cam.pt\")\n","input_ids_test_cam = torch.load(text_input + \"test_inputs_cam.pt\") \n","attention_masks_test_cam = torch.load(text_input + \"test_masks_cam.pt\") \n","\n","tr_inputs_flau = torch.load(text_input + \"tr_inputs_flau.pt\")\n","val_inputs_flau = torch.load(text_input + \"val_inputs_flau.pt\")\n","tr_masks_flau = torch.load(text_input + \"tr_masks_flau.pt\")\n","val_masks_flau = torch.load(text_input + \"val_masks_flau.pt\")\n","input_ids_test_flau = torch.load(text_input + \"test_inputs_flau.pt\")\n","attention_masks_test_flau = torch.load(text_input + \"test_masks_flau.pt\")"],"execution_count":48,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"i1O4EyDzCpsS"},"source":["# 4. Defining Models to be Fused\n","\n","Now, as our  data has been preprocessed, cleaned and text data was tokenzied , it is ready to be fed to the models. \n","- As a first step,  first we need to define and configure the models. \n"]},{"cell_type":"markdown","metadata":{"id":"rlj0Dz1FW7cM"},"source":["# 4.1. RESNet Model for Image Processing. \n","\n","In PyTorch, you always need to define a forward method for your neural network model. But you never have to call it explicitly.\n","Here we are defining our image processing class is subclass of nn.Module and is inheriting all methods. In the super class, nn.Module, there is a __call__ method which obtains the forward function from the subclass and calls it."]},{"cell_type":"markdown","metadata":{"id":"rnnJlj2Y3iii"},"source":["# 4.1.1.  Image Processing Model - RESNet50\n","\n","\n"]},{"cell_type":"code","metadata":{"id":"QWvMo_nV1XCb","executionInfo":{"status":"ok","timestamp":1619174084899,"user_tz":-120,"elapsed":307497,"user":{"displayName":"Sara Fattouh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjGrdvZo2x_8K8abskRubIgMK3SdtS4cG-aQE01nA=s64","userId":"09360594903764573322"}}},"source":["from torch.nn import functional as F\n","import torch.nn as nn\n","import pretrainedmodels\n","class SEResnext50_32x4d(nn.Module):\n","    def __init__(self, pretrained='imagenet'):\n","        super(SEResnext50_32x4d, self).__init__()\n","        \n","        self.base_model = pretrainedmodels.__dict__[\"se_resnext50_32x4d\"](pretrained=None)\n","        if pretrained is not None:\n","            self.base_model.load_state_dict(\n","                self.base_model.load_state_dict(torch.load(resnet_model_path))\n","                )\n","            \n","        self.l0 = nn.Linear(2048, 27)  # Applies a linear transformation to the incoming data\n","        # batch_size = 2048\n","    \n","    def forward(self, image):\n","        batch_size, _, _, _ = image.shape\n","\n","        # During the training you will get batches of images, \n","        # so your shape in the forward method will get an additional batch dimension at dim0: \n","        # [batch_size, channels, height, width].\n","        \n","        x = self.base_model.features(image) \n","\n","        #extracting feature vector from network after feature leaning \n","        #This is the flatten vector \n","\n","        x = F.adaptive_avg_pool2d(x, 1).reshape(batch_size, -1) \n","        #adaptive_avg_pool2d : Kernel size = (input_size+target_size-1) // target_size rounded up\n","        #Then the positions of where to apply the kernel are computed as rounded equidistant points between 0 and input_size - kernel_size\n","        \n","        out = self.l0(x)\n","\n","        return out"],"execution_count":49,"outputs":[]},{"cell_type":"code","metadata":{"id":"NaAYA3QY1cbO","executionInfo":{"status":"ok","timestamp":1619174084900,"user_tz":-120,"elapsed":307493,"user":{"displayName":"Sara Fattouh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjGrdvZo2x_8K8abskRubIgMK3SdtS4cG-aQE01nA=s64","userId":"09360594903764573322"}}},"source":["class Identity(nn.Module):\n","  \n","    def __init__(self):\n","        super(Identity, self).__init__()\n","        \n","    def forward(self, x):\n","        return x"],"execution_count":50,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"vr8ciZGg1Ee4"},"source":["# 4.1.2. Instaniating the Image Processing Network \n"," Now we create an instance from the SEResnext50_32x4d class that we defined and load the weights from a pretrained model, since the training is done previously. "]},{"cell_type":"code","metadata":{"id":"ZGGor-mS1zJe","executionInfo":{"status":"ok","timestamp":1619174084901,"user_tz":-120,"elapsed":307488,"user":{"displayName":"Sara Fattouh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjGrdvZo2x_8K8abskRubIgMK3SdtS4cG-aQE01nA=s64","userId":"09360594903764573322"}}},"source":["#data_path = '/content/drive/My Drive/Rakuten/'\n","\n","img_model = SEResnext50_32x4d(pretrained=None)\n","# img_model.load_state_dict(torch.load(os.path.join(data_path, 'models/RESNET_best_model.pt')))\n","\n","# img_model.cuda()"],"execution_count":51,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"P9gfXO9k21-o"},"source":["# 4.1.3. Prinitng Model's Params"]},{"cell_type":"code","metadata":{"id":"WOiwcopd8m8B","executionInfo":{"status":"ok","timestamp":1619174084901,"user_tz":-120,"elapsed":307483,"user":{"displayName":"Sara Fattouh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjGrdvZo2x_8K8abskRubIgMK3SdtS4cG-aQE01nA=s64","userId":"09360594903764573322"}}},"source":["img_model.l0 = Identity()"],"execution_count":52,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Jb7I6qBq2c0E","executionInfo":{"status":"ok","timestamp":1619174084902,"user_tz":-120,"elapsed":307475,"user":{"displayName":"Sara Fattouh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjGrdvZo2x_8K8abskRubIgMK3SdtS4cG-aQE01nA=s64","userId":"09360594903764573322"}},"outputId":"080aeccd-da10-43ae-c569-79bc35a0966a"},"source":["for param in img_model.parameters():\n","     print(type(param), param.size())"],"execution_count":53,"outputs":[{"output_type":"stream","text":["<class 'torch.nn.parameter.Parameter'> torch.Size([64, 3, 7, 7])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([64])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([64])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([128, 64, 1, 1])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([128])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([128])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([128, 4, 3, 3])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([128])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([128])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([256, 128, 1, 1])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([256])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([256])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([16, 256, 1, 1])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([16])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([256, 16, 1, 1])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([256])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([256, 64, 1, 1])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([256])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([256])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([128, 256, 1, 1])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([128])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([128])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([128, 4, 3, 3])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([128])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([128])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([256, 128, 1, 1])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([256])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([256])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([16, 256, 1, 1])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([16])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([256, 16, 1, 1])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([256])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([128, 256, 1, 1])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([128])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([128])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([128, 4, 3, 3])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([128])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([128])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([256, 128, 1, 1])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([256])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([256])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([16, 256, 1, 1])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([16])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([256, 16, 1, 1])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([256])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([256, 256, 1, 1])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([256])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([256])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([256, 8, 3, 3])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([256])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([256])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([512, 256, 1, 1])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([512])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([512])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([32, 512, 1, 1])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([32])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([512, 32, 1, 1])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([512])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([512, 256, 1, 1])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([512])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([512])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([256, 512, 1, 1])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([256])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([256])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([256, 8, 3, 3])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([256])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([256])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([512, 256, 1, 1])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([512])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([512])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([32, 512, 1, 1])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([32])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([512, 32, 1, 1])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([512])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([256, 512, 1, 1])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([256])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([256])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([256, 8, 3, 3])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([256])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([256])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([512, 256, 1, 1])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([512])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([512])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([32, 512, 1, 1])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([32])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([512, 32, 1, 1])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([512])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([256, 512, 1, 1])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([256])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([256])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([256, 8, 3, 3])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([256])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([256])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([512, 256, 1, 1])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([512])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([512])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([32, 512, 1, 1])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([32])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([512, 32, 1, 1])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([512])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([512, 512, 1, 1])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([512])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([512])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([512, 16, 3, 3])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([512])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([512])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([1024, 512, 1, 1])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([1024])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([1024])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([64, 1024, 1, 1])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([64])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([1024, 64, 1, 1])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([1024])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([1024, 512, 1, 1])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([1024])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([1024])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([512, 1024, 1, 1])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([512])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([512])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([512, 16, 3, 3])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([512])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([512])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([1024, 512, 1, 1])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([1024])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([1024])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([64, 1024, 1, 1])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([64])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([1024, 64, 1, 1])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([1024])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([512, 1024, 1, 1])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([512])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([512])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([512, 16, 3, 3])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([512])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([512])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([1024, 512, 1, 1])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([1024])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([1024])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([64, 1024, 1, 1])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([64])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([1024, 64, 1, 1])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([1024])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([512, 1024, 1, 1])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([512])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([512])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([512, 16, 3, 3])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([512])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([512])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([1024, 512, 1, 1])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([1024])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([1024])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([64, 1024, 1, 1])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([64])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([1024, 64, 1, 1])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([1024])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([512, 1024, 1, 1])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([512])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([512])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([512, 16, 3, 3])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([512])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([512])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([1024, 512, 1, 1])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([1024])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([1024])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([64, 1024, 1, 1])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([64])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([1024, 64, 1, 1])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([1024])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([512, 1024, 1, 1])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([512])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([512])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([512, 16, 3, 3])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([512])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([512])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([1024, 512, 1, 1])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([1024])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([1024])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([64, 1024, 1, 1])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([64])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([1024, 64, 1, 1])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([1024])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([1024, 1024, 1, 1])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([1024])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([1024])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([1024, 32, 3, 3])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([1024])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([1024])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([2048, 1024, 1, 1])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([2048])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([2048])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([128, 2048, 1, 1])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([128])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([2048, 128, 1, 1])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([2048])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([2048, 1024, 1, 1])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([2048])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([2048])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([1024, 2048, 1, 1])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([1024])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([1024])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([1024, 32, 3, 3])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([1024])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([1024])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([2048, 1024, 1, 1])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([2048])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([2048])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([128, 2048, 1, 1])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([128])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([2048, 128, 1, 1])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([2048])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([1024, 2048, 1, 1])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([1024])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([1024])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([1024, 32, 3, 3])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([1024])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([1024])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([2048, 1024, 1, 1])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([2048])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([2048])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([128, 2048, 1, 1])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([128])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([2048, 128, 1, 1])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([2048])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([1000, 2048])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([1000])\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"9iqaXH5A3Esl"},"source":["# 4.1.4. Model's Params Require No Grads\n","\n","These are just regular tensors, with one very special addition: we tell PyTorch that they require a gradient. This causes PyTorch to record all of the operations done on the tensor, so that it can calculate the gradient during back-propagation automatically!\n","As our model is already trained and weights are assigned, then there is no need to calculate the gradients so no need to send them to the optimizer."]},{"cell_type":"code","metadata":{"id":"c0XJcLgW8u09","executionInfo":{"status":"ok","timestamp":1619174084902,"user_tz":-120,"elapsed":307465,"user":{"displayName":"Sara Fattouh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjGrdvZo2x_8K8abskRubIgMK3SdtS4cG-aQE01nA=s64","userId":"09360594903764573322"}}},"source":["# params are iterators which contain model's parameters. Usually passed to the optimizer\n","for params in img_model.parameters():\n","      params.requires_grad = False"],"execution_count":54,"outputs":[]},{"cell_type":"code","metadata":{"id":"vaXULFfzkdB8","executionInfo":{"status":"ok","timestamp":1619174084903,"user_tz":-120,"elapsed":307461,"user":{"displayName":"Sara Fattouh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjGrdvZo2x_8K8abskRubIgMK3SdtS4cG-aQE01nA=s64","userId":"09360594903764573322"}}},"source":["img_model.out_proj = Identity()"],"execution_count":55,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"JWgGQqbU4nNR"},"source":["# Image Data Preparation"]},{"cell_type":"code","metadata":{"id":"4yNKxOJ2-QrA","executionInfo":{"status":"ok","timestamp":1619174084903,"user_tz":-120,"elapsed":307456,"user":{"displayName":"Sara Fattouh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjGrdvZo2x_8K8abskRubIgMK3SdtS4cG-aQE01nA=s64","userId":"09360594903764573322"}}},"source":["# # Data path\n","# text_data_path = os.path.join('/content/drive/My Drive/Rakuten')\n","# image_data_path = os.path.join('')\n"],"execution_count":56,"outputs":[]},{"cell_type":"code","metadata":{"id":"SM6bWAoKTsx7","executionInfo":{"status":"ok","timestamp":1619174084903,"user_tz":-120,"elapsed":307451,"user":{"displayName":"Sara Fattouh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjGrdvZo2x_8K8abskRubIgMK3SdtS4cG-aQE01nA=s64","userId":"09360594903764573322"}}},"source":["def get_img_path(img_id, prd_id, path):\n","    \n","    pattern = 'image'+'_'+str(img_id)+'_'+'product'+'_'+str(prd_id)+'.jpg'\n","    return path + pattern"],"execution_count":57,"outputs":[]},{"cell_type":"code","metadata":{"id":"oYomnrt7wKWw","executionInfo":{"status":"ok","timestamp":1619174084904,"user_tz":-120,"elapsed":307446,"user":{"displayName":"Sara Fattouh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjGrdvZo2x_8K8abskRubIgMK3SdtS4cG-aQE01nA=s64","userId":"09360594903764573322"}}},"source":["# print(len(Preprocess.train), len(train))"],"execution_count":58,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"QzgyQu0aY17J"},"source":["# Obtaining & Splitting Images "]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":185,"referenced_widgets":["84b2f906c4cc440b97611e847d7ace45","942936be5c344665a886b27dcfce8e72","af4bc2fceceb43548fcbb8d593ddb531","43eeccaeaf894bf3a144f35768333a3c","8fd78d7c990b4d74a1d62099f939e4f1","ba0bad8f31434350b339f73a44edd2cf","6b36d7322203489f8ffd52b3b0f805e9","e727404e940b4f6fbf93b5d6d0ae9951"]},"id":"emmAR45klxoi","executionInfo":{"status":"ok","timestamp":1619174087039,"user_tz":-120,"elapsed":309571,"user":{"displayName":"Sara Fattouh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjGrdvZo2x_8K8abskRubIgMK3SdtS4cG-aQE01nA=s64","userId":"09360594903764573322"}},"outputId":"08326823-0d6e-4c35-d7fa-871f9de6d6d1"},"source":["from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n","\n","#  train 65%\n","#  validation 15%\n","#  test 20%\n","# (original training) =>  train, test => 80, 20  val_size = 0.2\n","# (train) => train, val =>  85, 15   val_size = 0.15 \n","\n","\n","train_img = train[['Image_id','Product_id','labels','product']]\n","\n","train_img['image_path'] = Preprocess.train.progress_apply(lambda x: get_img_path(x['Image_id'], x['Product_id'],\n","                                                      path = os.path.join(image_data_path, 'image_training/')),axis=1)\n","\n","\n","\n","\n","tr_df, val_df, tr_labels, val_labels = train_test_split(train_img, train_img['labels'], \n","                                           random_state=2020,\n","                                           test_size = 0.2,\n","                                           stratify=train_img['labels'])\n","\n","\n","train_df, test_df, train_labels, test_labels = train_test_split(tr_df, tr_labels, \n","                                           random_state=2020,\n","                                           test_size = 0.15,\n","                                           stratify=tr_labels)\n","\n","\n","\n","# print(\"Train: \", len(train_df))\n","# print(\"Val:   \", len(val_df))\n","# print(\"Test:  \", len(test_df))\n","print (\"\")"],"execution_count":59,"outputs":[{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"84b2f906c4cc440b97611e847d7ace45","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=0.0, max=84916.0), HTML(value='')))"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["\n","\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:13: SettingWithCopyWarning: \n","A value is trying to be set on a copy of a slice from a DataFrame.\n","Try using .loc[row_indexer,col_indexer] = value instead\n","\n","See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n","  del sys.path[0]\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"B_h5_e_Gw-M9","executionInfo":{"status":"ok","timestamp":1619174087039,"user_tz":-120,"elapsed":309560,"user":{"displayName":"Sara Fattouh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjGrdvZo2x_8K8abskRubIgMK3SdtS4cG-aQE01nA=s64","userId":"09360594903764573322"}}},"source":["# print(\"Original Images Df:   \",  len(train_img))\n","# print(\"Train Images DF:      \" , len(train_df))\n","# print(\"Validation Images DF: \",  len(val_df))"],"execution_count":60,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"aIAzQAsCTzBs","executionInfo":{"status":"ok","timestamp":1619174087040,"user_tz":-120,"elapsed":309552,"user":{"displayName":"Sara Fattouh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjGrdvZo2x_8K8abskRubIgMK3SdtS4cG-aQE01nA=s64","userId":"09360594903764573322"}},"outputId":"3bebbb1f-d939-4aa9-ba6e-ee9c00f2d328"},"source":["print (train_img['image_path'][0])\n"],"execution_count":61,"outputs":[{"output_type":"stream","text":["/content/Rakuten/image/image_training/image_1263597046_product_3804725264.jpg\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"lRigVJwVBOcz"},"source":["# Image Data Augmentation\n","\n","We're going to be making use of Pytorch's transforms for preparing the input images to be used by our model. \n","\n","\n","\n","\n","\n","*   We'll need to make sure the images in the training set and validation set are the same size, so we'll be using transforms.Resize\n","*   We'll also be doing a little data augmentation, trying to improve the performance of our model by forcing it to learn about images at different angles and crops, so we'll randomly crop and rotate the images.\n","\n","*    we'll make tensors out of the images, as PyTorch works with tensors. \n","*   Finally, we'll normalize the images, which helps the network work with values that may be have a wide range of different values.\n","\n","\n","*   We then compose all our chosen transforms.\n","\n","It worth mentioning that validation transforms don't have any of the flipping or rotating, as they aren't part of our training set, so the network isn't learning about them\n","\n","\n","\n","\n","\n"]},{"cell_type":"code","metadata":{"id":"-Togfn7XmpW5","executionInfo":{"status":"ok","timestamp":1619174087041,"user_tz":-120,"elapsed":309542,"user":{"displayName":"Sara Fattouh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjGrdvZo2x_8K8abskRubIgMK3SdtS4cG-aQE01nA=s64","userId":"09360594903764573322"}}},"source":["input_size = 224 # for Resnt\n","\n","# Applying Transforms to the Data\n","\n","from torchvision import datasets, models, transforms\n","\n","image_transforms = { \n","    'train': transforms.Compose([\n","        transforms.RandomResizedCrop(size=256, scale=(0.8, 1.0)),\n","        transforms.RandomRotation(degrees=15),\n","        transforms.RandomHorizontalFlip(),\n","        transforms.Resize(size=256),\n","        transforms.CenterCrop(size=input_size),\n","        transforms.ToTensor(),\n","        transforms.Normalize([0.485, 0.456, 0.406],\n","                             [0.229, 0.224, 0.225])\n","    ]),\n","    'valid': transforms.Compose([\n","        transforms.Resize(size=256),\n","        transforms.CenterCrop(size=input_size),\n","        transforms.ToTensor(),\n","        transforms.Normalize([0.485, 0.456, 0.406],\n","                             [0.229, 0.224, 0.225])\n","    ]),\n","    'test': transforms.Compose([\n","        transforms.Resize(size=256),\n","        transforms.CenterCrop(size=input_size),\n","        transforms.ToTensor(),\n","        transforms.Normalize([0.485, 0.456, 0.406],\n","                             [0.229, 0.224, 0.225])\n","    ])\n","}"],"execution_count":62,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"nB9-K4lUNoqT"},"source":["# Text Processing Models - BertForSequenceClassification\n","\n","Thankfully, the huggingface pytorch implementation includes a set of interfaces designed for a variety of NLP tasks. Though these interfaces are all built on top of a trained BERT model, each has different top layers and output types designed to accomodate their specific NLP task.\n","\n","We first want to modify the pre-trained BERT model to give outputs for classification, and then we want to continue training the model on our dataset until that the entire model, end-to-end, is well-suited for our task.\n","\n","**BertForSequenceClassification** is one of the current of classes provided for fine-tuning.\n","\n","This is the normal BERT model with an added single linear layer on top for classification that we will use as a sentence classifier. As we feed input data, the entire pre-trained BERT model and the additional untrained classification layer is trained on our specific task.\n","\n","- Not to forget that Camembet model inherits RobertaModel"]},{"cell_type":"markdown","metadata":{"id":"I_vkDiuRC70z"},"source":["# 4.2 CamemBERT Model"]},{"cell_type":"code","metadata":{"id":"imOtgakyCtFe","executionInfo":{"status":"ok","timestamp":1619174087041,"user_tz":-120,"elapsed":309536,"user":{"displayName":"Sara Fattouh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjGrdvZo2x_8K8abskRubIgMK3SdtS4cG-aQE01nA=s64","userId":"09360594903764573322"}}},"source":["class vec_output_CamembertForSequenceClassification(CamembertModel):\n","  \n","    config_class = CamembertConfig\n","\n","    def __init__(self, config):\n","        super().__init__(config)\n","        self.num_labels = config.num_labels\n","\n","        self.roberta = CamembertModel(config)\n","        self.dense = nn.Linear(256*config.hidden_size, config.hidden_size)\n","        self.dropout = nn.Dropout(0.1)\n","        self.out_proj = nn.Linear(config.hidden_size, config.num_labels)\n","        self.init_weights()\n","\n","\n","    def forward(\n","        self,\n","        input_ids=None,\n","        attention_mask=None,\n","        token_type_ids=None,\n","        position_ids=None,\n","        head_mask=None,\n","        inputs_embeds=None,\n","        labels=None,\n","        output_attentions=None,\n","        output_hidden_states=None,\n","    ):\n","        outputs = self.roberta(\n","            input_ids,\n","            attention_mask = attention_mask,\n","            token_type_ids = token_type_ids,\n","            position_ids = position_ids,\n","            head_mask = head_mask,\n","            inputs_embeds=inputs_embeds,\n","#           output_attentions=output_attentions,\n","#           output_hidden_states=output_hidden_states,\n","        )\n","\n","        sequence_output = outputs[0] #(B,256,768)\n","\n","        x = sequence_output.view(sequence_output.shape[0], 256*768)\n","\n","#       x = sequence_output[:, 0, :]  # take <s> token (equiv. to [CLS])-> #(B,768) Image -> (B,2048)\n","\n","        x = self.dense(x)  # 768 -> 768\n","\n","        feat= torch.tanh(x) \n","\n","        logits = self.out_proj(feat) # 768 -> 27\n","\n","        outputs = (logits,) + outputs[2:] #3rd element onwards\n","\n","        return outputs,feat  # (loss), logits, (hidden_states), (attentions)"],"execution_count":63,"outputs":[]},{"cell_type":"code","metadata":{"id":"6KstmQqyd04x","executionInfo":{"status":"ok","timestamp":1619174087041,"user_tz":-120,"elapsed":309531,"user":{"displayName":"Sara Fattouh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjGrdvZo2x_8K8abskRubIgMK3SdtS4cG-aQE01nA=s64","userId":"09360594903764573322"}}},"source":["camembert_model_path = '/content/Rakuten/models/CamemBERT_best_model_description.pt'"],"execution_count":64,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"OtPoFtaqDAj6"},"source":["# FlauBERT Model"]},{"cell_type":"code","metadata":{"id":"a16smoYhDCmn","executionInfo":{"status":"ok","timestamp":1619174087042,"user_tz":-120,"elapsed":309527,"user":{"displayName":"Sara Fattouh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjGrdvZo2x_8K8abskRubIgMK3SdtS4cG-aQE01nA=s64","userId":"09360594903764573322"}}},"source":["num_classes = 27\n","\n","class vec_output_FlaubertForSequenceClassification(FlaubertModel):\n","    \n","    config_class = FlaubertConfig\n","    \n","\n","    def __init__(self, config):\n","        super().__init__(config)\n","        self.transformer = FlaubertModel(config)\n","        self.sequence_summary = SequenceSummary(config)\n","        self.init_weights()\n","        self.dropout =  torch.nn.Dropout(0.1)\n","        self.classifier = torch.nn.Linear(config.hidden_size, num_classes)\n","\n","\n","    def forward(\n","        self,\n","        input_ids=None,\n","        attention_mask=None,\n","        langs=None,\n","        token_type_ids=None,\n","        position_ids=None,\n","        lengths=None,\n","        cache=None,\n","        head_mask=None,\n","        inputs_embeds=None,\n","        labels=None,\n","    ):\n","        \n","        \n","        transformer_outputs = self.transformer(\n","            input_ids,\n","            attention_mask = attention_mask,\n","            langs = langs,\n","            token_type_ids = token_type_ids,\n","            position_ids = position_ids,\n","            lengths = lengths,\n","            cache = cache,\n","            head_mask = head_mask,\n","            inputs_embeds = inputs_embeds,\n","        )\n","\n","        #output = self.dropout(output)\n","        output = transformer_outputs[0] \n","        vec = output[:,0]\n","        \n","        \n","        #logits\n","        dense = self.dropout(vec)\n","        \n","        #classifier\n","        logits = self.classifier(dense)\n","        \n","        outputs = (logits,) + transformer_outputs[1:]  # Keep new_mems and attention/hidden states if they are here\n","       \n","        \n","        return outputs,dense"],"execution_count":65,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"b2krGO8BnVfd"},"source":["# Dataset Fusion"]},{"cell_type":"code","metadata":{"id":"qIIQ5-g3gU85","executionInfo":{"status":"ok","timestamp":1619174087042,"user_tz":-120,"elapsed":309522,"user":{"displayName":"Sara Fattouh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjGrdvZo2x_8K8abskRubIgMK3SdtS4cG-aQE01nA=s64","userId":"09360594903764573322"}}},"source":["# TODO DELELTE IMAGES WITH NO DESCRIPTION\n","# From the preprocesssed file"],"execution_count":66,"outputs":[]},{"cell_type":"code","metadata":{"id":"RRq24YrsnU9X","executionInfo":{"status":"ok","timestamp":1619174087044,"user_tz":-120,"elapsed":309519,"user":{"displayName":"Sara Fattouh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjGrdvZo2x_8K8abskRubIgMK3SdtS4cG-aQE01nA=s64","userId":"09360594903764573322"}}},"source":["from torch.utils.data import Dataset, DataLoader, Subset\n","from PIL import Image\n","import matplotlib.pyplot as plt\n","import cv2\n","\n","class FusionDataset(Dataset):\n","    \n","    def __init__(self, df, inputs_cam, masks_cam, inputs_flau, masks_flau, transform=None, mode='train'):\n","        self.df = df\n","        self.transform   = transform\n","        self.mode = mode\n","\n","        self.inputs_cam  = inputs_cam\n","        self.masks_cam   = masks_cam\n","\n","        self.inputs_flau  = inputs_flau\n","        self.masks_flau   = masks_flau\n","         \n","    def __len__(self):\n","        return len(self.df)\n","    \n","    def __getitem__(self,idx):\n","        \n","        im_path = self.df.iloc[idx]['image_path']\n","        img= plt.imread(im_path)\n","        #img = cv2.imread(im_path)\n","        #img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n","        img = Image.fromarray(img)\n","\n","        if self.transform is not None:\n","            img = self.transform(img)\n","\n","        img              = img.cuda()\n","        input_id_cam     = self.inputs_cam[idx].cuda()\n","        input_mask_cam   = self.masks_cam[idx].cuda()\n","        input_id_flau    = self.inputs_flau[idx].cuda()\n","        input_mask_flau  = self.masks_flau[idx].cuda()\n","        \n","        if self.mode =='test':\n","            return img,input_id_cam,input_mask_cam,input_id_flau,input_mask_flau\n","            \n","        else:\n","            labels = torch.tensor(self.df.iloc[idx]['labels']).cuda()             \n","            return img,input_id_cam,input_mask_cam,input_id_flau,input_mask_flau,labels"],"execution_count":67,"outputs":[]},{"cell_type":"code","metadata":{"id":"O5RRyCzd4XSd","executionInfo":{"status":"ok","timestamp":1619174087045,"user_tz":-120,"elapsed":309514,"user":{"displayName":"Sara Fattouh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjGrdvZo2x_8K8abskRubIgMK3SdtS4cG-aQE01nA=s64","userId":"09360594903764573322"}}},"source":["a1 = torch.randn(3,10,10)"],"execution_count":68,"outputs":[]},{"cell_type":"code","metadata":{"id":"o_51PvBY4Zpr","executionInfo":{"status":"ok","timestamp":1619174087045,"user_tz":-120,"elapsed":309507,"user":{"displayName":"Sara Fattouh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjGrdvZo2x_8K8abskRubIgMK3SdtS4cG-aQE01nA=s64","userId":"09360594903764573322"}}},"source":["reduce_dim = nn.Conv1d(in_channels = 10 , out_channels = 1 , kernel_size= 1)"],"execution_count":69,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"wryxtip-4b0m","executionInfo":{"status":"ok","timestamp":1619174087045,"user_tz":-120,"elapsed":309498,"user":{"displayName":"Sara Fattouh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjGrdvZo2x_8K8abskRubIgMK3SdtS4cG-aQE01nA=s64","userId":"09360594903764573322"}},"outputId":"3e00f3ef-c3d1-42df-db50-bacc6873753c"},"source":["reduce_dim(a1).view(3,10).shape"],"execution_count":70,"outputs":[{"output_type":"execute_result","data":{"text/plain":["torch.Size([3, 10])"]},"metadata":{"tags":[]},"execution_count":70}]},{"cell_type":"markdown","metadata":{"id":"Uvw1I4sNSJ9_"},"source":["# Test Sentences Tokenization"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"hTUa-CbEnluo","executionInfo":{"status":"ok","timestamp":1619174090450,"user_tz":-120,"elapsed":312890,"user":{"displayName":"Sara Fattouh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjGrdvZo2x_8K8abskRubIgMK3SdtS4cG-aQE01nA=s64","userId":"09360594903764573322"}},"outputId":"7caa1f26-f97d-4fa7-c8d0-0f905e14aa7c"},"source":["print('Using Camembert')\n","tokenizer_cam = CamembertTokenizer.from_pretrained('camembert-base', do_lowercase=False)\n","\n","print('Using Flaubert')\n","tokenizer_flau = FlaubertTokenizer.from_pretrained('flaubert/flaubert_base_cased', do_lowercase=False)\n","\n","# input_ids_test_flau,attention_masks_test_flau = prep_input(test_sentences, labels=None, max_len=max_len,tokenizer = tokenizer_flau)\n","\n","# input_ids_test_cam,attention_masks_test_cam = prep_input(test_sentences , labels=None, max_len=max_len,tokenizer = tokenizer_cam)"],"execution_count":71,"outputs":[{"output_type":"stream","text":["Using Camembert\n","Using Flaubert\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"e8gijJIM5pQX","executionInfo":{"status":"ok","timestamp":1619174090451,"user_tz":-120,"elapsed":312881,"user":{"displayName":"Sara Fattouh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjGrdvZo2x_8K8abskRubIgMK3SdtS4cG-aQE01nA=s64","userId":"09360594903764573322"}}},"source":["# print(type(Preprocess.test_sentences))\n","# print(len(Preprocess.test_sentences))"],"execution_count":72,"outputs":[]},{"cell_type":"code","metadata":{"id":"HXbLhSkyCilV","executionInfo":{"status":"ok","timestamp":1619174090451,"user_tz":-120,"elapsed":312876,"user":{"displayName":"Sara Fattouh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjGrdvZo2x_8K8abskRubIgMK3SdtS4cG-aQE01nA=s64","userId":"09360594903764573322"}}},"source":["# # Moodels path \n","# resnet_model_path = '/content/Rakuten/models/RESNET_best_model.pt'\n","# camembert_model_path = '/content/Rakuten/models/CamemBERT_best_model_title_description.pt' ###### TODO Change with the updated model!!!\n","# flaubert_model_path = '/content/Rakuten/models/FlauBERT_best_model_title_description.pt'\n","\n","#my_flau_path  = '/content/Rakuten/models/FlauBERT_best_model_description.pt'\n"],"execution_count":73,"outputs":[]},{"cell_type":"code","metadata":{"id":"LzlptrSui4af","executionInfo":{"status":"ok","timestamp":1619174090452,"user_tz":-120,"elapsed":312871,"user":{"displayName":"Sara Fattouh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjGrdvZo2x_8K8abskRubIgMK3SdtS4cG-aQE01nA=s64","userId":"09360594903764573322"}}},"source":["# Moodels path \n","resnet_model_path = '/content/Rakuten/models/Final_RESNet_model.pt'\n","\n","camembert_model_path_title = '/content/Rakuten/models/CamemBERT_best_model_split_title.pt'\n","camembert_model_path_desc = '/content/Rakuten/models/CamemBERT_best_model_split_description.pt'\n","\n","flaubert_model_path_title = '/content/Rakuten/models/FlauBERT_best_model_split_title.pt'\n","flaubert_model_path_desc = '/content/Rakuten/models/FlauBERT_best_model_split_description.pt'\n"],"execution_count":74,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"09NiYnUxfi94"},"source":["# Fuse\n"," When using pretrained models, PyTorch sets the model to be unfrozen (will have its weights adjusted) by default"]},{"cell_type":"code","metadata":{"id":"TxW8Ups_nr8O","executionInfo":{"status":"ok","timestamp":1619174090452,"user_tz":-120,"elapsed":312864,"user":{"displayName":"Sara Fattouh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjGrdvZo2x_8K8abskRubIgMK3SdtS4cG-aQE01nA=s64","userId":"09360594903764573322"}}},"source":["class vector_fusion(nn.Module):    \n","    def __init__(self):\n","        super(vector_fusion, self).__init__()\n","\n","        self.img_model = SEResnext50_32x4d(pretrained=None)\n","        self.img_model.load_state_dict(torch.load(resnet_model_path))\n","        self.img_model.l0=Identity()\n","        for params in self.img_model.parameters():\n","            params.requires_grad=False\n","\n","# ------ CamamBERT ------\n","\n","        self.cam_model_title = vec_output_CamembertForSequenceClassification.from_pretrained(\n","         'camembert-base', # Use the 12-layer BERT model, with an uncased vocab.\n","          num_labels = 27, # The number of output labels--2 for binary classification.\n","                    # You can increase this for multi-class tasks.   \n","          output_attentions = False, # Whether the model returns attentions weights.\n","          output_hidden_states = False,) # Whether the model returns all hidden-states.\n","        \n","        \n","        checkpoint = torch.load(camembert_model_path_title)\n","        self.cam_model_title.load_state_dict(checkpoint)\n","\n","        for param in self.cam_model_title.parameters():\n","            param.requires_grad=False\n","\n","        self.cam_model_title.out_proj = Identity()\n","\n","\n","        self.cam_model_desc = vec_output_CamembertForSequenceClassification.from_pretrained(\n","         'camembert-base', # Use the 12-layer BERT model, with an uncased vocab.\n","          num_labels = 27, # The number of output labels--2 for binary classification.\n","                    # You can increase this for multi-class tasks.   \n","          output_attentions = False, # Whether the model returns attentions weights.\n","          output_hidden_states = False,) # Whether the model returns all hidden-states.\n","        \n","        \n","        checkpoint = torch.load(camembert_model_path_desc)\n","        self.cam_model_desc.load_state_dict(checkpoint)\n","\n","        for param in self.cam_model_desc.parameters():\n","            param.requires_grad=False\n","\n","        self.cam_model_desc.out_proj = Identity()\n","\n"," # ----  FlauBERT ----- \n","\n","        \n","        self.flau_model_title = vec_output_FlaubertForSequenceClassification.from_pretrained(\n","        'flaubert/flaubert_base_cased', \n","        num_labels = 27, \n","        output_attentions = False,\n","        output_hidden_states = False,)\n","        checkpoint = torch.load(flaubert_model_path_title)\n","\n","        self.flau_model_title.load_state_dict(checkpoint)\n","\n","        for param in self.flau_model_title.parameters():\n","            param.requires_grad=False\n","\n","        self.flau_model_title.classifier=Identity()\n","\n","\n","      \n","        self.flau_model_desc = vec_output_FlaubertForSequenceClassification.from_pretrained(\n","        'flaubert/flaubert_base_cased', \n","        num_labels = 27, \n","        output_attentions = False,\n","        output_hidden_states = False,)\n","        checkpoint = torch.load(flaubert_model_path_desc)\n","\n","        self.flau_model_desc.load_state_dict(checkpoint)\n","\n","        for param in self.flau_model_desc.parameters():\n","            param.requires_grad=False\n","\n","        self.flau_model_desc.classifier=Identity()\n","\n","\n","# ------------------------------------------------------------\n","\n","        self.reduce_dim = nn.Conv1d(in_channels = 2048 , out_channels = 768 , kernel_size= 1)\n","        self.reduce_dim2 = nn.Conv1d(in_channels = 768 , out_channels = 1 , kernel_size= 1)\n","        self.out = nn.Linear(768*3, 27)\n","        \n","        #gamma\n","#         self.w1 = nn.Parameter(torch.zeros(1))\n","#         self.w2 = nn.Parameter(torch.zeros(1))\n","#         self.w3 = nn.Parameter(torch.zeros(1))\n","        \n","    def forward(self,img,input_id_cam,input_mask_cam,input_id_flau,input_mask_flau):\n","        \n","        cam_emb_title,vec1_title = self.cam_model_title(input_id_cam, \n","                     token_type_ids=None, \n","                     attention_mask=input_mask_cam)\n","        \n","        cam_emb_desc,vec1_desc = self.cam_model_desc(input_id_cam, \n","                     token_type_ids=None, \n","                     attention_mask=input_mask_cam)\n","        \n","\n","#---------------------------------\n","        \n","        flau_emb_title,vec2 =self.flau_model_title(input_id_flau, \n","                     token_type_ids=None, \n","                     attention_mask=input_mask_flau)\n","        \n","        flau_emb_desc,vec2_desc =self.flau_model_desc(input_id_flau, \n","                     token_type_ids=None, \n","                     attention_mask=input_mask_flau)\n","        \n","# ---------------------------------\n","        \n","        #Projecting the image embedding to lower dimension\n","        img_emb = self.img_model(img)\n","        \n","        img_emb = img_emb.view(img_emb.shape[0],img_emb.shape[1],1)\n","        img_emb = self.reduce_dim(img_emb)\n","        img_emb = img_emb.view(img_emb.shape[0],img_emb.shape[1]) ###### bs * 768 \n","# --------------------------------\n","\n","        #summing up the vectors\n","        cam_emb  = cam_emb_title + cam_emb_desc\n","        flau_emb = flau_emb_title + flau_emb_desc\n","        \n","        #Bilinear\n","        #text_emb = text_emb.view(text_emb.shape[0],1,text_emb.shape[1])  ##### bs * 1 * 768\n","        \n","        #Bilinear Pooling\n","        #pool_emb = torch.bmm(img_emb,text_emb) ### bs * 768 * 768\n","        #pool_emb = self.reduce_dim2(pool_emb).view(text_emb.shape[0],768)  #### bs * 1 * 768\n","        fuse= torch.cat([img_emb,cam_emb[0],flau_emb[0]],axis=1)\n","        \n","        \n","        logits = self.out(fuse)\n","        return logits"],"execution_count":75,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"J1VU1vrSQFzD","executionInfo":{"status":"ok","timestamp":1619174094049,"user_tz":-120,"elapsed":316452,"user":{"displayName":"Sara Fattouh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjGrdvZo2x_8K8abskRubIgMK3SdtS4cG-aQE01nA=s64","userId":"09360594903764573322"}},"outputId":"b60dced9-4859-42e2-a1c3-1907232a2050"},"source":[" img_model = SEResnext50_32x4d(pretrained=None)\n"," img_model.load_state_dict(torch.load(resnet_model_path))"],"execution_count":76,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<All keys matched successfully>"]},"metadata":{"tags":[]},"execution_count":76}]},{"cell_type":"code","metadata":{"id":"n4LM7TANsMqJ","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1619174104446,"user_tz":-120,"elapsed":326835,"user":{"displayName":"Sara Fattouh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjGrdvZo2x_8K8abskRubIgMK3SdtS4cG-aQE01nA=s64","userId":"09360594903764573322"}},"outputId":"d49d383d-f87b-4fc9-9d07-0c2b310c41a9"},"source":["cam_title= vec_output_CamembertForSequenceClassification.from_pretrained(\n","         'camembert-base', # Use the 12-layer BERT model, with an uncased vocab.\n","          num_labels = 27, # The number of output labels--2 for binary classification.\n","                    # You can increase this for multi-class tasks.   \n","          output_attentions = False, # Whether the model returns attentions weights.\n","          output_hidden_states = False,) # Whether the model returns all hidden-states.\n","        \n","        \n","checkpoint = torch.load(camembert_model_path_title)\n","cam_title.load_state_dict(checkpoint)"],"execution_count":77,"outputs":[{"output_type":"stream","text":["Some weights of the model checkpoint at camembert-base were not used when initializing vec_output_CamembertForSequenceClassification: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight']\n","- This IS expected if you are initializing vec_output_CamembertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing vec_output_CamembertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of vec_output_CamembertForSequenceClassification were not initialized from the model checkpoint at camembert-base and are newly initialized: ['embeddings.word_embeddings.weight', 'embeddings.position_embeddings.weight', 'embeddings.token_type_embeddings.weight', 'embeddings.LayerNorm.weight', 'embeddings.LayerNorm.bias', 'encoder.layer.0.attention.self.query.weight', 'encoder.layer.0.attention.self.query.bias', 'encoder.layer.0.attention.self.key.weight', 'encoder.layer.0.attention.self.key.bias', 'encoder.layer.0.attention.self.value.weight', 'encoder.layer.0.attention.self.value.bias', 'encoder.layer.0.attention.output.dense.weight', 'encoder.layer.0.attention.output.dense.bias', 'encoder.layer.0.attention.output.LayerNorm.weight', 'encoder.layer.0.attention.output.LayerNorm.bias', 'encoder.layer.0.intermediate.dense.weight', 'encoder.layer.0.intermediate.dense.bias', 'encoder.layer.0.output.dense.weight', 'encoder.layer.0.output.dense.bias', 'encoder.layer.0.output.LayerNorm.weight', 'encoder.layer.0.output.LayerNorm.bias', 'encoder.layer.1.attention.self.query.weight', 'encoder.layer.1.attention.self.query.bias', 'encoder.layer.1.attention.self.key.weight', 'encoder.layer.1.attention.self.key.bias', 'encoder.layer.1.attention.self.value.weight', 'encoder.layer.1.attention.self.value.bias', 'encoder.layer.1.attention.output.dense.weight', 'encoder.layer.1.attention.output.dense.bias', 'encoder.layer.1.attention.output.LayerNorm.weight', 'encoder.layer.1.attention.output.LayerNorm.bias', 'encoder.layer.1.intermediate.dense.weight', 'encoder.layer.1.intermediate.dense.bias', 'encoder.layer.1.output.dense.weight', 'encoder.layer.1.output.dense.bias', 'encoder.layer.1.output.LayerNorm.weight', 'encoder.layer.1.output.LayerNorm.bias', 'encoder.layer.2.attention.self.query.weight', 'encoder.layer.2.attention.self.query.bias', 'encoder.layer.2.attention.self.key.weight', 'encoder.layer.2.attention.self.key.bias', 'encoder.layer.2.attention.self.value.weight', 'encoder.layer.2.attention.self.value.bias', 'encoder.layer.2.attention.output.dense.weight', 'encoder.layer.2.attention.output.dense.bias', 'encoder.layer.2.attention.output.LayerNorm.weight', 'encoder.layer.2.attention.output.LayerNorm.bias', 'encoder.layer.2.intermediate.dense.weight', 'encoder.layer.2.intermediate.dense.bias', 'encoder.layer.2.output.dense.weight', 'encoder.layer.2.output.dense.bias', 'encoder.layer.2.output.LayerNorm.weight', 'encoder.layer.2.output.LayerNorm.bias', 'encoder.layer.3.attention.self.query.weight', 'encoder.layer.3.attention.self.query.bias', 'encoder.layer.3.attention.self.key.weight', 'encoder.layer.3.attention.self.key.bias', 'encoder.layer.3.attention.self.value.weight', 'encoder.layer.3.attention.self.value.bias', 'encoder.layer.3.attention.output.dense.weight', 'encoder.layer.3.attention.output.dense.bias', 'encoder.layer.3.attention.output.LayerNorm.weight', 'encoder.layer.3.attention.output.LayerNorm.bias', 'encoder.layer.3.intermediate.dense.weight', 'encoder.layer.3.intermediate.dense.bias', 'encoder.layer.3.output.dense.weight', 'encoder.layer.3.output.dense.bias', 'encoder.layer.3.output.LayerNorm.weight', 'encoder.layer.3.output.LayerNorm.bias', 'encoder.layer.4.attention.self.query.weight', 'encoder.layer.4.attention.self.query.bias', 'encoder.layer.4.attention.self.key.weight', 'encoder.layer.4.attention.self.key.bias', 'encoder.layer.4.attention.self.value.weight', 'encoder.layer.4.attention.self.value.bias', 'encoder.layer.4.attention.output.dense.weight', 'encoder.layer.4.attention.output.dense.bias', 'encoder.layer.4.attention.output.LayerNorm.weight', 'encoder.layer.4.attention.output.LayerNorm.bias', 'encoder.layer.4.intermediate.dense.weight', 'encoder.layer.4.intermediate.dense.bias', 'encoder.layer.4.output.dense.weight', 'encoder.layer.4.output.dense.bias', 'encoder.layer.4.output.LayerNorm.weight', 'encoder.layer.4.output.LayerNorm.bias', 'encoder.layer.5.attention.self.query.weight', 'encoder.layer.5.attention.self.query.bias', 'encoder.layer.5.attention.self.key.weight', 'encoder.layer.5.attention.self.key.bias', 'encoder.layer.5.attention.self.value.weight', 'encoder.layer.5.attention.self.value.bias', 'encoder.layer.5.attention.output.dense.weight', 'encoder.layer.5.attention.output.dense.bias', 'encoder.layer.5.attention.output.LayerNorm.weight', 'encoder.layer.5.attention.output.LayerNorm.bias', 'encoder.layer.5.intermediate.dense.weight', 'encoder.layer.5.intermediate.dense.bias', 'encoder.layer.5.output.dense.weight', 'encoder.layer.5.output.dense.bias', 'encoder.layer.5.output.LayerNorm.weight', 'encoder.layer.5.output.LayerNorm.bias', 'encoder.layer.6.attention.self.query.weight', 'encoder.layer.6.attention.self.query.bias', 'encoder.layer.6.attention.self.key.weight', 'encoder.layer.6.attention.self.key.bias', 'encoder.layer.6.attention.self.value.weight', 'encoder.layer.6.attention.self.value.bias', 'encoder.layer.6.attention.output.dense.weight', 'encoder.layer.6.attention.output.dense.bias', 'encoder.layer.6.attention.output.LayerNorm.weight', 'encoder.layer.6.attention.output.LayerNorm.bias', 'encoder.layer.6.intermediate.dense.weight', 'encoder.layer.6.intermediate.dense.bias', 'encoder.layer.6.output.dense.weight', 'encoder.layer.6.output.dense.bias', 'encoder.layer.6.output.LayerNorm.weight', 'encoder.layer.6.output.LayerNorm.bias', 'encoder.layer.7.attention.self.query.weight', 'encoder.layer.7.attention.self.query.bias', 'encoder.layer.7.attention.self.key.weight', 'encoder.layer.7.attention.self.key.bias', 'encoder.layer.7.attention.self.value.weight', 'encoder.layer.7.attention.self.value.bias', 'encoder.layer.7.attention.output.dense.weight', 'encoder.layer.7.attention.output.dense.bias', 'encoder.layer.7.attention.output.LayerNorm.weight', 'encoder.layer.7.attention.output.LayerNorm.bias', 'encoder.layer.7.intermediate.dense.weight', 'encoder.layer.7.intermediate.dense.bias', 'encoder.layer.7.output.dense.weight', 'encoder.layer.7.output.dense.bias', 'encoder.layer.7.output.LayerNorm.weight', 'encoder.layer.7.output.LayerNorm.bias', 'encoder.layer.8.attention.self.query.weight', 'encoder.layer.8.attention.self.query.bias', 'encoder.layer.8.attention.self.key.weight', 'encoder.layer.8.attention.self.key.bias', 'encoder.layer.8.attention.self.value.weight', 'encoder.layer.8.attention.self.value.bias', 'encoder.layer.8.attention.output.dense.weight', 'encoder.layer.8.attention.output.dense.bias', 'encoder.layer.8.attention.output.LayerNorm.weight', 'encoder.layer.8.attention.output.LayerNorm.bias', 'encoder.layer.8.intermediate.dense.weight', 'encoder.layer.8.intermediate.dense.bias', 'encoder.layer.8.output.dense.weight', 'encoder.layer.8.output.dense.bias', 'encoder.layer.8.output.LayerNorm.weight', 'encoder.layer.8.output.LayerNorm.bias', 'encoder.layer.9.attention.self.query.weight', 'encoder.layer.9.attention.self.query.bias', 'encoder.layer.9.attention.self.key.weight', 'encoder.layer.9.attention.self.key.bias', 'encoder.layer.9.attention.self.value.weight', 'encoder.layer.9.attention.self.value.bias', 'encoder.layer.9.attention.output.dense.weight', 'encoder.layer.9.attention.output.dense.bias', 'encoder.layer.9.attention.output.LayerNorm.weight', 'encoder.layer.9.attention.output.LayerNorm.bias', 'encoder.layer.9.intermediate.dense.weight', 'encoder.layer.9.intermediate.dense.bias', 'encoder.layer.9.output.dense.weight', 'encoder.layer.9.output.dense.bias', 'encoder.layer.9.output.LayerNorm.weight', 'encoder.layer.9.output.LayerNorm.bias', 'encoder.layer.10.attention.self.query.weight', 'encoder.layer.10.attention.self.query.bias', 'encoder.layer.10.attention.self.key.weight', 'encoder.layer.10.attention.self.key.bias', 'encoder.layer.10.attention.self.value.weight', 'encoder.layer.10.attention.self.value.bias', 'encoder.layer.10.attention.output.dense.weight', 'encoder.layer.10.attention.output.dense.bias', 'encoder.layer.10.attention.output.LayerNorm.weight', 'encoder.layer.10.attention.output.LayerNorm.bias', 'encoder.layer.10.intermediate.dense.weight', 'encoder.layer.10.intermediate.dense.bias', 'encoder.layer.10.output.dense.weight', 'encoder.layer.10.output.dense.bias', 'encoder.layer.10.output.LayerNorm.weight', 'encoder.layer.10.output.LayerNorm.bias', 'encoder.layer.11.attention.self.query.weight', 'encoder.layer.11.attention.self.query.bias', 'encoder.layer.11.attention.self.key.weight', 'encoder.layer.11.attention.self.key.bias', 'encoder.layer.11.attention.self.value.weight', 'encoder.layer.11.attention.self.value.bias', 'encoder.layer.11.attention.output.dense.weight', 'encoder.layer.11.attention.output.dense.bias', 'encoder.layer.11.attention.output.LayerNorm.weight', 'encoder.layer.11.attention.output.LayerNorm.bias', 'encoder.layer.11.intermediate.dense.weight', 'encoder.layer.11.intermediate.dense.bias', 'encoder.layer.11.output.dense.weight', 'encoder.layer.11.output.dense.bias', 'encoder.layer.11.output.LayerNorm.weight', 'encoder.layer.11.output.LayerNorm.bias', 'pooler.dense.weight', 'pooler.dense.bias', 'dense.weight', 'dense.bias', 'out_proj.weight', 'out_proj.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"],"name":"stderr"},{"output_type":"execute_result","data":{"text/plain":["<All keys matched successfully>"]},"metadata":{"tags":[]},"execution_count":77}]},{"cell_type":"code","metadata":{"id":"W0iQCIXYsTlf","executionInfo":{"status":"ok","timestamp":1619174104447,"user_tz":-120,"elapsed":326827,"user":{"displayName":"Sara Fattouh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjGrdvZo2x_8K8abskRubIgMK3SdtS4cG-aQE01nA=s64","userId":"09360594903764573322"}}},"source":["# cam_desc= vec_output_CamembertForSequenceClassification.from_pretrained(\n","#          'camembert-base', # Use the 12-layer BERT model, with an uncased vocab.\n","#           num_labels = 27, # The number of output labels--2 for binary classification.\n","#                     # You can increase this for multi-class tasks.   \n","#           output_attentions = False, # Whether the model returns attentions weights.\n","#           output_hidden_states = False,) # Whether the model returns all hidden-states.\n","        \n","        \n","# checkpoint = torch.load(camembert_model_path_desc)\n","# cam_desc.load_state_dict(checkpoint)"],"execution_count":78,"outputs":[]},{"cell_type":"code","metadata":{"id":"FZfncq4PO_EY","executionInfo":{"status":"ok","timestamp":1619174104447,"user_tz":-120,"elapsed":326821,"user":{"displayName":"Sara Fattouh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjGrdvZo2x_8K8abskRubIgMK3SdtS4cG-aQE01nA=s64","userId":"09360594903764573322"}}},"source":["# flau_title = vec_output_FlaubertForSequenceClassification.from_pretrained(\n","#         'flaubert/flaubert_base_cased', \n","#         num_labels = 27, \n","#         output_attentions = False,\n","#         output_hidden_states = False,)\n","\n","\n","# checkpoint = torch.load(flaubert_model_path_title)\n","# flau_title.load_state_dict(checkpoint)"],"execution_count":79,"outputs":[]},{"cell_type":"code","metadata":{"id":"Qo090Yeusw5V","executionInfo":{"status":"ok","timestamp":1619174104448,"user_tz":-120,"elapsed":326816,"user":{"displayName":"Sara Fattouh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjGrdvZo2x_8K8abskRubIgMK3SdtS4cG-aQE01nA=s64","userId":"09360594903764573322"}}},"source":["# flau_desc = vec_output_FlaubertForSequenceClassification.from_pretrained(\n","#         'flaubert/flaubert_base_cased', \n","#         num_labels = 27, \n","#         output_attentions = False,\n","#         output_hidden_states = False,)\n","\n","\n","# checkpoint = torch.load(flaubert_model_path_desc)\n","# flau_desc.load_state_dict(checkpoint)"],"execution_count":80,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"eU5pHNZKNKr3"},"source":["#  Instantiation  & Training of Fusion Model "]},{"cell_type":"code","metadata":{"id":"8Qpj2jQYu0P-","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1619174179401,"user_tz":-120,"elapsed":401759,"user":{"displayName":"Sara Fattouh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjGrdvZo2x_8K8abskRubIgMK3SdtS4cG-aQE01nA=s64","userId":"09360594903764573322"}},"outputId":"d7b388aa-b00e-4584-e7d5-9662449d7d6d"},"source":["model = vector_fusion() "],"execution_count":81,"outputs":[{"output_type":"stream","text":["Some weights of the model checkpoint at camembert-base were not used when initializing vec_output_CamembertForSequenceClassification: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight']\n","- This IS expected if you are initializing vec_output_CamembertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing vec_output_CamembertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of vec_output_CamembertForSequenceClassification were not initialized from the model checkpoint at camembert-base and are newly initialized: ['embeddings.word_embeddings.weight', 'embeddings.position_embeddings.weight', 'embeddings.token_type_embeddings.weight', 'embeddings.LayerNorm.weight', 'embeddings.LayerNorm.bias', 'encoder.layer.0.attention.self.query.weight', 'encoder.layer.0.attention.self.query.bias', 'encoder.layer.0.attention.self.key.weight', 'encoder.layer.0.attention.self.key.bias', 'encoder.layer.0.attention.self.value.weight', 'encoder.layer.0.attention.self.value.bias', 'encoder.layer.0.attention.output.dense.weight', 'encoder.layer.0.attention.output.dense.bias', 'encoder.layer.0.attention.output.LayerNorm.weight', 'encoder.layer.0.attention.output.LayerNorm.bias', 'encoder.layer.0.intermediate.dense.weight', 'encoder.layer.0.intermediate.dense.bias', 'encoder.layer.0.output.dense.weight', 'encoder.layer.0.output.dense.bias', 'encoder.layer.0.output.LayerNorm.weight', 'encoder.layer.0.output.LayerNorm.bias', 'encoder.layer.1.attention.self.query.weight', 'encoder.layer.1.attention.self.query.bias', 'encoder.layer.1.attention.self.key.weight', 'encoder.layer.1.attention.self.key.bias', 'encoder.layer.1.attention.self.value.weight', 'encoder.layer.1.attention.self.value.bias', 'encoder.layer.1.attention.output.dense.weight', 'encoder.layer.1.attention.output.dense.bias', 'encoder.layer.1.attention.output.LayerNorm.weight', 'encoder.layer.1.attention.output.LayerNorm.bias', 'encoder.layer.1.intermediate.dense.weight', 'encoder.layer.1.intermediate.dense.bias', 'encoder.layer.1.output.dense.weight', 'encoder.layer.1.output.dense.bias', 'encoder.layer.1.output.LayerNorm.weight', 'encoder.layer.1.output.LayerNorm.bias', 'encoder.layer.2.attention.self.query.weight', 'encoder.layer.2.attention.self.query.bias', 'encoder.layer.2.attention.self.key.weight', 'encoder.layer.2.attention.self.key.bias', 'encoder.layer.2.attention.self.value.weight', 'encoder.layer.2.attention.self.value.bias', 'encoder.layer.2.attention.output.dense.weight', 'encoder.layer.2.attention.output.dense.bias', 'encoder.layer.2.attention.output.LayerNorm.weight', 'encoder.layer.2.attention.output.LayerNorm.bias', 'encoder.layer.2.intermediate.dense.weight', 'encoder.layer.2.intermediate.dense.bias', 'encoder.layer.2.output.dense.weight', 'encoder.layer.2.output.dense.bias', 'encoder.layer.2.output.LayerNorm.weight', 'encoder.layer.2.output.LayerNorm.bias', 'encoder.layer.3.attention.self.query.weight', 'encoder.layer.3.attention.self.query.bias', 'encoder.layer.3.attention.self.key.weight', 'encoder.layer.3.attention.self.key.bias', 'encoder.layer.3.attention.self.value.weight', 'encoder.layer.3.attention.self.value.bias', 'encoder.layer.3.attention.output.dense.weight', 'encoder.layer.3.attention.output.dense.bias', 'encoder.layer.3.attention.output.LayerNorm.weight', 'encoder.layer.3.attention.output.LayerNorm.bias', 'encoder.layer.3.intermediate.dense.weight', 'encoder.layer.3.intermediate.dense.bias', 'encoder.layer.3.output.dense.weight', 'encoder.layer.3.output.dense.bias', 'encoder.layer.3.output.LayerNorm.weight', 'encoder.layer.3.output.LayerNorm.bias', 'encoder.layer.4.attention.self.query.weight', 'encoder.layer.4.attention.self.query.bias', 'encoder.layer.4.attention.self.key.weight', 'encoder.layer.4.attention.self.key.bias', 'encoder.layer.4.attention.self.value.weight', 'encoder.layer.4.attention.self.value.bias', 'encoder.layer.4.attention.output.dense.weight', 'encoder.layer.4.attention.output.dense.bias', 'encoder.layer.4.attention.output.LayerNorm.weight', 'encoder.layer.4.attention.output.LayerNorm.bias', 'encoder.layer.4.intermediate.dense.weight', 'encoder.layer.4.intermediate.dense.bias', 'encoder.layer.4.output.dense.weight', 'encoder.layer.4.output.dense.bias', 'encoder.layer.4.output.LayerNorm.weight', 'encoder.layer.4.output.LayerNorm.bias', 'encoder.layer.5.attention.self.query.weight', 'encoder.layer.5.attention.self.query.bias', 'encoder.layer.5.attention.self.key.weight', 'encoder.layer.5.attention.self.key.bias', 'encoder.layer.5.attention.self.value.weight', 'encoder.layer.5.attention.self.value.bias', 'encoder.layer.5.attention.output.dense.weight', 'encoder.layer.5.attention.output.dense.bias', 'encoder.layer.5.attention.output.LayerNorm.weight', 'encoder.layer.5.attention.output.LayerNorm.bias', 'encoder.layer.5.intermediate.dense.weight', 'encoder.layer.5.intermediate.dense.bias', 'encoder.layer.5.output.dense.weight', 'encoder.layer.5.output.dense.bias', 'encoder.layer.5.output.LayerNorm.weight', 'encoder.layer.5.output.LayerNorm.bias', 'encoder.layer.6.attention.self.query.weight', 'encoder.layer.6.attention.self.query.bias', 'encoder.layer.6.attention.self.key.weight', 'encoder.layer.6.attention.self.key.bias', 'encoder.layer.6.attention.self.value.weight', 'encoder.layer.6.attention.self.value.bias', 'encoder.layer.6.attention.output.dense.weight', 'encoder.layer.6.attention.output.dense.bias', 'encoder.layer.6.attention.output.LayerNorm.weight', 'encoder.layer.6.attention.output.LayerNorm.bias', 'encoder.layer.6.intermediate.dense.weight', 'encoder.layer.6.intermediate.dense.bias', 'encoder.layer.6.output.dense.weight', 'encoder.layer.6.output.dense.bias', 'encoder.layer.6.output.LayerNorm.weight', 'encoder.layer.6.output.LayerNorm.bias', 'encoder.layer.7.attention.self.query.weight', 'encoder.layer.7.attention.self.query.bias', 'encoder.layer.7.attention.self.key.weight', 'encoder.layer.7.attention.self.key.bias', 'encoder.layer.7.attention.self.value.weight', 'encoder.layer.7.attention.self.value.bias', 'encoder.layer.7.attention.output.dense.weight', 'encoder.layer.7.attention.output.dense.bias', 'encoder.layer.7.attention.output.LayerNorm.weight', 'encoder.layer.7.attention.output.LayerNorm.bias', 'encoder.layer.7.intermediate.dense.weight', 'encoder.layer.7.intermediate.dense.bias', 'encoder.layer.7.output.dense.weight', 'encoder.layer.7.output.dense.bias', 'encoder.layer.7.output.LayerNorm.weight', 'encoder.layer.7.output.LayerNorm.bias', 'encoder.layer.8.attention.self.query.weight', 'encoder.layer.8.attention.self.query.bias', 'encoder.layer.8.attention.self.key.weight', 'encoder.layer.8.attention.self.key.bias', 'encoder.layer.8.attention.self.value.weight', 'encoder.layer.8.attention.self.value.bias', 'encoder.layer.8.attention.output.dense.weight', 'encoder.layer.8.attention.output.dense.bias', 'encoder.layer.8.attention.output.LayerNorm.weight', 'encoder.layer.8.attention.output.LayerNorm.bias', 'encoder.layer.8.intermediate.dense.weight', 'encoder.layer.8.intermediate.dense.bias', 'encoder.layer.8.output.dense.weight', 'encoder.layer.8.output.dense.bias', 'encoder.layer.8.output.LayerNorm.weight', 'encoder.layer.8.output.LayerNorm.bias', 'encoder.layer.9.attention.self.query.weight', 'encoder.layer.9.attention.self.query.bias', 'encoder.layer.9.attention.self.key.weight', 'encoder.layer.9.attention.self.key.bias', 'encoder.layer.9.attention.self.value.weight', 'encoder.layer.9.attention.self.value.bias', 'encoder.layer.9.attention.output.dense.weight', 'encoder.layer.9.attention.output.dense.bias', 'encoder.layer.9.attention.output.LayerNorm.weight', 'encoder.layer.9.attention.output.LayerNorm.bias', 'encoder.layer.9.intermediate.dense.weight', 'encoder.layer.9.intermediate.dense.bias', 'encoder.layer.9.output.dense.weight', 'encoder.layer.9.output.dense.bias', 'encoder.layer.9.output.LayerNorm.weight', 'encoder.layer.9.output.LayerNorm.bias', 'encoder.layer.10.attention.self.query.weight', 'encoder.layer.10.attention.self.query.bias', 'encoder.layer.10.attention.self.key.weight', 'encoder.layer.10.attention.self.key.bias', 'encoder.layer.10.attention.self.value.weight', 'encoder.layer.10.attention.self.value.bias', 'encoder.layer.10.attention.output.dense.weight', 'encoder.layer.10.attention.output.dense.bias', 'encoder.layer.10.attention.output.LayerNorm.weight', 'encoder.layer.10.attention.output.LayerNorm.bias', 'encoder.layer.10.intermediate.dense.weight', 'encoder.layer.10.intermediate.dense.bias', 'encoder.layer.10.output.dense.weight', 'encoder.layer.10.output.dense.bias', 'encoder.layer.10.output.LayerNorm.weight', 'encoder.layer.10.output.LayerNorm.bias', 'encoder.layer.11.attention.self.query.weight', 'encoder.layer.11.attention.self.query.bias', 'encoder.layer.11.attention.self.key.weight', 'encoder.layer.11.attention.self.key.bias', 'encoder.layer.11.attention.self.value.weight', 'encoder.layer.11.attention.self.value.bias', 'encoder.layer.11.attention.output.dense.weight', 'encoder.layer.11.attention.output.dense.bias', 'encoder.layer.11.attention.output.LayerNorm.weight', 'encoder.layer.11.attention.output.LayerNorm.bias', 'encoder.layer.11.intermediate.dense.weight', 'encoder.layer.11.intermediate.dense.bias', 'encoder.layer.11.output.dense.weight', 'encoder.layer.11.output.dense.bias', 'encoder.layer.11.output.LayerNorm.weight', 'encoder.layer.11.output.LayerNorm.bias', 'pooler.dense.weight', 'pooler.dense.bias', 'dense.weight', 'dense.bias', 'out_proj.weight', 'out_proj.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of the model checkpoint at camembert-base were not used when initializing vec_output_CamembertForSequenceClassification: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight']\n","- This IS expected if you are initializing vec_output_CamembertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing vec_output_CamembertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of vec_output_CamembertForSequenceClassification were not initialized from the model checkpoint at camembert-base and are newly initialized: ['embeddings.word_embeddings.weight', 'embeddings.position_embeddings.weight', 'embeddings.token_type_embeddings.weight', 'embeddings.LayerNorm.weight', 'embeddings.LayerNorm.bias', 'encoder.layer.0.attention.self.query.weight', 'encoder.layer.0.attention.self.query.bias', 'encoder.layer.0.attention.self.key.weight', 'encoder.layer.0.attention.self.key.bias', 'encoder.layer.0.attention.self.value.weight', 'encoder.layer.0.attention.self.value.bias', 'encoder.layer.0.attention.output.dense.weight', 'encoder.layer.0.attention.output.dense.bias', 'encoder.layer.0.attention.output.LayerNorm.weight', 'encoder.layer.0.attention.output.LayerNorm.bias', 'encoder.layer.0.intermediate.dense.weight', 'encoder.layer.0.intermediate.dense.bias', 'encoder.layer.0.output.dense.weight', 'encoder.layer.0.output.dense.bias', 'encoder.layer.0.output.LayerNorm.weight', 'encoder.layer.0.output.LayerNorm.bias', 'encoder.layer.1.attention.self.query.weight', 'encoder.layer.1.attention.self.query.bias', 'encoder.layer.1.attention.self.key.weight', 'encoder.layer.1.attention.self.key.bias', 'encoder.layer.1.attention.self.value.weight', 'encoder.layer.1.attention.self.value.bias', 'encoder.layer.1.attention.output.dense.weight', 'encoder.layer.1.attention.output.dense.bias', 'encoder.layer.1.attention.output.LayerNorm.weight', 'encoder.layer.1.attention.output.LayerNorm.bias', 'encoder.layer.1.intermediate.dense.weight', 'encoder.layer.1.intermediate.dense.bias', 'encoder.layer.1.output.dense.weight', 'encoder.layer.1.output.dense.bias', 'encoder.layer.1.output.LayerNorm.weight', 'encoder.layer.1.output.LayerNorm.bias', 'encoder.layer.2.attention.self.query.weight', 'encoder.layer.2.attention.self.query.bias', 'encoder.layer.2.attention.self.key.weight', 'encoder.layer.2.attention.self.key.bias', 'encoder.layer.2.attention.self.value.weight', 'encoder.layer.2.attention.self.value.bias', 'encoder.layer.2.attention.output.dense.weight', 'encoder.layer.2.attention.output.dense.bias', 'encoder.layer.2.attention.output.LayerNorm.weight', 'encoder.layer.2.attention.output.LayerNorm.bias', 'encoder.layer.2.intermediate.dense.weight', 'encoder.layer.2.intermediate.dense.bias', 'encoder.layer.2.output.dense.weight', 'encoder.layer.2.output.dense.bias', 'encoder.layer.2.output.LayerNorm.weight', 'encoder.layer.2.output.LayerNorm.bias', 'encoder.layer.3.attention.self.query.weight', 'encoder.layer.3.attention.self.query.bias', 'encoder.layer.3.attention.self.key.weight', 'encoder.layer.3.attention.self.key.bias', 'encoder.layer.3.attention.self.value.weight', 'encoder.layer.3.attention.self.value.bias', 'encoder.layer.3.attention.output.dense.weight', 'encoder.layer.3.attention.output.dense.bias', 'encoder.layer.3.attention.output.LayerNorm.weight', 'encoder.layer.3.attention.output.LayerNorm.bias', 'encoder.layer.3.intermediate.dense.weight', 'encoder.layer.3.intermediate.dense.bias', 'encoder.layer.3.output.dense.weight', 'encoder.layer.3.output.dense.bias', 'encoder.layer.3.output.LayerNorm.weight', 'encoder.layer.3.output.LayerNorm.bias', 'encoder.layer.4.attention.self.query.weight', 'encoder.layer.4.attention.self.query.bias', 'encoder.layer.4.attention.self.key.weight', 'encoder.layer.4.attention.self.key.bias', 'encoder.layer.4.attention.self.value.weight', 'encoder.layer.4.attention.self.value.bias', 'encoder.layer.4.attention.output.dense.weight', 'encoder.layer.4.attention.output.dense.bias', 'encoder.layer.4.attention.output.LayerNorm.weight', 'encoder.layer.4.attention.output.LayerNorm.bias', 'encoder.layer.4.intermediate.dense.weight', 'encoder.layer.4.intermediate.dense.bias', 'encoder.layer.4.output.dense.weight', 'encoder.layer.4.output.dense.bias', 'encoder.layer.4.output.LayerNorm.weight', 'encoder.layer.4.output.LayerNorm.bias', 'encoder.layer.5.attention.self.query.weight', 'encoder.layer.5.attention.self.query.bias', 'encoder.layer.5.attention.self.key.weight', 'encoder.layer.5.attention.self.key.bias', 'encoder.layer.5.attention.self.value.weight', 'encoder.layer.5.attention.self.value.bias', 'encoder.layer.5.attention.output.dense.weight', 'encoder.layer.5.attention.output.dense.bias', 'encoder.layer.5.attention.output.LayerNorm.weight', 'encoder.layer.5.attention.output.LayerNorm.bias', 'encoder.layer.5.intermediate.dense.weight', 'encoder.layer.5.intermediate.dense.bias', 'encoder.layer.5.output.dense.weight', 'encoder.layer.5.output.dense.bias', 'encoder.layer.5.output.LayerNorm.weight', 'encoder.layer.5.output.LayerNorm.bias', 'encoder.layer.6.attention.self.query.weight', 'encoder.layer.6.attention.self.query.bias', 'encoder.layer.6.attention.self.key.weight', 'encoder.layer.6.attention.self.key.bias', 'encoder.layer.6.attention.self.value.weight', 'encoder.layer.6.attention.self.value.bias', 'encoder.layer.6.attention.output.dense.weight', 'encoder.layer.6.attention.output.dense.bias', 'encoder.layer.6.attention.output.LayerNorm.weight', 'encoder.layer.6.attention.output.LayerNorm.bias', 'encoder.layer.6.intermediate.dense.weight', 'encoder.layer.6.intermediate.dense.bias', 'encoder.layer.6.output.dense.weight', 'encoder.layer.6.output.dense.bias', 'encoder.layer.6.output.LayerNorm.weight', 'encoder.layer.6.output.LayerNorm.bias', 'encoder.layer.7.attention.self.query.weight', 'encoder.layer.7.attention.self.query.bias', 'encoder.layer.7.attention.self.key.weight', 'encoder.layer.7.attention.self.key.bias', 'encoder.layer.7.attention.self.value.weight', 'encoder.layer.7.attention.self.value.bias', 'encoder.layer.7.attention.output.dense.weight', 'encoder.layer.7.attention.output.dense.bias', 'encoder.layer.7.attention.output.LayerNorm.weight', 'encoder.layer.7.attention.output.LayerNorm.bias', 'encoder.layer.7.intermediate.dense.weight', 'encoder.layer.7.intermediate.dense.bias', 'encoder.layer.7.output.dense.weight', 'encoder.layer.7.output.dense.bias', 'encoder.layer.7.output.LayerNorm.weight', 'encoder.layer.7.output.LayerNorm.bias', 'encoder.layer.8.attention.self.query.weight', 'encoder.layer.8.attention.self.query.bias', 'encoder.layer.8.attention.self.key.weight', 'encoder.layer.8.attention.self.key.bias', 'encoder.layer.8.attention.self.value.weight', 'encoder.layer.8.attention.self.value.bias', 'encoder.layer.8.attention.output.dense.weight', 'encoder.layer.8.attention.output.dense.bias', 'encoder.layer.8.attention.output.LayerNorm.weight', 'encoder.layer.8.attention.output.LayerNorm.bias', 'encoder.layer.8.intermediate.dense.weight', 'encoder.layer.8.intermediate.dense.bias', 'encoder.layer.8.output.dense.weight', 'encoder.layer.8.output.dense.bias', 'encoder.layer.8.output.LayerNorm.weight', 'encoder.layer.8.output.LayerNorm.bias', 'encoder.layer.9.attention.self.query.weight', 'encoder.layer.9.attention.self.query.bias', 'encoder.layer.9.attention.self.key.weight', 'encoder.layer.9.attention.self.key.bias', 'encoder.layer.9.attention.self.value.weight', 'encoder.layer.9.attention.self.value.bias', 'encoder.layer.9.attention.output.dense.weight', 'encoder.layer.9.attention.output.dense.bias', 'encoder.layer.9.attention.output.LayerNorm.weight', 'encoder.layer.9.attention.output.LayerNorm.bias', 'encoder.layer.9.intermediate.dense.weight', 'encoder.layer.9.intermediate.dense.bias', 'encoder.layer.9.output.dense.weight', 'encoder.layer.9.output.dense.bias', 'encoder.layer.9.output.LayerNorm.weight', 'encoder.layer.9.output.LayerNorm.bias', 'encoder.layer.10.attention.self.query.weight', 'encoder.layer.10.attention.self.query.bias', 'encoder.layer.10.attention.self.key.weight', 'encoder.layer.10.attention.self.key.bias', 'encoder.layer.10.attention.self.value.weight', 'encoder.layer.10.attention.self.value.bias', 'encoder.layer.10.attention.output.dense.weight', 'encoder.layer.10.attention.output.dense.bias', 'encoder.layer.10.attention.output.LayerNorm.weight', 'encoder.layer.10.attention.output.LayerNorm.bias', 'encoder.layer.10.intermediate.dense.weight', 'encoder.layer.10.intermediate.dense.bias', 'encoder.layer.10.output.dense.weight', 'encoder.layer.10.output.dense.bias', 'encoder.layer.10.output.LayerNorm.weight', 'encoder.layer.10.output.LayerNorm.bias', 'encoder.layer.11.attention.self.query.weight', 'encoder.layer.11.attention.self.query.bias', 'encoder.layer.11.attention.self.key.weight', 'encoder.layer.11.attention.self.key.bias', 'encoder.layer.11.attention.self.value.weight', 'encoder.layer.11.attention.self.value.bias', 'encoder.layer.11.attention.output.dense.weight', 'encoder.layer.11.attention.output.dense.bias', 'encoder.layer.11.attention.output.LayerNorm.weight', 'encoder.layer.11.attention.output.LayerNorm.bias', 'encoder.layer.11.intermediate.dense.weight', 'encoder.layer.11.intermediate.dense.bias', 'encoder.layer.11.output.dense.weight', 'encoder.layer.11.output.dense.bias', 'encoder.layer.11.output.LayerNorm.weight', 'encoder.layer.11.output.LayerNorm.bias', 'pooler.dense.weight', 'pooler.dense.bias', 'dense.weight', 'dense.bias', 'out_proj.weight', 'out_proj.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of the model checkpoint at flaubert/flaubert_base_cased were not used when initializing vec_output_FlaubertForSequenceClassification: ['pred_layer.proj.bias', 'pred_layer.proj.weight']\n","- This IS expected if you are initializing vec_output_FlaubertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing vec_output_FlaubertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of vec_output_FlaubertForSequenceClassification were not initialized from the model checkpoint at flaubert/flaubert_base_cased and are newly initialized: ['position_embeddings.weight', 'embeddings.weight', 'layer_norm_emb.weight', 'layer_norm_emb.bias', 'attentions.0.q_lin.weight', 'attentions.0.q_lin.bias', 'attentions.0.k_lin.weight', 'attentions.0.k_lin.bias', 'attentions.0.v_lin.weight', 'attentions.0.v_lin.bias', 'attentions.0.out_lin.weight', 'attentions.0.out_lin.bias', 'attentions.1.q_lin.weight', 'attentions.1.q_lin.bias', 'attentions.1.k_lin.weight', 'attentions.1.k_lin.bias', 'attentions.1.v_lin.weight', 'attentions.1.v_lin.bias', 'attentions.1.out_lin.weight', 'attentions.1.out_lin.bias', 'attentions.2.q_lin.weight', 'attentions.2.q_lin.bias', 'attentions.2.k_lin.weight', 'attentions.2.k_lin.bias', 'attentions.2.v_lin.weight', 'attentions.2.v_lin.bias', 'attentions.2.out_lin.weight', 'attentions.2.out_lin.bias', 'attentions.3.q_lin.weight', 'attentions.3.q_lin.bias', 'attentions.3.k_lin.weight', 'attentions.3.k_lin.bias', 'attentions.3.v_lin.weight', 'attentions.3.v_lin.bias', 'attentions.3.out_lin.weight', 'attentions.3.out_lin.bias', 'attentions.4.q_lin.weight', 'attentions.4.q_lin.bias', 'attentions.4.k_lin.weight', 'attentions.4.k_lin.bias', 'attentions.4.v_lin.weight', 'attentions.4.v_lin.bias', 'attentions.4.out_lin.weight', 'attentions.4.out_lin.bias', 'attentions.5.q_lin.weight', 'attentions.5.q_lin.bias', 'attentions.5.k_lin.weight', 'attentions.5.k_lin.bias', 'attentions.5.v_lin.weight', 'attentions.5.v_lin.bias', 'attentions.5.out_lin.weight', 'attentions.5.out_lin.bias', 'attentions.6.q_lin.weight', 'attentions.6.q_lin.bias', 'attentions.6.k_lin.weight', 'attentions.6.k_lin.bias', 'attentions.6.v_lin.weight', 'attentions.6.v_lin.bias', 'attentions.6.out_lin.weight', 'attentions.6.out_lin.bias', 'attentions.7.q_lin.weight', 'attentions.7.q_lin.bias', 'attentions.7.k_lin.weight', 'attentions.7.k_lin.bias', 'attentions.7.v_lin.weight', 'attentions.7.v_lin.bias', 'attentions.7.out_lin.weight', 'attentions.7.out_lin.bias', 'attentions.8.q_lin.weight', 'attentions.8.q_lin.bias', 'attentions.8.k_lin.weight', 'attentions.8.k_lin.bias', 'attentions.8.v_lin.weight', 'attentions.8.v_lin.bias', 'attentions.8.out_lin.weight', 'attentions.8.out_lin.bias', 'attentions.9.q_lin.weight', 'attentions.9.q_lin.bias', 'attentions.9.k_lin.weight', 'attentions.9.k_lin.bias', 'attentions.9.v_lin.weight', 'attentions.9.v_lin.bias', 'attentions.9.out_lin.weight', 'attentions.9.out_lin.bias', 'attentions.10.q_lin.weight', 'attentions.10.q_lin.bias', 'attentions.10.k_lin.weight', 'attentions.10.k_lin.bias', 'attentions.10.v_lin.weight', 'attentions.10.v_lin.bias', 'attentions.10.out_lin.weight', 'attentions.10.out_lin.bias', 'attentions.11.q_lin.weight', 'attentions.11.q_lin.bias', 'attentions.11.k_lin.weight', 'attentions.11.k_lin.bias', 'attentions.11.v_lin.weight', 'attentions.11.v_lin.bias', 'attentions.11.out_lin.weight', 'attentions.11.out_lin.bias', 'layer_norm1.0.weight', 'layer_norm1.0.bias', 'layer_norm1.1.weight', 'layer_norm1.1.bias', 'layer_norm1.2.weight', 'layer_norm1.2.bias', 'layer_norm1.3.weight', 'layer_norm1.3.bias', 'layer_norm1.4.weight', 'layer_norm1.4.bias', 'layer_norm1.5.weight', 'layer_norm1.5.bias', 'layer_norm1.6.weight', 'layer_norm1.6.bias', 'layer_norm1.7.weight', 'layer_norm1.7.bias', 'layer_norm1.8.weight', 'layer_norm1.8.bias', 'layer_norm1.9.weight', 'layer_norm1.9.bias', 'layer_norm1.10.weight', 'layer_norm1.10.bias', 'layer_norm1.11.weight', 'layer_norm1.11.bias', 'ffns.0.lin1.weight', 'ffns.0.lin1.bias', 'ffns.0.lin2.weight', 'ffns.0.lin2.bias', 'ffns.1.lin1.weight', 'ffns.1.lin1.bias', 'ffns.1.lin2.weight', 'ffns.1.lin2.bias', 'ffns.2.lin1.weight', 'ffns.2.lin1.bias', 'ffns.2.lin2.weight', 'ffns.2.lin2.bias', 'ffns.3.lin1.weight', 'ffns.3.lin1.bias', 'ffns.3.lin2.weight', 'ffns.3.lin2.bias', 'ffns.4.lin1.weight', 'ffns.4.lin1.bias', 'ffns.4.lin2.weight', 'ffns.4.lin2.bias', 'ffns.5.lin1.weight', 'ffns.5.lin1.bias', 'ffns.5.lin2.weight', 'ffns.5.lin2.bias', 'ffns.6.lin1.weight', 'ffns.6.lin1.bias', 'ffns.6.lin2.weight', 'ffns.6.lin2.bias', 'ffns.7.lin1.weight', 'ffns.7.lin1.bias', 'ffns.7.lin2.weight', 'ffns.7.lin2.bias', 'ffns.8.lin1.weight', 'ffns.8.lin1.bias', 'ffns.8.lin2.weight', 'ffns.8.lin2.bias', 'ffns.9.lin1.weight', 'ffns.9.lin1.bias', 'ffns.9.lin2.weight', 'ffns.9.lin2.bias', 'ffns.10.lin1.weight', 'ffns.10.lin1.bias', 'ffns.10.lin2.weight', 'ffns.10.lin2.bias', 'ffns.11.lin1.weight', 'ffns.11.lin1.bias', 'ffns.11.lin2.weight', 'ffns.11.lin2.bias', 'layer_norm2.0.weight', 'layer_norm2.0.bias', 'layer_norm2.1.weight', 'layer_norm2.1.bias', 'layer_norm2.2.weight', 'layer_norm2.2.bias', 'layer_norm2.3.weight', 'layer_norm2.3.bias', 'layer_norm2.4.weight', 'layer_norm2.4.bias', 'layer_norm2.5.weight', 'layer_norm2.5.bias', 'layer_norm2.6.weight', 'layer_norm2.6.bias', 'layer_norm2.7.weight', 'layer_norm2.7.bias', 'layer_norm2.8.weight', 'layer_norm2.8.bias', 'layer_norm2.9.weight', 'layer_norm2.9.bias', 'layer_norm2.10.weight', 'layer_norm2.10.bias', 'layer_norm2.11.weight', 'layer_norm2.11.bias', 'sequence_summary.summary.weight', 'sequence_summary.summary.bias', 'classifier.weight', 'classifier.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of the model checkpoint at flaubert/flaubert_base_cased were not used when initializing vec_output_FlaubertForSequenceClassification: ['pred_layer.proj.bias', 'pred_layer.proj.weight']\n","- This IS expected if you are initializing vec_output_FlaubertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing vec_output_FlaubertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of vec_output_FlaubertForSequenceClassification were not initialized from the model checkpoint at flaubert/flaubert_base_cased and are newly initialized: ['position_embeddings.weight', 'embeddings.weight', 'layer_norm_emb.weight', 'layer_norm_emb.bias', 'attentions.0.q_lin.weight', 'attentions.0.q_lin.bias', 'attentions.0.k_lin.weight', 'attentions.0.k_lin.bias', 'attentions.0.v_lin.weight', 'attentions.0.v_lin.bias', 'attentions.0.out_lin.weight', 'attentions.0.out_lin.bias', 'attentions.1.q_lin.weight', 'attentions.1.q_lin.bias', 'attentions.1.k_lin.weight', 'attentions.1.k_lin.bias', 'attentions.1.v_lin.weight', 'attentions.1.v_lin.bias', 'attentions.1.out_lin.weight', 'attentions.1.out_lin.bias', 'attentions.2.q_lin.weight', 'attentions.2.q_lin.bias', 'attentions.2.k_lin.weight', 'attentions.2.k_lin.bias', 'attentions.2.v_lin.weight', 'attentions.2.v_lin.bias', 'attentions.2.out_lin.weight', 'attentions.2.out_lin.bias', 'attentions.3.q_lin.weight', 'attentions.3.q_lin.bias', 'attentions.3.k_lin.weight', 'attentions.3.k_lin.bias', 'attentions.3.v_lin.weight', 'attentions.3.v_lin.bias', 'attentions.3.out_lin.weight', 'attentions.3.out_lin.bias', 'attentions.4.q_lin.weight', 'attentions.4.q_lin.bias', 'attentions.4.k_lin.weight', 'attentions.4.k_lin.bias', 'attentions.4.v_lin.weight', 'attentions.4.v_lin.bias', 'attentions.4.out_lin.weight', 'attentions.4.out_lin.bias', 'attentions.5.q_lin.weight', 'attentions.5.q_lin.bias', 'attentions.5.k_lin.weight', 'attentions.5.k_lin.bias', 'attentions.5.v_lin.weight', 'attentions.5.v_lin.bias', 'attentions.5.out_lin.weight', 'attentions.5.out_lin.bias', 'attentions.6.q_lin.weight', 'attentions.6.q_lin.bias', 'attentions.6.k_lin.weight', 'attentions.6.k_lin.bias', 'attentions.6.v_lin.weight', 'attentions.6.v_lin.bias', 'attentions.6.out_lin.weight', 'attentions.6.out_lin.bias', 'attentions.7.q_lin.weight', 'attentions.7.q_lin.bias', 'attentions.7.k_lin.weight', 'attentions.7.k_lin.bias', 'attentions.7.v_lin.weight', 'attentions.7.v_lin.bias', 'attentions.7.out_lin.weight', 'attentions.7.out_lin.bias', 'attentions.8.q_lin.weight', 'attentions.8.q_lin.bias', 'attentions.8.k_lin.weight', 'attentions.8.k_lin.bias', 'attentions.8.v_lin.weight', 'attentions.8.v_lin.bias', 'attentions.8.out_lin.weight', 'attentions.8.out_lin.bias', 'attentions.9.q_lin.weight', 'attentions.9.q_lin.bias', 'attentions.9.k_lin.weight', 'attentions.9.k_lin.bias', 'attentions.9.v_lin.weight', 'attentions.9.v_lin.bias', 'attentions.9.out_lin.weight', 'attentions.9.out_lin.bias', 'attentions.10.q_lin.weight', 'attentions.10.q_lin.bias', 'attentions.10.k_lin.weight', 'attentions.10.k_lin.bias', 'attentions.10.v_lin.weight', 'attentions.10.v_lin.bias', 'attentions.10.out_lin.weight', 'attentions.10.out_lin.bias', 'attentions.11.q_lin.weight', 'attentions.11.q_lin.bias', 'attentions.11.k_lin.weight', 'attentions.11.k_lin.bias', 'attentions.11.v_lin.weight', 'attentions.11.v_lin.bias', 'attentions.11.out_lin.weight', 'attentions.11.out_lin.bias', 'layer_norm1.0.weight', 'layer_norm1.0.bias', 'layer_norm1.1.weight', 'layer_norm1.1.bias', 'layer_norm1.2.weight', 'layer_norm1.2.bias', 'layer_norm1.3.weight', 'layer_norm1.3.bias', 'layer_norm1.4.weight', 'layer_norm1.4.bias', 'layer_norm1.5.weight', 'layer_norm1.5.bias', 'layer_norm1.6.weight', 'layer_norm1.6.bias', 'layer_norm1.7.weight', 'layer_norm1.7.bias', 'layer_norm1.8.weight', 'layer_norm1.8.bias', 'layer_norm1.9.weight', 'layer_norm1.9.bias', 'layer_norm1.10.weight', 'layer_norm1.10.bias', 'layer_norm1.11.weight', 'layer_norm1.11.bias', 'ffns.0.lin1.weight', 'ffns.0.lin1.bias', 'ffns.0.lin2.weight', 'ffns.0.lin2.bias', 'ffns.1.lin1.weight', 'ffns.1.lin1.bias', 'ffns.1.lin2.weight', 'ffns.1.lin2.bias', 'ffns.2.lin1.weight', 'ffns.2.lin1.bias', 'ffns.2.lin2.weight', 'ffns.2.lin2.bias', 'ffns.3.lin1.weight', 'ffns.3.lin1.bias', 'ffns.3.lin2.weight', 'ffns.3.lin2.bias', 'ffns.4.lin1.weight', 'ffns.4.lin1.bias', 'ffns.4.lin2.weight', 'ffns.4.lin2.bias', 'ffns.5.lin1.weight', 'ffns.5.lin1.bias', 'ffns.5.lin2.weight', 'ffns.5.lin2.bias', 'ffns.6.lin1.weight', 'ffns.6.lin1.bias', 'ffns.6.lin2.weight', 'ffns.6.lin2.bias', 'ffns.7.lin1.weight', 'ffns.7.lin1.bias', 'ffns.7.lin2.weight', 'ffns.7.lin2.bias', 'ffns.8.lin1.weight', 'ffns.8.lin1.bias', 'ffns.8.lin2.weight', 'ffns.8.lin2.bias', 'ffns.9.lin1.weight', 'ffns.9.lin1.bias', 'ffns.9.lin2.weight', 'ffns.9.lin2.bias', 'ffns.10.lin1.weight', 'ffns.10.lin1.bias', 'ffns.10.lin2.weight', 'ffns.10.lin2.bias', 'ffns.11.lin1.weight', 'ffns.11.lin1.bias', 'ffns.11.lin2.weight', 'ffns.11.lin2.bias', 'layer_norm2.0.weight', 'layer_norm2.0.bias', 'layer_norm2.1.weight', 'layer_norm2.1.bias', 'layer_norm2.2.weight', 'layer_norm2.2.bias', 'layer_norm2.3.weight', 'layer_norm2.3.bias', 'layer_norm2.4.weight', 'layer_norm2.4.bias', 'layer_norm2.5.weight', 'layer_norm2.5.bias', 'layer_norm2.6.weight', 'layer_norm2.6.bias', 'layer_norm2.7.weight', 'layer_norm2.7.bias', 'layer_norm2.8.weight', 'layer_norm2.8.bias', 'layer_norm2.9.weight', 'layer_norm2.9.bias', 'layer_norm2.10.weight', 'layer_norm2.10.bias', 'layer_norm2.11.weight', 'layer_norm2.11.bias', 'sequence_summary.summary.weight', 'sequence_summary.summary.bias', 'classifier.weight', 'classifier.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"GN9HIANRu1_R","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1619174181333,"user_tz":-120,"elapsed":403680,"user":{"displayName":"Sara Fattouh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjGrdvZo2x_8K8abskRubIgMK3SdtS4cG-aQE01nA=s64","userId":"09360594903764573322"}},"outputId":"3310119b-3177-4e4a-e640-2caa85cf8c2c"},"source":["model.cuda()"],"execution_count":82,"outputs":[{"output_type":"execute_result","data":{"text/plain":["vector_fusion(\n","  (img_model): SEResnext50_32x4d(\n","    (base_model): SENet(\n","      (layer0): Sequential(\n","        (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n","        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (relu1): ReLU(inplace=True)\n","        (pool): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=True)\n","      )\n","      (layer1): Sequential(\n","        (0): SEResNeXtBottleneck(\n","          (conv1): Conv2d(64, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n","          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (conv3): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu): ReLU(inplace=True)\n","          (se_module): SEModule(\n","            (avg_pool): AdaptiveAvgPool2d(output_size=1)\n","            (fc1): Conv2d(256, 16, kernel_size=(1, 1), stride=(1, 1))\n","            (relu): ReLU(inplace=True)\n","            (fc2): Conv2d(16, 256, kernel_size=(1, 1), stride=(1, 1))\n","            (sigmoid): Sigmoid()\n","          )\n","          (downsample): Sequential(\n","            (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","            (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          )\n","        )\n","        (1): SEResNeXtBottleneck(\n","          (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n","          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (conv3): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu): ReLU(inplace=True)\n","          (se_module): SEModule(\n","            (avg_pool): AdaptiveAvgPool2d(output_size=1)\n","            (fc1): Conv2d(256, 16, kernel_size=(1, 1), stride=(1, 1))\n","            (relu): ReLU(inplace=True)\n","            (fc2): Conv2d(16, 256, kernel_size=(1, 1), stride=(1, 1))\n","            (sigmoid): Sigmoid()\n","          )\n","        )\n","        (2): SEResNeXtBottleneck(\n","          (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n","          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (conv3): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu): ReLU(inplace=True)\n","          (se_module): SEModule(\n","            (avg_pool): AdaptiveAvgPool2d(output_size=1)\n","            (fc1): Conv2d(256, 16, kernel_size=(1, 1), stride=(1, 1))\n","            (relu): ReLU(inplace=True)\n","            (fc2): Conv2d(16, 256, kernel_size=(1, 1), stride=(1, 1))\n","            (sigmoid): Sigmoid()\n","          )\n","        )\n","      )\n","      (layer2): Sequential(\n","        (0): SEResNeXtBottleneck(\n","          (conv1): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=32, bias=False)\n","          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (conv3): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu): ReLU(inplace=True)\n","          (se_module): SEModule(\n","            (avg_pool): AdaptiveAvgPool2d(output_size=1)\n","            (fc1): Conv2d(512, 32, kernel_size=(1, 1), stride=(1, 1))\n","            (relu): ReLU(inplace=True)\n","            (fc2): Conv2d(32, 512, kernel_size=(1, 1), stride=(1, 1))\n","            (sigmoid): Sigmoid()\n","          )\n","          (downsample): Sequential(\n","            (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n","            (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          )\n","        )\n","        (1): SEResNeXtBottleneck(\n","          (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n","          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (conv3): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu): ReLU(inplace=True)\n","          (se_module): SEModule(\n","            (avg_pool): AdaptiveAvgPool2d(output_size=1)\n","            (fc1): Conv2d(512, 32, kernel_size=(1, 1), stride=(1, 1))\n","            (relu): ReLU(inplace=True)\n","            (fc2): Conv2d(32, 512, kernel_size=(1, 1), stride=(1, 1))\n","            (sigmoid): Sigmoid()\n","          )\n","        )\n","        (2): SEResNeXtBottleneck(\n","          (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n","          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (conv3): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu): ReLU(inplace=True)\n","          (se_module): SEModule(\n","            (avg_pool): AdaptiveAvgPool2d(output_size=1)\n","            (fc1): Conv2d(512, 32, kernel_size=(1, 1), stride=(1, 1))\n","            (relu): ReLU(inplace=True)\n","            (fc2): Conv2d(32, 512, kernel_size=(1, 1), stride=(1, 1))\n","            (sigmoid): Sigmoid()\n","          )\n","        )\n","        (3): SEResNeXtBottleneck(\n","          (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n","          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (conv3): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu): ReLU(inplace=True)\n","          (se_module): SEModule(\n","            (avg_pool): AdaptiveAvgPool2d(output_size=1)\n","            (fc1): Conv2d(512, 32, kernel_size=(1, 1), stride=(1, 1))\n","            (relu): ReLU(inplace=True)\n","            (fc2): Conv2d(32, 512, kernel_size=(1, 1), stride=(1, 1))\n","            (sigmoid): Sigmoid()\n","          )\n","        )\n","      )\n","      (layer3): Sequential(\n","        (0): SEResNeXtBottleneck(\n","          (conv1): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=32, bias=False)\n","          (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (conv3): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu): ReLU(inplace=True)\n","          (se_module): SEModule(\n","            (avg_pool): AdaptiveAvgPool2d(output_size=1)\n","            (fc1): Conv2d(1024, 64, kernel_size=(1, 1), stride=(1, 1))\n","            (relu): ReLU(inplace=True)\n","            (fc2): Conv2d(64, 1024, kernel_size=(1, 1), stride=(1, 1))\n","            (sigmoid): Sigmoid()\n","          )\n","          (downsample): Sequential(\n","            (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n","            (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          )\n","        )\n","        (1): SEResNeXtBottleneck(\n","          (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n","          (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (conv3): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu): ReLU(inplace=True)\n","          (se_module): SEModule(\n","            (avg_pool): AdaptiveAvgPool2d(output_size=1)\n","            (fc1): Conv2d(1024, 64, kernel_size=(1, 1), stride=(1, 1))\n","            (relu): ReLU(inplace=True)\n","            (fc2): Conv2d(64, 1024, kernel_size=(1, 1), stride=(1, 1))\n","            (sigmoid): Sigmoid()\n","          )\n","        )\n","        (2): SEResNeXtBottleneck(\n","          (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n","          (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (conv3): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu): ReLU(inplace=True)\n","          (se_module): SEModule(\n","            (avg_pool): AdaptiveAvgPool2d(output_size=1)\n","            (fc1): Conv2d(1024, 64, kernel_size=(1, 1), stride=(1, 1))\n","            (relu): ReLU(inplace=True)\n","            (fc2): Conv2d(64, 1024, kernel_size=(1, 1), stride=(1, 1))\n","            (sigmoid): Sigmoid()\n","          )\n","        )\n","        (3): SEResNeXtBottleneck(\n","          (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n","          (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (conv3): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu): ReLU(inplace=True)\n","          (se_module): SEModule(\n","            (avg_pool): AdaptiveAvgPool2d(output_size=1)\n","            (fc1): Conv2d(1024, 64, kernel_size=(1, 1), stride=(1, 1))\n","            (relu): ReLU(inplace=True)\n","            (fc2): Conv2d(64, 1024, kernel_size=(1, 1), stride=(1, 1))\n","            (sigmoid): Sigmoid()\n","          )\n","        )\n","        (4): SEResNeXtBottleneck(\n","          (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n","          (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (conv3): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu): ReLU(inplace=True)\n","          (se_module): SEModule(\n","            (avg_pool): AdaptiveAvgPool2d(output_size=1)\n","            (fc1): Conv2d(1024, 64, kernel_size=(1, 1), stride=(1, 1))\n","            (relu): ReLU(inplace=True)\n","            (fc2): Conv2d(64, 1024, kernel_size=(1, 1), stride=(1, 1))\n","            (sigmoid): Sigmoid()\n","          )\n","        )\n","        (5): SEResNeXtBottleneck(\n","          (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n","          (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (conv3): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu): ReLU(inplace=True)\n","          (se_module): SEModule(\n","            (avg_pool): AdaptiveAvgPool2d(output_size=1)\n","            (fc1): Conv2d(1024, 64, kernel_size=(1, 1), stride=(1, 1))\n","            (relu): ReLU(inplace=True)\n","            (fc2): Conv2d(64, 1024, kernel_size=(1, 1), stride=(1, 1))\n","            (sigmoid): Sigmoid()\n","          )\n","        )\n","      )\n","      (layer4): Sequential(\n","        (0): SEResNeXtBottleneck(\n","          (conv1): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (conv2): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=32, bias=False)\n","          (bn2): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (conv3): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu): ReLU(inplace=True)\n","          (se_module): SEModule(\n","            (avg_pool): AdaptiveAvgPool2d(output_size=1)\n","            (fc1): Conv2d(2048, 128, kernel_size=(1, 1), stride=(1, 1))\n","            (relu): ReLU(inplace=True)\n","            (fc2): Conv2d(128, 2048, kernel_size=(1, 1), stride=(1, 1))\n","            (sigmoid): Sigmoid()\n","          )\n","          (downsample): Sequential(\n","            (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n","            (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          )\n","        )\n","        (1): SEResNeXtBottleneck(\n","          (conv1): Conv2d(2048, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (conv2): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n","          (bn2): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (conv3): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu): ReLU(inplace=True)\n","          (se_module): SEModule(\n","            (avg_pool): AdaptiveAvgPool2d(output_size=1)\n","            (fc1): Conv2d(2048, 128, kernel_size=(1, 1), stride=(1, 1))\n","            (relu): ReLU(inplace=True)\n","            (fc2): Conv2d(128, 2048, kernel_size=(1, 1), stride=(1, 1))\n","            (sigmoid): Sigmoid()\n","          )\n","        )\n","        (2): SEResNeXtBottleneck(\n","          (conv1): Conv2d(2048, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (conv2): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n","          (bn2): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (conv3): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu): ReLU(inplace=True)\n","          (se_module): SEModule(\n","            (avg_pool): AdaptiveAvgPool2d(output_size=1)\n","            (fc1): Conv2d(2048, 128, kernel_size=(1, 1), stride=(1, 1))\n","            (relu): ReLU(inplace=True)\n","            (fc2): Conv2d(128, 2048, kernel_size=(1, 1), stride=(1, 1))\n","            (sigmoid): Sigmoid()\n","          )\n","        )\n","      )\n","      (avg_pool): AvgPool2d(kernel_size=7, stride=1, padding=0)\n","      (last_linear): Linear(in_features=2048, out_features=1000, bias=True)\n","    )\n","    (l0): Identity()\n","  )\n","  (cam_model_title): vec_output_CamembertForSequenceClassification(\n","    (embeddings): RobertaEmbeddings(\n","      (word_embeddings): Embedding(32005, 768, padding_idx=1)\n","      (position_embeddings): Embedding(514, 768, padding_idx=1)\n","      (token_type_embeddings): Embedding(1, 768)\n","      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","      (dropout): Dropout(p=0.1, inplace=False)\n","    )\n","    (encoder): RobertaEncoder(\n","      (layer): ModuleList(\n","        (0): RobertaLayer(\n","          (attention): RobertaAttention(\n","            (self): RobertaSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): RobertaSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): RobertaIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): RobertaOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (1): RobertaLayer(\n","          (attention): RobertaAttention(\n","            (self): RobertaSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): RobertaSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): RobertaIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): RobertaOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (2): RobertaLayer(\n","          (attention): RobertaAttention(\n","            (self): RobertaSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): RobertaSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): RobertaIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): RobertaOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (3): RobertaLayer(\n","          (attention): RobertaAttention(\n","            (self): RobertaSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): RobertaSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): RobertaIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): RobertaOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (4): RobertaLayer(\n","          (attention): RobertaAttention(\n","            (self): RobertaSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): RobertaSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): RobertaIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): RobertaOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (5): RobertaLayer(\n","          (attention): RobertaAttention(\n","            (self): RobertaSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): RobertaSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): RobertaIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): RobertaOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (6): RobertaLayer(\n","          (attention): RobertaAttention(\n","            (self): RobertaSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): RobertaSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): RobertaIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): RobertaOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (7): RobertaLayer(\n","          (attention): RobertaAttention(\n","            (self): RobertaSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): RobertaSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): RobertaIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): RobertaOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (8): RobertaLayer(\n","          (attention): RobertaAttention(\n","            (self): RobertaSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): RobertaSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): RobertaIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): RobertaOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (9): RobertaLayer(\n","          (attention): RobertaAttention(\n","            (self): RobertaSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): RobertaSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): RobertaIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): RobertaOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (10): RobertaLayer(\n","          (attention): RobertaAttention(\n","            (self): RobertaSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): RobertaSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): RobertaIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): RobertaOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (11): RobertaLayer(\n","          (attention): RobertaAttention(\n","            (self): RobertaSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): RobertaSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): RobertaIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): RobertaOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","    )\n","    (pooler): RobertaPooler(\n","      (dense): Linear(in_features=768, out_features=768, bias=True)\n","      (activation): Tanh()\n","    )\n","    (roberta): CamembertModel(\n","      (embeddings): RobertaEmbeddings(\n","        (word_embeddings): Embedding(32005, 768, padding_idx=1)\n","        (position_embeddings): Embedding(514, 768, padding_idx=1)\n","        (token_type_embeddings): Embedding(1, 768)\n","        (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","        (dropout): Dropout(p=0.1, inplace=False)\n","      )\n","      (encoder): RobertaEncoder(\n","        (layer): ModuleList(\n","          (0): RobertaLayer(\n","            (attention): RobertaAttention(\n","              (self): RobertaSelfAttention(\n","                (query): Linear(in_features=768, out_features=768, bias=True)\n","                (key): Linear(in_features=768, out_features=768, bias=True)\n","                (value): Linear(in_features=768, out_features=768, bias=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","              (output): RobertaSelfOutput(\n","                (dense): Linear(in_features=768, out_features=768, bias=True)\n","                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","            )\n","            (intermediate): RobertaIntermediate(\n","              (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            )\n","            (output): RobertaOutput(\n","              (dense): Linear(in_features=3072, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (1): RobertaLayer(\n","            (attention): RobertaAttention(\n","              (self): RobertaSelfAttention(\n","                (query): Linear(in_features=768, out_features=768, bias=True)\n","                (key): Linear(in_features=768, out_features=768, bias=True)\n","                (value): Linear(in_features=768, out_features=768, bias=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","              (output): RobertaSelfOutput(\n","                (dense): Linear(in_features=768, out_features=768, bias=True)\n","                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","            )\n","            (intermediate): RobertaIntermediate(\n","              (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            )\n","            (output): RobertaOutput(\n","              (dense): Linear(in_features=3072, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (2): RobertaLayer(\n","            (attention): RobertaAttention(\n","              (self): RobertaSelfAttention(\n","                (query): Linear(in_features=768, out_features=768, bias=True)\n","                (key): Linear(in_features=768, out_features=768, bias=True)\n","                (value): Linear(in_features=768, out_features=768, bias=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","              (output): RobertaSelfOutput(\n","                (dense): Linear(in_features=768, out_features=768, bias=True)\n","                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","            )\n","            (intermediate): RobertaIntermediate(\n","              (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            )\n","            (output): RobertaOutput(\n","              (dense): Linear(in_features=3072, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (3): RobertaLayer(\n","            (attention): RobertaAttention(\n","              (self): RobertaSelfAttention(\n","                (query): Linear(in_features=768, out_features=768, bias=True)\n","                (key): Linear(in_features=768, out_features=768, bias=True)\n","                (value): Linear(in_features=768, out_features=768, bias=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","              (output): RobertaSelfOutput(\n","                (dense): Linear(in_features=768, out_features=768, bias=True)\n","                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","            )\n","            (intermediate): RobertaIntermediate(\n","              (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            )\n","            (output): RobertaOutput(\n","              (dense): Linear(in_features=3072, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (4): RobertaLayer(\n","            (attention): RobertaAttention(\n","              (self): RobertaSelfAttention(\n","                (query): Linear(in_features=768, out_features=768, bias=True)\n","                (key): Linear(in_features=768, out_features=768, bias=True)\n","                (value): Linear(in_features=768, out_features=768, bias=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","              (output): RobertaSelfOutput(\n","                (dense): Linear(in_features=768, out_features=768, bias=True)\n","                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","            )\n","            (intermediate): RobertaIntermediate(\n","              (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            )\n","            (output): RobertaOutput(\n","              (dense): Linear(in_features=3072, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (5): RobertaLayer(\n","            (attention): RobertaAttention(\n","              (self): RobertaSelfAttention(\n","                (query): Linear(in_features=768, out_features=768, bias=True)\n","                (key): Linear(in_features=768, out_features=768, bias=True)\n","                (value): Linear(in_features=768, out_features=768, bias=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","              (output): RobertaSelfOutput(\n","                (dense): Linear(in_features=768, out_features=768, bias=True)\n","                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","            )\n","            (intermediate): RobertaIntermediate(\n","              (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            )\n","            (output): RobertaOutput(\n","              (dense): Linear(in_features=3072, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (6): RobertaLayer(\n","            (attention): RobertaAttention(\n","              (self): RobertaSelfAttention(\n","                (query): Linear(in_features=768, out_features=768, bias=True)\n","                (key): Linear(in_features=768, out_features=768, bias=True)\n","                (value): Linear(in_features=768, out_features=768, bias=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","              (output): RobertaSelfOutput(\n","                (dense): Linear(in_features=768, out_features=768, bias=True)\n","                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","            )\n","            (intermediate): RobertaIntermediate(\n","              (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            )\n","            (output): RobertaOutput(\n","              (dense): Linear(in_features=3072, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (7): RobertaLayer(\n","            (attention): RobertaAttention(\n","              (self): RobertaSelfAttention(\n","                (query): Linear(in_features=768, out_features=768, bias=True)\n","                (key): Linear(in_features=768, out_features=768, bias=True)\n","                (value): Linear(in_features=768, out_features=768, bias=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","              (output): RobertaSelfOutput(\n","                (dense): Linear(in_features=768, out_features=768, bias=True)\n","                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","            )\n","            (intermediate): RobertaIntermediate(\n","              (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            )\n","            (output): RobertaOutput(\n","              (dense): Linear(in_features=3072, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (8): RobertaLayer(\n","            (attention): RobertaAttention(\n","              (self): RobertaSelfAttention(\n","                (query): Linear(in_features=768, out_features=768, bias=True)\n","                (key): Linear(in_features=768, out_features=768, bias=True)\n","                (value): Linear(in_features=768, out_features=768, bias=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","              (output): RobertaSelfOutput(\n","                (dense): Linear(in_features=768, out_features=768, bias=True)\n","                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","            )\n","            (intermediate): RobertaIntermediate(\n","              (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            )\n","            (output): RobertaOutput(\n","              (dense): Linear(in_features=3072, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (9): RobertaLayer(\n","            (attention): RobertaAttention(\n","              (self): RobertaSelfAttention(\n","                (query): Linear(in_features=768, out_features=768, bias=True)\n","                (key): Linear(in_features=768, out_features=768, bias=True)\n","                (value): Linear(in_features=768, out_features=768, bias=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","              (output): RobertaSelfOutput(\n","                (dense): Linear(in_features=768, out_features=768, bias=True)\n","                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","            )\n","            (intermediate): RobertaIntermediate(\n","              (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            )\n","            (output): RobertaOutput(\n","              (dense): Linear(in_features=3072, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (10): RobertaLayer(\n","            (attention): RobertaAttention(\n","              (self): RobertaSelfAttention(\n","                (query): Linear(in_features=768, out_features=768, bias=True)\n","                (key): Linear(in_features=768, out_features=768, bias=True)\n","                (value): Linear(in_features=768, out_features=768, bias=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","              (output): RobertaSelfOutput(\n","                (dense): Linear(in_features=768, out_features=768, bias=True)\n","                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","            )\n","            (intermediate): RobertaIntermediate(\n","              (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            )\n","            (output): RobertaOutput(\n","              (dense): Linear(in_features=3072, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (11): RobertaLayer(\n","            (attention): RobertaAttention(\n","              (self): RobertaSelfAttention(\n","                (query): Linear(in_features=768, out_features=768, bias=True)\n","                (key): Linear(in_features=768, out_features=768, bias=True)\n","                (value): Linear(in_features=768, out_features=768, bias=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","              (output): RobertaSelfOutput(\n","                (dense): Linear(in_features=768, out_features=768, bias=True)\n","                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","            )\n","            (intermediate): RobertaIntermediate(\n","              (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            )\n","            (output): RobertaOutput(\n","              (dense): Linear(in_features=3072, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","        )\n","      )\n","      (pooler): RobertaPooler(\n","        (dense): Linear(in_features=768, out_features=768, bias=True)\n","        (activation): Tanh()\n","      )\n","    )\n","    (dense): Linear(in_features=196608, out_features=768, bias=True)\n","    (dropout): Dropout(p=0.1, inplace=False)\n","    (out_proj): Identity()\n","  )\n","  (cam_model_desc): vec_output_CamembertForSequenceClassification(\n","    (embeddings): RobertaEmbeddings(\n","      (word_embeddings): Embedding(32005, 768, padding_idx=1)\n","      (position_embeddings): Embedding(514, 768, padding_idx=1)\n","      (token_type_embeddings): Embedding(1, 768)\n","      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","      (dropout): Dropout(p=0.1, inplace=False)\n","    )\n","    (encoder): RobertaEncoder(\n","      (layer): ModuleList(\n","        (0): RobertaLayer(\n","          (attention): RobertaAttention(\n","            (self): RobertaSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): RobertaSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): RobertaIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): RobertaOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (1): RobertaLayer(\n","          (attention): RobertaAttention(\n","            (self): RobertaSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): RobertaSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): RobertaIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): RobertaOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (2): RobertaLayer(\n","          (attention): RobertaAttention(\n","            (self): RobertaSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): RobertaSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): RobertaIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): RobertaOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (3): RobertaLayer(\n","          (attention): RobertaAttention(\n","            (self): RobertaSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): RobertaSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): RobertaIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): RobertaOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (4): RobertaLayer(\n","          (attention): RobertaAttention(\n","            (self): RobertaSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): RobertaSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): RobertaIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): RobertaOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (5): RobertaLayer(\n","          (attention): RobertaAttention(\n","            (self): RobertaSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): RobertaSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): RobertaIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): RobertaOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (6): RobertaLayer(\n","          (attention): RobertaAttention(\n","            (self): RobertaSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): RobertaSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): RobertaIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): RobertaOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (7): RobertaLayer(\n","          (attention): RobertaAttention(\n","            (self): RobertaSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): RobertaSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): RobertaIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): RobertaOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (8): RobertaLayer(\n","          (attention): RobertaAttention(\n","            (self): RobertaSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): RobertaSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): RobertaIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): RobertaOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (9): RobertaLayer(\n","          (attention): RobertaAttention(\n","            (self): RobertaSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): RobertaSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): RobertaIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): RobertaOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (10): RobertaLayer(\n","          (attention): RobertaAttention(\n","            (self): RobertaSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): RobertaSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): RobertaIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): RobertaOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (11): RobertaLayer(\n","          (attention): RobertaAttention(\n","            (self): RobertaSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): RobertaSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): RobertaIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): RobertaOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","    )\n","    (pooler): RobertaPooler(\n","      (dense): Linear(in_features=768, out_features=768, bias=True)\n","      (activation): Tanh()\n","    )\n","    (roberta): CamembertModel(\n","      (embeddings): RobertaEmbeddings(\n","        (word_embeddings): Embedding(32005, 768, padding_idx=1)\n","        (position_embeddings): Embedding(514, 768, padding_idx=1)\n","        (token_type_embeddings): Embedding(1, 768)\n","        (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","        (dropout): Dropout(p=0.1, inplace=False)\n","      )\n","      (encoder): RobertaEncoder(\n","        (layer): ModuleList(\n","          (0): RobertaLayer(\n","            (attention): RobertaAttention(\n","              (self): RobertaSelfAttention(\n","                (query): Linear(in_features=768, out_features=768, bias=True)\n","                (key): Linear(in_features=768, out_features=768, bias=True)\n","                (value): Linear(in_features=768, out_features=768, bias=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","              (output): RobertaSelfOutput(\n","                (dense): Linear(in_features=768, out_features=768, bias=True)\n","                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","            )\n","            (intermediate): RobertaIntermediate(\n","              (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            )\n","            (output): RobertaOutput(\n","              (dense): Linear(in_features=3072, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (1): RobertaLayer(\n","            (attention): RobertaAttention(\n","              (self): RobertaSelfAttention(\n","                (query): Linear(in_features=768, out_features=768, bias=True)\n","                (key): Linear(in_features=768, out_features=768, bias=True)\n","                (value): Linear(in_features=768, out_features=768, bias=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","              (output): RobertaSelfOutput(\n","                (dense): Linear(in_features=768, out_features=768, bias=True)\n","                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","            )\n","            (intermediate): RobertaIntermediate(\n","              (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            )\n","            (output): RobertaOutput(\n","              (dense): Linear(in_features=3072, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (2): RobertaLayer(\n","            (attention): RobertaAttention(\n","              (self): RobertaSelfAttention(\n","                (query): Linear(in_features=768, out_features=768, bias=True)\n","                (key): Linear(in_features=768, out_features=768, bias=True)\n","                (value): Linear(in_features=768, out_features=768, bias=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","              (output): RobertaSelfOutput(\n","                (dense): Linear(in_features=768, out_features=768, bias=True)\n","                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","            )\n","            (intermediate): RobertaIntermediate(\n","              (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            )\n","            (output): RobertaOutput(\n","              (dense): Linear(in_features=3072, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (3): RobertaLayer(\n","            (attention): RobertaAttention(\n","              (self): RobertaSelfAttention(\n","                (query): Linear(in_features=768, out_features=768, bias=True)\n","                (key): Linear(in_features=768, out_features=768, bias=True)\n","                (value): Linear(in_features=768, out_features=768, bias=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","              (output): RobertaSelfOutput(\n","                (dense): Linear(in_features=768, out_features=768, bias=True)\n","                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","            )\n","            (intermediate): RobertaIntermediate(\n","              (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            )\n","            (output): RobertaOutput(\n","              (dense): Linear(in_features=3072, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (4): RobertaLayer(\n","            (attention): RobertaAttention(\n","              (self): RobertaSelfAttention(\n","                (query): Linear(in_features=768, out_features=768, bias=True)\n","                (key): Linear(in_features=768, out_features=768, bias=True)\n","                (value): Linear(in_features=768, out_features=768, bias=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","              (output): RobertaSelfOutput(\n","                (dense): Linear(in_features=768, out_features=768, bias=True)\n","                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","            )\n","            (intermediate): RobertaIntermediate(\n","              (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            )\n","            (output): RobertaOutput(\n","              (dense): Linear(in_features=3072, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (5): RobertaLayer(\n","            (attention): RobertaAttention(\n","              (self): RobertaSelfAttention(\n","                (query): Linear(in_features=768, out_features=768, bias=True)\n","                (key): Linear(in_features=768, out_features=768, bias=True)\n","                (value): Linear(in_features=768, out_features=768, bias=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","              (output): RobertaSelfOutput(\n","                (dense): Linear(in_features=768, out_features=768, bias=True)\n","                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","            )\n","            (intermediate): RobertaIntermediate(\n","              (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            )\n","            (output): RobertaOutput(\n","              (dense): Linear(in_features=3072, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (6): RobertaLayer(\n","            (attention): RobertaAttention(\n","              (self): RobertaSelfAttention(\n","                (query): Linear(in_features=768, out_features=768, bias=True)\n","                (key): Linear(in_features=768, out_features=768, bias=True)\n","                (value): Linear(in_features=768, out_features=768, bias=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","              (output): RobertaSelfOutput(\n","                (dense): Linear(in_features=768, out_features=768, bias=True)\n","                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","            )\n","            (intermediate): RobertaIntermediate(\n","              (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            )\n","            (output): RobertaOutput(\n","              (dense): Linear(in_features=3072, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (7): RobertaLayer(\n","            (attention): RobertaAttention(\n","              (self): RobertaSelfAttention(\n","                (query): Linear(in_features=768, out_features=768, bias=True)\n","                (key): Linear(in_features=768, out_features=768, bias=True)\n","                (value): Linear(in_features=768, out_features=768, bias=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","              (output): RobertaSelfOutput(\n","                (dense): Linear(in_features=768, out_features=768, bias=True)\n","                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","            )\n","            (intermediate): RobertaIntermediate(\n","              (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            )\n","            (output): RobertaOutput(\n","              (dense): Linear(in_features=3072, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (8): RobertaLayer(\n","            (attention): RobertaAttention(\n","              (self): RobertaSelfAttention(\n","                (query): Linear(in_features=768, out_features=768, bias=True)\n","                (key): Linear(in_features=768, out_features=768, bias=True)\n","                (value): Linear(in_features=768, out_features=768, bias=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","              (output): RobertaSelfOutput(\n","                (dense): Linear(in_features=768, out_features=768, bias=True)\n","                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","            )\n","            (intermediate): RobertaIntermediate(\n","              (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            )\n","            (output): RobertaOutput(\n","              (dense): Linear(in_features=3072, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (9): RobertaLayer(\n","            (attention): RobertaAttention(\n","              (self): RobertaSelfAttention(\n","                (query): Linear(in_features=768, out_features=768, bias=True)\n","                (key): Linear(in_features=768, out_features=768, bias=True)\n","                (value): Linear(in_features=768, out_features=768, bias=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","              (output): RobertaSelfOutput(\n","                (dense): Linear(in_features=768, out_features=768, bias=True)\n","                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","            )\n","            (intermediate): RobertaIntermediate(\n","              (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            )\n","            (output): RobertaOutput(\n","              (dense): Linear(in_features=3072, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (10): RobertaLayer(\n","            (attention): RobertaAttention(\n","              (self): RobertaSelfAttention(\n","                (query): Linear(in_features=768, out_features=768, bias=True)\n","                (key): Linear(in_features=768, out_features=768, bias=True)\n","                (value): Linear(in_features=768, out_features=768, bias=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","              (output): RobertaSelfOutput(\n","                (dense): Linear(in_features=768, out_features=768, bias=True)\n","                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","            )\n","            (intermediate): RobertaIntermediate(\n","              (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            )\n","            (output): RobertaOutput(\n","              (dense): Linear(in_features=3072, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (11): RobertaLayer(\n","            (attention): RobertaAttention(\n","              (self): RobertaSelfAttention(\n","                (query): Linear(in_features=768, out_features=768, bias=True)\n","                (key): Linear(in_features=768, out_features=768, bias=True)\n","                (value): Linear(in_features=768, out_features=768, bias=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","              (output): RobertaSelfOutput(\n","                (dense): Linear(in_features=768, out_features=768, bias=True)\n","                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","            )\n","            (intermediate): RobertaIntermediate(\n","              (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            )\n","            (output): RobertaOutput(\n","              (dense): Linear(in_features=3072, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","        )\n","      )\n","      (pooler): RobertaPooler(\n","        (dense): Linear(in_features=768, out_features=768, bias=True)\n","        (activation): Tanh()\n","      )\n","    )\n","    (dense): Linear(in_features=196608, out_features=768, bias=True)\n","    (dropout): Dropout(p=0.1, inplace=False)\n","    (out_proj): Identity()\n","  )\n","  (flau_model_title): vec_output_FlaubertForSequenceClassification(\n","    (position_embeddings): Embedding(512, 768)\n","    (embeddings): Embedding(68729, 768, padding_idx=2)\n","    (layer_norm_emb): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","    (attentions): ModuleList(\n","      (0): MultiHeadAttention(\n","        (q_lin): Linear(in_features=768, out_features=768, bias=True)\n","        (k_lin): Linear(in_features=768, out_features=768, bias=True)\n","        (v_lin): Linear(in_features=768, out_features=768, bias=True)\n","        (out_lin): Linear(in_features=768, out_features=768, bias=True)\n","      )\n","      (1): MultiHeadAttention(\n","        (q_lin): Linear(in_features=768, out_features=768, bias=True)\n","        (k_lin): Linear(in_features=768, out_features=768, bias=True)\n","        (v_lin): Linear(in_features=768, out_features=768, bias=True)\n","        (out_lin): Linear(in_features=768, out_features=768, bias=True)\n","      )\n","      (2): MultiHeadAttention(\n","        (q_lin): Linear(in_features=768, out_features=768, bias=True)\n","        (k_lin): Linear(in_features=768, out_features=768, bias=True)\n","        (v_lin): Linear(in_features=768, out_features=768, bias=True)\n","        (out_lin): Linear(in_features=768, out_features=768, bias=True)\n","      )\n","      (3): MultiHeadAttention(\n","        (q_lin): Linear(in_features=768, out_features=768, bias=True)\n","        (k_lin): Linear(in_features=768, out_features=768, bias=True)\n","        (v_lin): Linear(in_features=768, out_features=768, bias=True)\n","        (out_lin): Linear(in_features=768, out_features=768, bias=True)\n","      )\n","      (4): MultiHeadAttention(\n","        (q_lin): Linear(in_features=768, out_features=768, bias=True)\n","        (k_lin): Linear(in_features=768, out_features=768, bias=True)\n","        (v_lin): Linear(in_features=768, out_features=768, bias=True)\n","        (out_lin): Linear(in_features=768, out_features=768, bias=True)\n","      )\n","      (5): MultiHeadAttention(\n","        (q_lin): Linear(in_features=768, out_features=768, bias=True)\n","        (k_lin): Linear(in_features=768, out_features=768, bias=True)\n","        (v_lin): Linear(in_features=768, out_features=768, bias=True)\n","        (out_lin): Linear(in_features=768, out_features=768, bias=True)\n","      )\n","      (6): MultiHeadAttention(\n","        (q_lin): Linear(in_features=768, out_features=768, bias=True)\n","        (k_lin): Linear(in_features=768, out_features=768, bias=True)\n","        (v_lin): Linear(in_features=768, out_features=768, bias=True)\n","        (out_lin): Linear(in_features=768, out_features=768, bias=True)\n","      )\n","      (7): MultiHeadAttention(\n","        (q_lin): Linear(in_features=768, out_features=768, bias=True)\n","        (k_lin): Linear(in_features=768, out_features=768, bias=True)\n","        (v_lin): Linear(in_features=768, out_features=768, bias=True)\n","        (out_lin): Linear(in_features=768, out_features=768, bias=True)\n","      )\n","      (8): MultiHeadAttention(\n","        (q_lin): Linear(in_features=768, out_features=768, bias=True)\n","        (k_lin): Linear(in_features=768, out_features=768, bias=True)\n","        (v_lin): Linear(in_features=768, out_features=768, bias=True)\n","        (out_lin): Linear(in_features=768, out_features=768, bias=True)\n","      )\n","      (9): MultiHeadAttention(\n","        (q_lin): Linear(in_features=768, out_features=768, bias=True)\n","        (k_lin): Linear(in_features=768, out_features=768, bias=True)\n","        (v_lin): Linear(in_features=768, out_features=768, bias=True)\n","        (out_lin): Linear(in_features=768, out_features=768, bias=True)\n","      )\n","      (10): MultiHeadAttention(\n","        (q_lin): Linear(in_features=768, out_features=768, bias=True)\n","        (k_lin): Linear(in_features=768, out_features=768, bias=True)\n","        (v_lin): Linear(in_features=768, out_features=768, bias=True)\n","        (out_lin): Linear(in_features=768, out_features=768, bias=True)\n","      )\n","      (11): MultiHeadAttention(\n","        (q_lin): Linear(in_features=768, out_features=768, bias=True)\n","        (k_lin): Linear(in_features=768, out_features=768, bias=True)\n","        (v_lin): Linear(in_features=768, out_features=768, bias=True)\n","        (out_lin): Linear(in_features=768, out_features=768, bias=True)\n","      )\n","    )\n","    (layer_norm1): ModuleList(\n","      (0): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","      (1): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","      (2): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","      (3): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","      (4): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","      (5): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","      (6): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","      (7): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","      (8): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","      (9): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","      (10): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","      (11): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","    )\n","    (ffns): ModuleList(\n","      (0): TransformerFFN(\n","        (lin1): Linear(in_features=768, out_features=3072, bias=True)\n","        (lin2): Linear(in_features=3072, out_features=768, bias=True)\n","      )\n","      (1): TransformerFFN(\n","        (lin1): Linear(in_features=768, out_features=3072, bias=True)\n","        (lin2): Linear(in_features=3072, out_features=768, bias=True)\n","      )\n","      (2): TransformerFFN(\n","        (lin1): Linear(in_features=768, out_features=3072, bias=True)\n","        (lin2): Linear(in_features=3072, out_features=768, bias=True)\n","      )\n","      (3): TransformerFFN(\n","        (lin1): Linear(in_features=768, out_features=3072, bias=True)\n","        (lin2): Linear(in_features=3072, out_features=768, bias=True)\n","      )\n","      (4): TransformerFFN(\n","        (lin1): Linear(in_features=768, out_features=3072, bias=True)\n","        (lin2): Linear(in_features=3072, out_features=768, bias=True)\n","      )\n","      (5): TransformerFFN(\n","        (lin1): Linear(in_features=768, out_features=3072, bias=True)\n","        (lin2): Linear(in_features=3072, out_features=768, bias=True)\n","      )\n","      (6): TransformerFFN(\n","        (lin1): Linear(in_features=768, out_features=3072, bias=True)\n","        (lin2): Linear(in_features=3072, out_features=768, bias=True)\n","      )\n","      (7): TransformerFFN(\n","        (lin1): Linear(in_features=768, out_features=3072, bias=True)\n","        (lin2): Linear(in_features=3072, out_features=768, bias=True)\n","      )\n","      (8): TransformerFFN(\n","        (lin1): Linear(in_features=768, out_features=3072, bias=True)\n","        (lin2): Linear(in_features=3072, out_features=768, bias=True)\n","      )\n","      (9): TransformerFFN(\n","        (lin1): Linear(in_features=768, out_features=3072, bias=True)\n","        (lin2): Linear(in_features=3072, out_features=768, bias=True)\n","      )\n","      (10): TransformerFFN(\n","        (lin1): Linear(in_features=768, out_features=3072, bias=True)\n","        (lin2): Linear(in_features=3072, out_features=768, bias=True)\n","      )\n","      (11): TransformerFFN(\n","        (lin1): Linear(in_features=768, out_features=3072, bias=True)\n","        (lin2): Linear(in_features=3072, out_features=768, bias=True)\n","      )\n","    )\n","    (layer_norm2): ModuleList(\n","      (0): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","      (1): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","      (2): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","      (3): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","      (4): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","      (5): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","      (6): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","      (7): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","      (8): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","      (9): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","      (10): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","      (11): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","    )\n","    (transformer): FlaubertModel(\n","      (position_embeddings): Embedding(512, 768)\n","      (embeddings): Embedding(68729, 768, padding_idx=2)\n","      (layer_norm_emb): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","      (attentions): ModuleList(\n","        (0): MultiHeadAttention(\n","          (q_lin): Linear(in_features=768, out_features=768, bias=True)\n","          (k_lin): Linear(in_features=768, out_features=768, bias=True)\n","          (v_lin): Linear(in_features=768, out_features=768, bias=True)\n","          (out_lin): Linear(in_features=768, out_features=768, bias=True)\n","        )\n","        (1): MultiHeadAttention(\n","          (q_lin): Linear(in_features=768, out_features=768, bias=True)\n","          (k_lin): Linear(in_features=768, out_features=768, bias=True)\n","          (v_lin): Linear(in_features=768, out_features=768, bias=True)\n","          (out_lin): Linear(in_features=768, out_features=768, bias=True)\n","        )\n","        (2): MultiHeadAttention(\n","          (q_lin): Linear(in_features=768, out_features=768, bias=True)\n","          (k_lin): Linear(in_features=768, out_features=768, bias=True)\n","          (v_lin): Linear(in_features=768, out_features=768, bias=True)\n","          (out_lin): Linear(in_features=768, out_features=768, bias=True)\n","        )\n","        (3): MultiHeadAttention(\n","          (q_lin): Linear(in_features=768, out_features=768, bias=True)\n","          (k_lin): Linear(in_features=768, out_features=768, bias=True)\n","          (v_lin): Linear(in_features=768, out_features=768, bias=True)\n","          (out_lin): Linear(in_features=768, out_features=768, bias=True)\n","        )\n","        (4): MultiHeadAttention(\n","          (q_lin): Linear(in_features=768, out_features=768, bias=True)\n","          (k_lin): Linear(in_features=768, out_features=768, bias=True)\n","          (v_lin): Linear(in_features=768, out_features=768, bias=True)\n","          (out_lin): Linear(in_features=768, out_features=768, bias=True)\n","        )\n","        (5): MultiHeadAttention(\n","          (q_lin): Linear(in_features=768, out_features=768, bias=True)\n","          (k_lin): Linear(in_features=768, out_features=768, bias=True)\n","          (v_lin): Linear(in_features=768, out_features=768, bias=True)\n","          (out_lin): Linear(in_features=768, out_features=768, bias=True)\n","        )\n","        (6): MultiHeadAttention(\n","          (q_lin): Linear(in_features=768, out_features=768, bias=True)\n","          (k_lin): Linear(in_features=768, out_features=768, bias=True)\n","          (v_lin): Linear(in_features=768, out_features=768, bias=True)\n","          (out_lin): Linear(in_features=768, out_features=768, bias=True)\n","        )\n","        (7): MultiHeadAttention(\n","          (q_lin): Linear(in_features=768, out_features=768, bias=True)\n","          (k_lin): Linear(in_features=768, out_features=768, bias=True)\n","          (v_lin): Linear(in_features=768, out_features=768, bias=True)\n","          (out_lin): Linear(in_features=768, out_features=768, bias=True)\n","        )\n","        (8): MultiHeadAttention(\n","          (q_lin): Linear(in_features=768, out_features=768, bias=True)\n","          (k_lin): Linear(in_features=768, out_features=768, bias=True)\n","          (v_lin): Linear(in_features=768, out_features=768, bias=True)\n","          (out_lin): Linear(in_features=768, out_features=768, bias=True)\n","        )\n","        (9): MultiHeadAttention(\n","          (q_lin): Linear(in_features=768, out_features=768, bias=True)\n","          (k_lin): Linear(in_features=768, out_features=768, bias=True)\n","          (v_lin): Linear(in_features=768, out_features=768, bias=True)\n","          (out_lin): Linear(in_features=768, out_features=768, bias=True)\n","        )\n","        (10): MultiHeadAttention(\n","          (q_lin): Linear(in_features=768, out_features=768, bias=True)\n","          (k_lin): Linear(in_features=768, out_features=768, bias=True)\n","          (v_lin): Linear(in_features=768, out_features=768, bias=True)\n","          (out_lin): Linear(in_features=768, out_features=768, bias=True)\n","        )\n","        (11): MultiHeadAttention(\n","          (q_lin): Linear(in_features=768, out_features=768, bias=True)\n","          (k_lin): Linear(in_features=768, out_features=768, bias=True)\n","          (v_lin): Linear(in_features=768, out_features=768, bias=True)\n","          (out_lin): Linear(in_features=768, out_features=768, bias=True)\n","        )\n","      )\n","      (layer_norm1): ModuleList(\n","        (0): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","        (1): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","        (2): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","        (3): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","        (4): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","        (5): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","        (6): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","        (7): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","        (8): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","        (9): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","        (10): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","        (11): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","      )\n","      (ffns): ModuleList(\n","        (0): TransformerFFN(\n","          (lin1): Linear(in_features=768, out_features=3072, bias=True)\n","          (lin2): Linear(in_features=3072, out_features=768, bias=True)\n","        )\n","        (1): TransformerFFN(\n","          (lin1): Linear(in_features=768, out_features=3072, bias=True)\n","          (lin2): Linear(in_features=3072, out_features=768, bias=True)\n","        )\n","        (2): TransformerFFN(\n","          (lin1): Linear(in_features=768, out_features=3072, bias=True)\n","          (lin2): Linear(in_features=3072, out_features=768, bias=True)\n","        )\n","        (3): TransformerFFN(\n","          (lin1): Linear(in_features=768, out_features=3072, bias=True)\n","          (lin2): Linear(in_features=3072, out_features=768, bias=True)\n","        )\n","        (4): TransformerFFN(\n","          (lin1): Linear(in_features=768, out_features=3072, bias=True)\n","          (lin2): Linear(in_features=3072, out_features=768, bias=True)\n","        )\n","        (5): TransformerFFN(\n","          (lin1): Linear(in_features=768, out_features=3072, bias=True)\n","          (lin2): Linear(in_features=3072, out_features=768, bias=True)\n","        )\n","        (6): TransformerFFN(\n","          (lin1): Linear(in_features=768, out_features=3072, bias=True)\n","          (lin2): Linear(in_features=3072, out_features=768, bias=True)\n","        )\n","        (7): TransformerFFN(\n","          (lin1): Linear(in_features=768, out_features=3072, bias=True)\n","          (lin2): Linear(in_features=3072, out_features=768, bias=True)\n","        )\n","        (8): TransformerFFN(\n","          (lin1): Linear(in_features=768, out_features=3072, bias=True)\n","          (lin2): Linear(in_features=3072, out_features=768, bias=True)\n","        )\n","        (9): TransformerFFN(\n","          (lin1): Linear(in_features=768, out_features=3072, bias=True)\n","          (lin2): Linear(in_features=3072, out_features=768, bias=True)\n","        )\n","        (10): TransformerFFN(\n","          (lin1): Linear(in_features=768, out_features=3072, bias=True)\n","          (lin2): Linear(in_features=3072, out_features=768, bias=True)\n","        )\n","        (11): TransformerFFN(\n","          (lin1): Linear(in_features=768, out_features=3072, bias=True)\n","          (lin2): Linear(in_features=3072, out_features=768, bias=True)\n","        )\n","      )\n","      (layer_norm2): ModuleList(\n","        (0): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","        (1): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","        (2): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","        (3): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","        (4): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","        (5): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","        (6): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","        (7): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","        (8): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","        (9): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","        (10): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","        (11): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","      )\n","    )\n","    (sequence_summary): SequenceSummary(\n","      (summary): Linear(in_features=768, out_features=27, bias=True)\n","      (activation): Identity()\n","      (first_dropout): Dropout(p=0.1, inplace=False)\n","      (last_dropout): Identity()\n","    )\n","    (dropout): Dropout(p=0.1, inplace=False)\n","    (classifier): Identity()\n","  )\n","  (flau_model_desc): vec_output_FlaubertForSequenceClassification(\n","    (position_embeddings): Embedding(512, 768)\n","    (embeddings): Embedding(68729, 768, padding_idx=2)\n","    (layer_norm_emb): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","    (attentions): ModuleList(\n","      (0): MultiHeadAttention(\n","        (q_lin): Linear(in_features=768, out_features=768, bias=True)\n","        (k_lin): Linear(in_features=768, out_features=768, bias=True)\n","        (v_lin): Linear(in_features=768, out_features=768, bias=True)\n","        (out_lin): Linear(in_features=768, out_features=768, bias=True)\n","      )\n","      (1): MultiHeadAttention(\n","        (q_lin): Linear(in_features=768, out_features=768, bias=True)\n","        (k_lin): Linear(in_features=768, out_features=768, bias=True)\n","        (v_lin): Linear(in_features=768, out_features=768, bias=True)\n","        (out_lin): Linear(in_features=768, out_features=768, bias=True)\n","      )\n","      (2): MultiHeadAttention(\n","        (q_lin): Linear(in_features=768, out_features=768, bias=True)\n","        (k_lin): Linear(in_features=768, out_features=768, bias=True)\n","        (v_lin): Linear(in_features=768, out_features=768, bias=True)\n","        (out_lin): Linear(in_features=768, out_features=768, bias=True)\n","      )\n","      (3): MultiHeadAttention(\n","        (q_lin): Linear(in_features=768, out_features=768, bias=True)\n","        (k_lin): Linear(in_features=768, out_features=768, bias=True)\n","        (v_lin): Linear(in_features=768, out_features=768, bias=True)\n","        (out_lin): Linear(in_features=768, out_features=768, bias=True)\n","      )\n","      (4): MultiHeadAttention(\n","        (q_lin): Linear(in_features=768, out_features=768, bias=True)\n","        (k_lin): Linear(in_features=768, out_features=768, bias=True)\n","        (v_lin): Linear(in_features=768, out_features=768, bias=True)\n","        (out_lin): Linear(in_features=768, out_features=768, bias=True)\n","      )\n","      (5): MultiHeadAttention(\n","        (q_lin): Linear(in_features=768, out_features=768, bias=True)\n","        (k_lin): Linear(in_features=768, out_features=768, bias=True)\n","        (v_lin): Linear(in_features=768, out_features=768, bias=True)\n","        (out_lin): Linear(in_features=768, out_features=768, bias=True)\n","      )\n","      (6): MultiHeadAttention(\n","        (q_lin): Linear(in_features=768, out_features=768, bias=True)\n","        (k_lin): Linear(in_features=768, out_features=768, bias=True)\n","        (v_lin): Linear(in_features=768, out_features=768, bias=True)\n","        (out_lin): Linear(in_features=768, out_features=768, bias=True)\n","      )\n","      (7): MultiHeadAttention(\n","        (q_lin): Linear(in_features=768, out_features=768, bias=True)\n","        (k_lin): Linear(in_features=768, out_features=768, bias=True)\n","        (v_lin): Linear(in_features=768, out_features=768, bias=True)\n","        (out_lin): Linear(in_features=768, out_features=768, bias=True)\n","      )\n","      (8): MultiHeadAttention(\n","        (q_lin): Linear(in_features=768, out_features=768, bias=True)\n","        (k_lin): Linear(in_features=768, out_features=768, bias=True)\n","        (v_lin): Linear(in_features=768, out_features=768, bias=True)\n","        (out_lin): Linear(in_features=768, out_features=768, bias=True)\n","      )\n","      (9): MultiHeadAttention(\n","        (q_lin): Linear(in_features=768, out_features=768, bias=True)\n","        (k_lin): Linear(in_features=768, out_features=768, bias=True)\n","        (v_lin): Linear(in_features=768, out_features=768, bias=True)\n","        (out_lin): Linear(in_features=768, out_features=768, bias=True)\n","      )\n","      (10): MultiHeadAttention(\n","        (q_lin): Linear(in_features=768, out_features=768, bias=True)\n","        (k_lin): Linear(in_features=768, out_features=768, bias=True)\n","        (v_lin): Linear(in_features=768, out_features=768, bias=True)\n","        (out_lin): Linear(in_features=768, out_features=768, bias=True)\n","      )\n","      (11): MultiHeadAttention(\n","        (q_lin): Linear(in_features=768, out_features=768, bias=True)\n","        (k_lin): Linear(in_features=768, out_features=768, bias=True)\n","        (v_lin): Linear(in_features=768, out_features=768, bias=True)\n","        (out_lin): Linear(in_features=768, out_features=768, bias=True)\n","      )\n","    )\n","    (layer_norm1): ModuleList(\n","      (0): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","      (1): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","      (2): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","      (3): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","      (4): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","      (5): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","      (6): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","      (7): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","      (8): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","      (9): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","      (10): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","      (11): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","    )\n","    (ffns): ModuleList(\n","      (0): TransformerFFN(\n","        (lin1): Linear(in_features=768, out_features=3072, bias=True)\n","        (lin2): Linear(in_features=3072, out_features=768, bias=True)\n","      )\n","      (1): TransformerFFN(\n","        (lin1): Linear(in_features=768, out_features=3072, bias=True)\n","        (lin2): Linear(in_features=3072, out_features=768, bias=True)\n","      )\n","      (2): TransformerFFN(\n","        (lin1): Linear(in_features=768, out_features=3072, bias=True)\n","        (lin2): Linear(in_features=3072, out_features=768, bias=True)\n","      )\n","      (3): TransformerFFN(\n","        (lin1): Linear(in_features=768, out_features=3072, bias=True)\n","        (lin2): Linear(in_features=3072, out_features=768, bias=True)\n","      )\n","      (4): TransformerFFN(\n","        (lin1): Linear(in_features=768, out_features=3072, bias=True)\n","        (lin2): Linear(in_features=3072, out_features=768, bias=True)\n","      )\n","      (5): TransformerFFN(\n","        (lin1): Linear(in_features=768, out_features=3072, bias=True)\n","        (lin2): Linear(in_features=3072, out_features=768, bias=True)\n","      )\n","      (6): TransformerFFN(\n","        (lin1): Linear(in_features=768, out_features=3072, bias=True)\n","        (lin2): Linear(in_features=3072, out_features=768, bias=True)\n","      )\n","      (7): TransformerFFN(\n","        (lin1): Linear(in_features=768, out_features=3072, bias=True)\n","        (lin2): Linear(in_features=3072, out_features=768, bias=True)\n","      )\n","      (8): TransformerFFN(\n","        (lin1): Linear(in_features=768, out_features=3072, bias=True)\n","        (lin2): Linear(in_features=3072, out_features=768, bias=True)\n","      )\n","      (9): TransformerFFN(\n","        (lin1): Linear(in_features=768, out_features=3072, bias=True)\n","        (lin2): Linear(in_features=3072, out_features=768, bias=True)\n","      )\n","      (10): TransformerFFN(\n","        (lin1): Linear(in_features=768, out_features=3072, bias=True)\n","        (lin2): Linear(in_features=3072, out_features=768, bias=True)\n","      )\n","      (11): TransformerFFN(\n","        (lin1): Linear(in_features=768, out_features=3072, bias=True)\n","        (lin2): Linear(in_features=3072, out_features=768, bias=True)\n","      )\n","    )\n","    (layer_norm2): ModuleList(\n","      (0): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","      (1): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","      (2): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","      (3): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","      (4): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","      (5): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","      (6): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","      (7): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","      (8): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","      (9): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","      (10): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","      (11): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","    )\n","    (transformer): FlaubertModel(\n","      (position_embeddings): Embedding(512, 768)\n","      (embeddings): Embedding(68729, 768, padding_idx=2)\n","      (layer_norm_emb): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","      (attentions): ModuleList(\n","        (0): MultiHeadAttention(\n","          (q_lin): Linear(in_features=768, out_features=768, bias=True)\n","          (k_lin): Linear(in_features=768, out_features=768, bias=True)\n","          (v_lin): Linear(in_features=768, out_features=768, bias=True)\n","          (out_lin): Linear(in_features=768, out_features=768, bias=True)\n","        )\n","        (1): MultiHeadAttention(\n","          (q_lin): Linear(in_features=768, out_features=768, bias=True)\n","          (k_lin): Linear(in_features=768, out_features=768, bias=True)\n","          (v_lin): Linear(in_features=768, out_features=768, bias=True)\n","          (out_lin): Linear(in_features=768, out_features=768, bias=True)\n","        )\n","        (2): MultiHeadAttention(\n","          (q_lin): Linear(in_features=768, out_features=768, bias=True)\n","          (k_lin): Linear(in_features=768, out_features=768, bias=True)\n","          (v_lin): Linear(in_features=768, out_features=768, bias=True)\n","          (out_lin): Linear(in_features=768, out_features=768, bias=True)\n","        )\n","        (3): MultiHeadAttention(\n","          (q_lin): Linear(in_features=768, out_features=768, bias=True)\n","          (k_lin): Linear(in_features=768, out_features=768, bias=True)\n","          (v_lin): Linear(in_features=768, out_features=768, bias=True)\n","          (out_lin): Linear(in_features=768, out_features=768, bias=True)\n","        )\n","        (4): MultiHeadAttention(\n","          (q_lin): Linear(in_features=768, out_features=768, bias=True)\n","          (k_lin): Linear(in_features=768, out_features=768, bias=True)\n","          (v_lin): Linear(in_features=768, out_features=768, bias=True)\n","          (out_lin): Linear(in_features=768, out_features=768, bias=True)\n","        )\n","        (5): MultiHeadAttention(\n","          (q_lin): Linear(in_features=768, out_features=768, bias=True)\n","          (k_lin): Linear(in_features=768, out_features=768, bias=True)\n","          (v_lin): Linear(in_features=768, out_features=768, bias=True)\n","          (out_lin): Linear(in_features=768, out_features=768, bias=True)\n","        )\n","        (6): MultiHeadAttention(\n","          (q_lin): Linear(in_features=768, out_features=768, bias=True)\n","          (k_lin): Linear(in_features=768, out_features=768, bias=True)\n","          (v_lin): Linear(in_features=768, out_features=768, bias=True)\n","          (out_lin): Linear(in_features=768, out_features=768, bias=True)\n","        )\n","        (7): MultiHeadAttention(\n","          (q_lin): Linear(in_features=768, out_features=768, bias=True)\n","          (k_lin): Linear(in_features=768, out_features=768, bias=True)\n","          (v_lin): Linear(in_features=768, out_features=768, bias=True)\n","          (out_lin): Linear(in_features=768, out_features=768, bias=True)\n","        )\n","        (8): MultiHeadAttention(\n","          (q_lin): Linear(in_features=768, out_features=768, bias=True)\n","          (k_lin): Linear(in_features=768, out_features=768, bias=True)\n","          (v_lin): Linear(in_features=768, out_features=768, bias=True)\n","          (out_lin): Linear(in_features=768, out_features=768, bias=True)\n","        )\n","        (9): MultiHeadAttention(\n","          (q_lin): Linear(in_features=768, out_features=768, bias=True)\n","          (k_lin): Linear(in_features=768, out_features=768, bias=True)\n","          (v_lin): Linear(in_features=768, out_features=768, bias=True)\n","          (out_lin): Linear(in_features=768, out_features=768, bias=True)\n","        )\n","        (10): MultiHeadAttention(\n","          (q_lin): Linear(in_features=768, out_features=768, bias=True)\n","          (k_lin): Linear(in_features=768, out_features=768, bias=True)\n","          (v_lin): Linear(in_features=768, out_features=768, bias=True)\n","          (out_lin): Linear(in_features=768, out_features=768, bias=True)\n","        )\n","        (11): MultiHeadAttention(\n","          (q_lin): Linear(in_features=768, out_features=768, bias=True)\n","          (k_lin): Linear(in_features=768, out_features=768, bias=True)\n","          (v_lin): Linear(in_features=768, out_features=768, bias=True)\n","          (out_lin): Linear(in_features=768, out_features=768, bias=True)\n","        )\n","      )\n","      (layer_norm1): ModuleList(\n","        (0): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","        (1): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","        (2): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","        (3): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","        (4): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","        (5): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","        (6): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","        (7): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","        (8): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","        (9): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","        (10): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","        (11): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","      )\n","      (ffns): ModuleList(\n","        (0): TransformerFFN(\n","          (lin1): Linear(in_features=768, out_features=3072, bias=True)\n","          (lin2): Linear(in_features=3072, out_features=768, bias=True)\n","        )\n","        (1): TransformerFFN(\n","          (lin1): Linear(in_features=768, out_features=3072, bias=True)\n","          (lin2): Linear(in_features=3072, out_features=768, bias=True)\n","        )\n","        (2): TransformerFFN(\n","          (lin1): Linear(in_features=768, out_features=3072, bias=True)\n","          (lin2): Linear(in_features=3072, out_features=768, bias=True)\n","        )\n","        (3): TransformerFFN(\n","          (lin1): Linear(in_features=768, out_features=3072, bias=True)\n","          (lin2): Linear(in_features=3072, out_features=768, bias=True)\n","        )\n","        (4): TransformerFFN(\n","          (lin1): Linear(in_features=768, out_features=3072, bias=True)\n","          (lin2): Linear(in_features=3072, out_features=768, bias=True)\n","        )\n","        (5): TransformerFFN(\n","          (lin1): Linear(in_features=768, out_features=3072, bias=True)\n","          (lin2): Linear(in_features=3072, out_features=768, bias=True)\n","        )\n","        (6): TransformerFFN(\n","          (lin1): Linear(in_features=768, out_features=3072, bias=True)\n","          (lin2): Linear(in_features=3072, out_features=768, bias=True)\n","        )\n","        (7): TransformerFFN(\n","          (lin1): Linear(in_features=768, out_features=3072, bias=True)\n","          (lin2): Linear(in_features=3072, out_features=768, bias=True)\n","        )\n","        (8): TransformerFFN(\n","          (lin1): Linear(in_features=768, out_features=3072, bias=True)\n","          (lin2): Linear(in_features=3072, out_features=768, bias=True)\n","        )\n","        (9): TransformerFFN(\n","          (lin1): Linear(in_features=768, out_features=3072, bias=True)\n","          (lin2): Linear(in_features=3072, out_features=768, bias=True)\n","        )\n","        (10): TransformerFFN(\n","          (lin1): Linear(in_features=768, out_features=3072, bias=True)\n","          (lin2): Linear(in_features=3072, out_features=768, bias=True)\n","        )\n","        (11): TransformerFFN(\n","          (lin1): Linear(in_features=768, out_features=3072, bias=True)\n","          (lin2): Linear(in_features=3072, out_features=768, bias=True)\n","        )\n","      )\n","      (layer_norm2): ModuleList(\n","        (0): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","        (1): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","        (2): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","        (3): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","        (4): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","        (5): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","        (6): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","        (7): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","        (8): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","        (9): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","        (10): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","        (11): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","      )\n","    )\n","    (sequence_summary): SequenceSummary(\n","      (summary): Linear(in_features=768, out_features=27, bias=True)\n","      (activation): Identity()\n","      (first_dropout): Dropout(p=0.1, inplace=False)\n","      (last_dropout): Identity()\n","    )\n","    (dropout): Dropout(p=0.1, inplace=False)\n","    (classifier): Identity()\n","  )\n","  (reduce_dim): Conv1d(2048, 768, kernel_size=(1,), stride=(1,))\n","  (reduce_dim2): Conv1d(768, 1, kernel_size=(1,), stride=(1,))\n","  (out): Linear(in_features=2304, out_features=27, bias=True)\n",")"]},"metadata":{"tags":[]},"execution_count":82}]},{"cell_type":"markdown","metadata":{"id":"wbah-djKPyRB"},"source":["# Fuse Input Data"]},{"cell_type":"code","metadata":{"id":"ZZBQFQnFSn6L","executionInfo":{"status":"ok","timestamp":1619174181335,"user_tz":-120,"elapsed":403677,"user":{"displayName":"Sara Fattouh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjGrdvZo2x_8K8abskRubIgMK3SdtS4cG-aQE01nA=s64","userId":"09360594903764573322"}}},"source":["train_dataset = FusionDataset (train_df, tr_inputs_cam, tr_masks_cam, tr_inputs_flau, tr_masks_flau,\n","                            transform = image_transforms['train'])\n","\n","val_dataset = FusionDataset (val_df, val_inputs_cam, val_masks_cam, val_inputs_flau, val_masks_flau,\n","                          transform = image_transforms['valid'])\n","\n","\n","\n","test_dataset = FusionDataset (test_df, input_ids_test_cam, attention_masks_test_cam, input_ids_test_flau, attention_masks_test_flau\n","                           , transform = image_transforms['test'])"],"execution_count":83,"outputs":[]},{"cell_type":"code","metadata":{"id":"yPY9mTkrs2Jl","executionInfo":{"status":"ok","timestamp":1619174181335,"user_tz":-120,"elapsed":403671,"user":{"displayName":"Sara Fattouh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjGrdvZo2x_8K8abskRubIgMK3SdtS4cG-aQE01nA=s64","userId":"09360594903764573322"}}},"source":["# print(len(train_df), len(tr_inputs_cam), len(tr_inputs_flau))"],"execution_count":84,"outputs":[]},{"cell_type":"code","metadata":{"id":"VXWSFsjKVnXn","executionInfo":{"status":"ok","timestamp":1619174181336,"user_tz":-120,"elapsed":403667,"user":{"displayName":"Sara Fattouh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjGrdvZo2x_8K8abskRubIgMK3SdtS4cG-aQE01nA=s64","userId":"09360594903764573322"}}},"source":["# print(len(train_dataset))"],"execution_count":85,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Uh2PccgCXJeI"},"source":["# Data Loaders\n","\n","We need to use the DataLoaders to create iterable objects for us to work with. We tell it which datasets we want to use, give it a batch size, and shuffle the data"]},{"cell_type":"code","metadata":{"id":"nfFhYJYpSr05","executionInfo":{"status":"ok","timestamp":1619174181336,"user_tz":-120,"elapsed":403662,"user":{"displayName":"Sara Fattouh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjGrdvZo2x_8K8abskRubIgMK3SdtS4cG-aQE01nA=s64","userId":"09360594903764573322"}}},"source":["batch_size = 32  #increase batch size to reduce the noise \n","\n","train_dataloader = DataLoader(train_dataset,batch_size=batch_size,shuffle=True)\n","\n","validation_dataloader = DataLoader(val_dataset,batch_size=batch_size,shuffle=False)\n"," \n","test_dataloader = DataLoader(test_dataset,batch_size=batch_size,shuffle=False)"],"execution_count":86,"outputs":[]},{"cell_type":"code","metadata":{"id":"Y-gP7nVnSwAt","executionInfo":{"status":"ok","timestamp":1619174181336,"user_tz":-120,"elapsed":403657,"user":{"displayName":"Sara Fattouh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjGrdvZo2x_8K8abskRubIgMK3SdtS4cG-aQE01nA=s64","userId":"09360594903764573322"}}},"source":["optimizer = AdamW( model.parameters(),\n","                  lr = 2e-4, # args.learning_rate - default is 5e-5, our notebook had 2e-5\n","                  eps = 1e-8, # args.adam_epsilon  - default is 1e-8.\n","                  weight_decay= 0.001\n","                )"],"execution_count":87,"outputs":[]},{"cell_type":"code","metadata":{"id":"PgoWU2g6S1CZ","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1619174181337,"user_tz":-120,"elapsed":403652,"user":{"displayName":"Sara Fattouh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjGrdvZo2x_8K8abskRubIgMK3SdtS4cG-aQE01nA=s64","userId":"09360594903764573322"}},"outputId":"f5d720ee-179d-4d0f-d201-346ba34cbfbe"},"source":["def count_parameters(model):\n","    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n","count_parameters(model)"],"execution_count":88,"outputs":[{"output_type":"execute_result","data":{"text/plain":["1636636"]},"metadata":{"tags":[]},"execution_count":88}]},{"cell_type":"code","metadata":{"id":"0CRNzbwCS6vU","executionInfo":{"status":"ok","timestamp":1619174181337,"user_tz":-120,"elapsed":403647,"user":{"displayName":"Sara Fattouh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjGrdvZo2x_8K8abskRubIgMK3SdtS4cG-aQE01nA=s64","userId":"09360594903764573322"}}},"source":["from transformers import get_linear_schedule_with_warmup\n","\n","# Number of training epochs. The BERT authors recommend between 2 and 4. \n","# We chose to run for 4, but we'll see later that this may be over-fitting the\n","# training data.\n","epochs = 6\n","\n","# Total number of training steps is [number of batches] x [number of epochs]. \n","# (Note that this is not the same as the number of training samples).\n","total_steps = len(train_dataloader) * epochs\n","\n","# Create the learning rate scheduler.\n","scheduler = get_linear_schedule_with_warmup(optimizer, \n","                                            num_warmup_steps = 0, # Default value in run_glue.py\n","                                            num_training_steps = total_steps)"],"execution_count":89,"outputs":[]},{"cell_type":"code","metadata":{"id":"en6CE-_8S-Vw","executionInfo":{"status":"ok","timestamp":1619174181337,"user_tz":-120,"elapsed":403640,"user":{"displayName":"Sara Fattouh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjGrdvZo2x_8K8abskRubIgMK3SdtS4cG-aQE01nA=s64","userId":"09360594903764573322"}}},"source":["import torch.nn as nn\n","loss_criterion = nn.CrossEntropyLoss()"],"execution_count":90,"outputs":[]},{"cell_type":"code","metadata":{"id":"MLg8d2UmTBLp","executionInfo":{"status":"ok","timestamp":1619174181338,"user_tz":-120,"elapsed":403637,"user":{"displayName":"Sara Fattouh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjGrdvZo2x_8K8abskRubIgMK3SdtS4cG-aQE01nA=s64","userId":"09360594903764573322"}}},"source":["def flat_accuracy(preds, labels):\n","    pred_flat = np.argmax(preds, axis=1).flatten()\n","    labels_flat = labels.flatten()\n","    return np.sum(pred_flat == labels_flat) / len(labels_flat)"],"execution_count":91,"outputs":[]},{"cell_type":"code","metadata":{"id":"kY7xbQXqTHUF","colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"status":"ok","timestamp":1619192218088,"user_tz":-120,"elapsed":18440381,"user":{"displayName":"Sara Fattouh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjGrdvZo2x_8K8abskRubIgMK3SdtS4cG-aQE01nA=s64","userId":"09360594903764573322"}},"outputId":"714d432a-040f-4e18-ccfb-1e06faa58b89"},"source":["from sklearn.metrics import f1_score\n","\n","#seed_val = 42\n","seed_val = 42\n","\n","random.seed(seed_val)\n","np.random.seed(seed_val)\n","torch.manual_seed(seed_val)\n","torch.cuda.manual_seed_all(seed_val)\n","\n","# We'll store a number of quantities such as training and validation loss, \n","# validation accuracy, and timings.\n","training_stats = []\n","train_loss_values = []\n","\n","val_loss_values = []\n","logits_values =[]\n","\n","############\n","\n","total_train_accuracy = 0\n","avg_train_accuracy = 0\n","\n","train_accuracy_values = []\n","val_accuracy_values = []\n","\n","##########\n","\n","# Measure the total training time for the whole run.\n","total_t0 = time.time()\n","\n","\n","\n","# For each epoch...\n","for epoch_i in range(0, epochs):\n","    \n","    # ========================================\n","    #               Training\n","    # ========================================\n","    \n","    # Perform one full pass over the training set.\n","\n","    print(\"\")\n","    print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n","    print('Training...')\n","    \n","    #tr and val\n","#     vec_output_tr = []\n","#     vec_output_val =[]\n","\n","    # Measure how long the training epoch takes.\n","    t0 = time.time()\n","\n","    # Reset the total loss for this epoch.\n","    total_train_loss = 0\n","    total_train_accuracy = 0\n","    predictions=[]\n","    true_labels=[]\n","\n","    # Put the model into training mode. Don't be mislead--the call to \n","    # `train` just changes the *mode*, it doesn't *perform* the training.\n","    # `dropout` and `batchnorm` layers behave differently during training\n","    # vs. test (source: https://stackoverflow.com/questions/51433378/what-does-model-train-do-in-pytorch)\n","\n","    model.to(device)\n","    best_f1 = 0\n","    model.train()\n","\n","    # For each batch of training data...\n","    for step, batch in (enumerate(train_dataloader)):\n","        \n","        # Unpack this training batch from our dataloader. \n","        #   \n","        # As we unpack the batch, we'll also copy each tensor to the GPU using the \n","        # `to` method.\n","        #\n","        # `batch` contains three pytorch tensors:\n","        #   [0]: input ids \n","        #   [1]: attention masks\n","        #   [2]: labels \n","#         return img,input_id_cam,input_mask_cam,input_id_flau,input_mask_flau\n","\n","        b_img               = batch[0].to(device)\n","        b_input_id_cam      = batch[1].to(device)\n","        b_input_mask_cam    = batch[2].to(device)\n","        b_input_id_flau     = batch[3].to(device)\n","        b_input_mask_flau   = batch[4].to(device)\n","        b_labels            = batch[5].to(device)\n","        \n","        \n","        model.zero_grad()    #set the gradients to zero before starting to do backpropragation because PyTorch accumulates \n","                            # the gradients on subsequent backward passes\n","\n","        \n","        logits = model(b_img, b_input_id_cam , b_input_mask_cam, b_input_id_flau, b_input_mask_flau)  # 27\n","                            \n","        #Defining the loss\n","        loss = loss_criterion(logits, b_labels)\n","        \n","        #saving the features_tr\n","#         vec = vec.detach().cpu().numpy()\n","#         vec_output_tr.extend(vec)\n","        \n","        # Accumulate the training loss over all of the batches so that we can\n","        # calculate the average loss at the end. `loss` is a Tensor containing a\n","        # single value; the `.item()` function just returns the Python value \n","        # from the tensor.\n","\n","        total_train_loss += loss.item()\n","#-------------------------------------------------------\n","\n","        # Move logits and labels to CPU\n","        logits = logits.detach().cpu().numpy()\n","\n","        # Move logits and labels to CPU\n","        predicted_labels=np.argmax(logits,axis=1)\n","        predictions.extend(predicted_labels)\n","        label_ids = b_labels.to('cpu').numpy()\n","        true_labels.extend(label_ids)\n","\n","        total_train_accuracy += flat_accuracy(logits, label_ids)\n","\n","\n","#-------------------------------------------------------\n","\n","        # Perform a backward pass to calculate the gradients.\n","        loss.backward()\n","\n","        # Clip the norm of the gradients to 1.0.\n","        # This is to help prevent the \"exploding gradients\" problem.\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n","\n","        # Update parameters and take a step using the computed gradient.\n","        # The optimizer dictates the \"update rule\"--how the parameters are\n","        # modified based on their gradients, the learning rate, etc.\n","        optimizer.step()\n","\n","        # Update the learning rate.\n","        scheduler.step()\n","\n","\n","  # ------------------------------------------------------\n","        \n","    avg_train_accuracy = total_train_accuracy / len(train_dataloader)\n","\n","    print(\"\")\n","    print(\"Training Accuracy: {0:.2f}\".format(avg_train_accuracy))\n","    train_accuracy_values.append(avg_train_accuracy)\n","\n","    ######################################################################################\n","\n","    # Calculate the average loss over all of the batches.\n","    avg_train_loss = total_train_loss / len(train_dataloader)  \n","    train_loss_values.append(avg_train_loss)  #-- move 2 lines up (newly added code block)\n","             \n","    \n","    # Measure how long this epoch took.\n","    training_time = format_time(time.time() - t0)\n","\n","    \n","    print(\"  Average training loss: {0:.2f} \".format(avg_train_loss))\n","    print(\"  Training epcoh took: {:} \".format(training_time))\n","        \n","    # ========================================\n","    #               Validation\n","    # ========================================\n","    # After the completion of each training epoch, measure our performance on\n","    # our validation set.\n","\n","    print(\"\")\n","    print(\"Running Validation...\")\n","\n","    t0 = time.time()\n","\n","    # Put the model in evaluation mode--the dropout layers behave differently\n","    # during evaluation.\n","    model.eval()\n","\n","    # Tracking variables \n","    total_eval_accuracy = 0\n","    total_eval_loss = 0\n","    nb_eval_steps = 0\n","    predictions=[]\n","    true_labels=[]\n","    \n","\n","    # Evaluate data for one epoch\n","    for batch in (validation_dataloader):\n","        \n","        # Unpack this training batch from our dataloader. \n","        #\n","        # As we unpack the batch, we'll also copy each tensor to the GPU using \n","        # the `to` method.\n","        #\n","        # `batch` contains three pytorch tensors:\n","        #   [0]: input ids \n","        #   [1]: attention masks\n","        #   [2]: labels \n","        \n","        b_img = batch[0].to(device)\n","\n","        b_input_id_cam = batch[1].to(device)\n","        b_input_mask_cam = batch[2].to(device)\n","        b_input_id_flau = batch[3].to(device)\n","        b_input_mask_flau = batch[4].to(device)\n","\n","        b_labels = batch[5].to(device)\n","        \n","        \n","        # Tell pytorch not to bother with constructing the compute graph during\n","        # the forward pass, since this is only needed for backprop (training).\n","        with torch.no_grad():       \n","        \n","\n","            # Forward pass, calculate logit predictions.\n","            # token_type_ids is the same as the \"segment ids\", which \n","            # differentiates sentence 1 and 2 in 2-sentence tasks.\n","            # The documentation for this `model` function is here: \n","            # https://huggingface.co/transformers/v2.2.0/model_doc/bert.html#transformers.BertForSequenceClassification\n","            # Get the \"logits\" output by the model. The \"logits\" are the output\n","            # values prior to applying an activation function like the softmax.\n","            logits = model(b_img,b_input_id_cam ,b_input_mask_cam,b_input_id_flau,b_input_mask_flau)\n","            \n","        #new\n","        \n","        #defining the val loss\n","        loss = loss_criterion(logits, b_labels)\n","        \n","        # Accumulate the validation loss.\n","        total_eval_loss += loss.item()\n","\n","        # Move logits and labels to CPU\n","        logits = logits.detach().cpu().numpy()\n","\n","        # Move logits and labels to CPU\n","        predicted_labels=np.argmax(logits,axis=1)\n","        predictions.extend(predicted_labels)\n","        label_ids = b_labels.to('cpu').numpy()\n","        true_labels.extend(label_ids)\n","\n","        ##########################################################################\n","\n","        logits_values.append(predicted_labels)\n","\n","        ##########################################################################\n","\n","        #saving the features_tr\n","#         vec = vec.detach().cpu().numpy()\n","#         vec_output_val.extend(vec)\n","        \n","\n","        # Calculate the accuracy for this batch of test sentences, and\n","        # accumulate it over all batches.\n","        total_eval_accuracy += flat_accuracy(logits, label_ids)\n","        \n","\n","    # Report the final accuracy for this validation run.\n","    avg_val_accuracy = total_eval_accuracy / len(validation_dataloader)\n","\n","#--------------------------------\n","    val_accuracy_values.append(avg_val_accuracy)\n","#--------------------------------\n","    print(\"  Accuracy: {0:.2f}\".format(avg_val_accuracy))\n","\n","    # Calculate the average loss over all of the batches.\n","    avg_val_loss = total_eval_loss / len(validation_dataloader)\n","#-----------------------------\n","    val_loss_values.append(avg_val_loss)\n","\n","    \n","    \n","    # Measure how long the validation run took.\n","    validation_time = format_time(time.time() - t0)\n","    \n","    print(\"  Validation Loss: {0:.2f}\".format(avg_val_loss))\n","    print(\"  Validation took: {:}\".format(validation_time))\n","    print(\"  Validation F1-Score: {}\".format(f1_score(true_labels,predictions,average='macro')))\n","    curr_f1=f1_score(true_labels,predictions,average='macro')\n","    if curr_f1 > best_f1:\n","        best_f1=curr_f1\n","        torch.save(model.state_dict(), '/content/drive/My Drive/Rakuten/models/Final_Hirarical_model.pt')\n","#         np.save('best_vec_train_model_train.npy',vec_output_tr)\n","#         np.save('best_vec_val.npy',vec_output_val)\n","        \n","    # Record all statistics from this epoch.\n","#     training_stats.append(\n","#         {\n","#             'epoch': epoch_i + 1,\n","#             'Training Loss': avg_train_loss,\n","#             'Valid. Loss': avg_val_loss,\n","#             'Valid. Accur.': avg_val_accuracy,\n","#             'Training Time': training_time,\n","#             'Validation Time': validation_time\n","#         }\n","#     )\n","\n","print(\"\")\n","print(\"Training complete!\")\n","\n","print(\"Total training took {:} (h:mm:ss)\".format(format_time(time.time()-total_t0)))\n","\n","print()\n","\n","plt.plot(np.array(train_loss_values), 'r', label='train loss')\n","plt.plot(np.array(val_loss_values), 'b', label='val loss'  )\n","plt.legend()\n","plt.title('Loss Curve')\n","plt.show()\n","\n","print()\n","\n","plt.plot(np.array(train_accuracy_values), 'r', label='train accuracy')\n","plt.plot(np.array(val_accuracy_values), 'b', label='val accuracy'  )\n","plt.legend()\n","plt.title('Train Curve')\n","plt.show()\n","\n","#print(logits_values)\n"],"execution_count":92,"outputs":[{"output_type":"stream","text":["\n","======== Epoch 1 / 6 ========\n","Training...\n","\n","Training Accuracy: 0.88\n","  Average training loss: 0.43 \n","  Training epcoh took: 0:38:35 \n","\n","Running Validation...\n","  Accuracy: 0.90\n","  Validation Loss: 0.36\n","  Validation took: 0:10:29\n","Validation F1-Score: 0.8814926401645289\n","\n","======== Epoch 2 / 6 ========\n","Training...\n","\n","Training Accuracy: 0.90\n","  Average training loss: 0.34 \n","  Training epcoh took: 0:39:13 \n","\n","Running Validation...\n","  Accuracy: 0.90\n","  Validation Loss: 0.34\n","  Validation took: 0:10:27\n","Validation F1-Score: 0.8878940373922387\n","\n","======== Epoch 3 / 6 ========\n","Training...\n","\n","Training Accuracy: 0.90\n","  Average training loss: 0.32 \n","  Training epcoh took: 0:39:15 \n","\n","Running Validation...\n","  Accuracy: 0.90\n","  Validation Loss: 0.36\n","  Validation took: 0:10:29\n","Validation F1-Score: 0.8896125711353403\n","\n","======== Epoch 4 / 6 ========\n","Training...\n","\n","Training Accuracy: 0.91\n","  Average training loss: 0.31 \n","  Training epcoh took: 0:39:04 \n","\n","Running Validation...\n","  Accuracy: 0.91\n","  Validation Loss: 0.32\n","  Validation took: 0:10:27\n","Validation F1-Score: 0.896028301806234\n","\n","======== Epoch 5 / 6 ========\n","Training...\n","\n","Training Accuracy: 0.91\n","  Average training loss: 0.29 \n","  Training epcoh took: 0:38:44 \n","\n","Running Validation...\n","  Accuracy: 0.91\n","  Validation Loss: 0.32\n","  Validation took: 0:10:21\n","Validation F1-Score: 0.8978702797100709\n","\n","======== Epoch 6 / 6 ========\n","Training...\n","\n","Training Accuracy: 0.91\n","  Average training loss: 0.29 \n","  Training epcoh took: 0:38:01 \n","\n","Running Validation...\n","  Accuracy: 0.91\n","  Validation Loss: 0.31\n","  Validation took: 0:10:23\n","Validation F1-Score: 0.8996660958607078\n","\n","Training complete!\n","Total training took 5:00:36 (h:mm:ss)\n","\n"],"name":"stdout"},{"output_type":"display_data","data":{"image/png":"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\n","text/plain":["<Figure size 432x288 with 1 Axes>"]},"metadata":{"tags":[],"needs_background":"light"}},{"output_type":"stream","text":["\n"],"name":"stdout"},{"output_type":"display_data","data":{"image/png":"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\n","text/plain":["<Figure size 432x288 with 1 Axes>"]},"metadata":{"tags":[],"needs_background":"light"}}]},{"cell_type":"code","metadata":{"id":"A_lgPHpW0kv3","executionInfo":{"status":"ok","timestamp":1619192218091,"user_tz":-120,"elapsed":18440379,"user":{"displayName":"Sara Fattouh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjGrdvZo2x_8K8abskRubIgMK3SdtS4cG-aQE01nA=s64","userId":"09360594903764573322"}}},"source":["# pwd"],"execution_count":93,"outputs":[]},{"cell_type":"code","metadata":{"id":"Te03NzKu28Fs","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1619192222480,"user_tz":-120,"elapsed":18444762,"user":{"displayName":"Sara Fattouh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjGrdvZo2x_8K8abskRubIgMK3SdtS4cG-aQE01nA=s64","userId":"09360594903764573322"}},"outputId":"b7d4da66-bc33-4f47-847c-8437bbb97027"},"source":["ls -la"],"execution_count":94,"outputs":[{"output_type":"stream","text":["total 679404\n","drwxr-xr-x 1 root root      4096 Apr 23 10:34 \u001b[0m\u001b[01;34m.\u001b[0m/\n","drwxr-xr-x 1 root root      4096 Apr 23 08:37 \u001b[01;34m..\u001b[0m/\n","drwxr-xr-x 4 root root      4096 Apr 21 13:38 \u001b[01;34m.config\u001b[0m/\n","drwx------ 5 root root      4096 Apr 23 08:38 \u001b[01;34mdrive\u001b[0m/\n","drwxr-xr-x 6 root root      4096 Apr 23 09:04 \u001b[01;34mRakuten\u001b[0m/\n","drwxr-xr-x 1 root root      4096 Apr 21 13:39 \u001b[01;34msample_data\u001b[0m/\n","-rw-r--r-- 1 root root  20869880 Apr 23 10:34 test_inputs_cam.pt\n","-rw-r--r-- 1 root root  20869880 Apr 23 10:34 test_inputs_flau.pt\n","-rw-r--r-- 1 root root  20869880 Apr 23 10:34 test_masks_cam.pt\n","-rw-r--r-- 1 root root  20869880 Apr 23 10:34 test_masks_flau.pt\n","-rw-r--r-- 1 root root 118256376 Apr 23 10:34 tr_inputs_cam.pt\n","-rw-r--r-- 1 root root 118256376 Apr 23 10:34 tr_inputs_flau.pt\n","-rw-r--r-- 1 root root 118256376 Apr 23 10:34 tr_masks_cam.pt\n","-rw-r--r-- 1 root root 118256376 Apr 23 10:34 tr_masks_flau.pt\n","-rw-r--r-- 1 root root  34783992 Apr 23 10:34 val_inputs_cam.pt\n","-rw-r--r-- 1 root root  34783992 Apr 23 10:34 val_inputs_flau.pt\n","-rw-r--r-- 1 root root  34783992 Apr 23 10:34 val_masks_cam.pt\n","-rw-r--r-- 1 root root  34783992 Apr 23 10:34 val_masks_flau.pt\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"GMNZStqu0ZzH","executionInfo":{"status":"ok","timestamp":1619192222482,"user_tz":-120,"elapsed":18444760,"user":{"displayName":"Sara Fattouh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjGrdvZo2x_8K8abskRubIgMK3SdtS4cG-aQE01nA=s64","userId":"09360594903764573322"}}},"source":["# cp './Hirarical_add_concat_best_model.pt' '../drive/My Drive/Rakuten/models/'"],"execution_count":95,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"RkB48fg1HidV"},"source":["# Model Testing"]},{"cell_type":"code","metadata":{"id":"XjYObOyZ-GAd","executionInfo":{"status":"ok","timestamp":1619192222483,"user_tz":-120,"elapsed":18444755,"user":{"displayName":"Sara Fattouh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjGrdvZo2x_8K8abskRubIgMK3SdtS4cG-aQE01nA=s64","userId":"09360594903764573322"}}},"source":["model_path =  '/content/drive/My Drive/Rakuten/models/Final_Hirarical_model.pt'"],"execution_count":96,"outputs":[]},{"cell_type":"code","metadata":{"id":"jKTbBLHzHoYL","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1619192252577,"user_tz":-120,"elapsed":18474845,"user":{"displayName":"Sara Fattouh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjGrdvZo2x_8K8abskRubIgMK3SdtS4cG-aQE01nA=s64","userId":"09360594903764573322"}},"outputId":"8138b01c-fa29-4086-988e-dbd045cf27b9"},"source":["checkpoint = torch.load(model_path)\n","model.load_state_dict(checkpoint) # A state_dict is simply a Python dictionary object \n","                                  # that maps each layer to its parameter tensor"],"execution_count":97,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<All keys matched successfully>"]},"metadata":{"tags":[]},"execution_count":97}]},{"cell_type":"code","metadata":{"id":"46UaiCmcHwR4","executionInfo":{"status":"ok","timestamp":1619192252578,"user_tz":-120,"elapsed":18474840,"user":{"displayName":"Sara Fattouh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjGrdvZo2x_8K8abskRubIgMK3SdtS4cG-aQE01nA=s64","userId":"09360594903764573322"}}},"source":["import matplotlib.pyplot as plt\n","from sklearn.metrics import f1_score\n","\n","def predict_pyt(model, prediction_dataloader):\n","    \"\"\"\n","    model: pytorch model\n","    prediction_dataloader: DataLoader object for which the predictions has to be made.\n","    return:\n","        predictions:    - Direct predicted labels\n","        softmax_logits: - logits which are normalized with softmax on output\"\"\"\n","    \n","    \n","    print(\"\")\n","    print(\"Running Testing...\")\n","\n","    t0 = time.time()\n","\n","    # Put the model in evaluation mode--the dropout layers behave differently\n","    # during evaluation.\n","    model.eval()\n","\n","    # Tracking variables \n","    total_eval_accuracy = 0\n","    total_eval_loss = 0\n","    nb_eval_steps = 0\n","    predictions=[]\n","    true_labels=[]\n","    logits_values =[]\n","    val_accuracy_values = []\n","    val_loss_values = []\n","    \n","    \n","    total_t0 = time.time()\n","    # Evaluate data for one epoch\n","    for batch in (prediction_dataloader):\n","        \n","        # Unpack this training batch from our dataloader. \n","        #\n","        # As we unpack the batch, we'll also copy each tensor to the GPU using \n","        # the `to` method.\n","        #\n","        # `batch` contains three pytorch tensors:\n","        #   [0]: input ids \n","        #   [1]: attention masks\n","        #   [2]: labels \n","        \n","        b_img = batch[0].to(device)\n","\n","        b_input_id_cam = batch[1].to(device)\n","        b_input_mask_cam = batch[2].to(device)\n","        b_input_id_flau = batch[3].to(device)\n","        b_input_mask_flau = batch[4].to(device)\n","\n","        b_labels = batch[5].to(device)\n","        \n","        \n","        # Tell pytorch not to bother with constructing the compute graph during\n","        # the forward pass, since this is only needed for backprop (training).\n","        with torch.no_grad():       \n","        \n","\n","            # Forward pass, calculate logit predictions.\n","            # token_type_ids is the same as the \"segment ids\", which \n","            # differentiates sentence 1 and 2 in 2-sentence tasks.\n","            # The documentation for this `model` function is here: \n","            # https://huggingface.co/transformers/v2.2.0/model_doc/bert.html#transformers.BertForSequenceClassification\n","            # Get the \"logits\" output by the model. The \"logits\" are the output\n","            # values prior to applying an activation function like the softmax.\n","\n","\n","            logits = model(b_img,b_input_id_cam ,b_input_mask_cam,b_input_id_flau,b_input_mask_flau)\n","            \n","        #new\n","        \n","        #defining the val loss\n","        loss = loss_criterion(logits, b_labels)\n","        \n","        # Accumulate the validation loss.\n","        total_eval_loss += loss.item()\n","\n","        # Move logits and labels to CPU\n","        logits = logits.detach().cpu().numpy()\n","\n","        # Move logits and labels to CPU\n","        predicted_labels=np.argmax(logits,axis=1)\n","        predictions.extend(predicted_labels)\n","        label_ids = b_labels.to('cpu').numpy()\n","        true_labels.extend(label_ids)\n","\n","        ##########################################################################\n","\n","        logits_values.append(predicted_labels)\n","\n","        ##########################################################################\n","\n","        #saving the features_tr\n","#         vec = vec.detach().cpu().numpy()\n","#         vec_output_val.extend(vec)\n","        \n","\n","        # Calculate the accuracy for this batch of test sentences, and\n","        # accumulate it over all batches.\n","        total_eval_accuracy += flat_accuracy(logits, label_ids)\n","        \n","\n","    # Report the final accuracy for this validation run.\n","    avg_val_accuracy = total_eval_accuracy / len(validation_dataloader)\n","\n","#--------------------------------\n","    val_accuracy_values.append(avg_val_accuracy)\n","#--------------------------------\n","    print(\"  Accuracy: {0:.2f}\".format(avg_val_accuracy))\n","\n","    # Calculate the average loss over all of the batches.\n","    avg_val_loss = total_eval_loss / len(validation_dataloader)\n","#-----------------------------\n","    val_loss_values.append(avg_val_loss)\n","\n","    \n","    \n","    # Measure how long the validation run took.\n","    validation_time = format_time(time.time() - t0)\n","    \n","    print(\"  Test Loss: {0:.2f}\".format(avg_val_loss))\n","    print(\"  Test took: {:}\".format(validation_time))\n","    print(\"Test F1-Score: {}\".format(f1_score(true_labels,predictions,average='macro')))\n","    curr_f1=f1_score(true_labels,predictions,average='macro')\n","\n","\n","    print(\"\")\n","    print(\"Testing complete!\")\n","\n","    #print(\"Total testing took {:} (h:mm:ss)\".format(format_time(time.time()-total_t0)))\n","    print()\n","    plt.plot(np.array(val_accuracy_values), 'r', label='Test accuracy')\n","    plt.legend()\n","    plt.title('Test Curve')\n","    plt.show()\n","\n","    print()\n","    print('DONE')\n"],"execution_count":98,"outputs":[]},{"cell_type":"code","metadata":{"id":"TwK3RdRpLy30","colab":{"base_uri":"https://localhost:8080/","height":468},"executionInfo":{"status":"ok","timestamp":1619193408214,"user_tz":-120,"elapsed":634613,"user":{"displayName":"Sara Fattouh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjGrdvZo2x_8K8abskRubIgMK3SdtS4cG-aQE01nA=s64","userId":"09360594903764573322"}},"outputId":"5f5e47be-a8b3-430b-8941-14b5333e943d"},"source":["predict_pyt(model, test_loader)"],"execution_count":102,"outputs":[{"output_type":"stream","text":["\n","Running Testing...\n","  Accuracy: 0.91\n","  Test Loss: 0.31\n","  Test took: 0:10:34\n","Test F1-Score: 0.8996660958607078\n","\n","Testing complete!\n","\n"],"name":"stdout"},{"output_type":"display_data","data":{"image/png":"iVBORw0KGgoAAAANSUhEUgAAAXoAAAEICAYAAABRSj9aAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAX8UlEQVR4nO3df5RU5Z3n8fdHWsAEFIUOIbYCmXUSGuwGLXAkZxaMEjQnKogzwWBQR+O4LnhmV06CYZIhZDxRo0fXxDmGzfh7DbgYoonZcURh9IzOQKMosgyCSEIjTnogIEiQX9/9o26zRVvQ1XQ11f34eZ1Tp289z3NvfZ/ucz59695btxQRmJlZuo6rdAFmZtaxHPRmZolz0JuZJc5Bb2aWOAe9mVniHPRmZolz0JuZJc5Bb12KpJ0FjwOS/lDwfMpRbG+JpOtaGdNd0mxJayV9IGmDpAckDTraeZgdSw5661IiolfzA/gtcHFB2//qoJddAFwCfA04CagHlgPnt3VDkqrKW5pZ6xz0lgRJx0maKeltSVskPSHplKyvp6THsvZtkpZJ6i/pVuBPgR9n7wh+XGS7FwDjgEsjYllE7IuI7RFxX0T8fTZmQzaueZ3Zkh7LlgdJCknXSvot8IKk/yNpWovXeV3SZdny5yU9J2mrpDWS/ryDfm32MeGgt1RMByYAY4DPAL8H7sv6riK/J34a0Be4AfhDRMwCXgKmZe8Ipn1kq3ABsDQiNrazvjHAEGA88DPgiuYOSbXAQOAZSZ8EngMeBz4FTAb+LhtjdlQc9JaKG4BZEdEYER8Cs4HLs0Mle8kH/H+KiP0RsTwi3i9xu32BzWWob3ZEfBARfwAWAsMlDcz6pgA/z+r+CrAhIh7M3j28BjwJ/FkZarCPKQe9pWIgsDA7NLMNWA3sB/oDjwLPAvMkvSvpDknHl7jdLcCAMtR38B1BROwAniG/tw75vfvm8wsDgXOa55HNZQrw6TLUYB9TDnpLxUbgoojoU/DoGRGbImJvRHwvImqB0eT3mqdm67V2+9ZFwChJNUcY8wHwiYLnxUK55ev8DLhC0rlAT2BxwTz+qcU8ekXEf2mlTrPDctBbKu4Hbm0+HCKpWtKl2fJ5ks6U1A14n/yhnAPZev8OfPZwG42IReSPmS+UdLakKkm9Jd0g6S+yYSuAyZKOl5QDLi+h3l+T33ufA8yPiOZ6fgX8saSvZ9s7XtJISUPa8LswO4SD3lLxP4CngX+UtAP4F+CcrO/T5C+RfJ/8IZ1/In84p3m9yyX9XtK9h9n25eSDeT6wHXgTyJHf2wf4DvBH5E8Af4/8idQjyo7H/5z8yd7HC9p3AF8if1jnXeA94HagR2vbNDsc+YtHzMzS5j16M7PEOejNzBLnoDczS5yD3swscZ3uBkv9+vWLQYMGVboMM7MuZfny5f8REdXF+koKekkXkr8MrRvw04i4rUX/QOABoBrYClwZEY1Z3+nAT8nfZySAL0fEhsO91qBBg2hoaCilLDMzy0j6zeH6Wj10k33I5D7gIqCW/Kf5Wt5g6U7gkYioI/8BkB8U9D0C/DAihgCjgN+1rXwzM2uPUo7RjwLWRcT6iNgDzAMubTGmFnghW17c3J/9Q6iKiOcAImJnROwqS+VmZlaSUoL+VApuyAQ0Zm2FXgcuy5YnAr0l9QX+GNgm6eeSXpP0w+wdwiEkXS+pQVJDU1NT22dhZmaHVa6TsTPIf3nD1cCLwCbydw6sIv/FDiPIfxvQfOBq4O8LV46IucBcgFwu54/qmnUCe/fupbGxkd27d1e6FCvQs2dPampqOP74Um/AWlrQbyJ/IrVZTdZ2UES8S7ZHL6kXMCkitklqBFZExPqs7xfAn9Ai6M2s82lsbKR3794MGjQISZUux4CIYMuWLTQ2NjJ48OCS1yvl0M0y4AxJgyV1J3+zpacLB0jqJ6l5W7eQvwKned0+kpov+fki8H9Lrs7MKmb37t307dvXId+JSKJv375tfpfVatBHxD5gGvkvblgNPBERqyTNkXRJNmwssEbSW+S/6OHWbN395A/rPC9pJSDgf7apQjOrGId853M0f5OSjtFHxK/J36a1sO27BcsLyN8Gtti6zwF1ba7MzMzKotN9MtbMbMuWLZx//vkAvPfee3Tr1o3q6vwR4KVLl9K9e/cjrr9kyRK6d+/O6NGjO7zWrsBBb2adTt++fVmxYgUAs2fPplevXsyYMaPk9ZcsWUKvXr0qHvT79++nW7ePXFF+zPmmZmbWJSxfvpwxY8Zw9tlnM378eDZv3gzAvffeS21tLXV1dUyePJkNGzZw//33c/fddzN8+HBeeumlQ7azdOlSzj33XEaMGMHo0aNZs2YNkA/lGTNmMGzYMOrq6vjRj34EwLJlyxg9ejT19fWMGjWKHTt28NBDDzFt2rSD2/zKV77CkiVLAOjVqxc333wz9fX1vPLKK8yZM4eRI0cybNgwrr/+epq/7GndunVccMEF1NfXc9ZZZ/H2228zdepUfvGLXxzc7pQpU3jqqafa/bvzHr2Zte6v/gqyPeyyGT4c7rmnpKERwfTp03nqqaeorq5m/vz5zJo1iwceeIDbbruNd955hx49erBt2zb69OnDDTfccNh3AZ///Od56aWXqKqqYtGiRXz729/mySefZO7cuWzYsIEVK1ZQVVXF1q1b2bNnD1/96leZP38+I0eO5P333+eEE044Yq0ffPAB55xzDnfddRcAtbW1fPe7+VOaX//61/nVr37FxRdfzJQpU5g5cyYTJ05k9+7dHDhwgGuvvZa7776bCRMmsH37dl5++WUefvjhNv5iP8pBb2ad3ocffsibb77JuHHjgPze94ABAwCoq6tjypQpTJgwgQkTJrS6re3bt3PVVVexdu1aJLF3714AFi1axA033EBVVT4WTznlFFauXMmAAQMYOXIkACeeeGKr2+/WrRuTJk06+Hzx4sXccccd7Nq1i61btzJ06FDGjh3Lpk2bmDhxIpD/EBTAmDFjuPHGG2lqauLJJ59k0qRJB+tpDwe9mbWuxD3vjhIRDB06lFdeeeUjfc888wwvvvgiv/zlL7n11ltZuXLlEbf1ne98h/POO4+FCxeyYcMGxo4d2+Z6qqqqOHDgwMHnhde19+zZ8+Bx+d27d3PjjTfS0NDAaaedxuzZs1u9Bn7q1Kk89thjzJs3jwcffLDNtRXjY/Rm1un16NGDpqamg0G/d+9eVq1axYEDB9i4cSPnnXcet99+O9u3b2fnzp307t2bHTt2FN3W9u3bOfXU/O26HnrooYPt48aN4yc/+Qn79u0DYOvWrXzuc59j8+bNLFu2DIAdO3awb98+Bg0axIoVKw6+/tKlS4u+VnOo9+vXj507d7JgQf4q9N69e1NTU3PwePyHH37Irl35+z1effXV3JP9Y62tbXmj4KPjoDezTu+4445jwYIFfOtb36K+vp7hw4fz8ssvs3//fq688krOPPNMRowYwU033USfPn24+OKLWbhwYdGTsd/85je55ZZbGDFixMFQB7juuus4/fTTqauro76+nscff5zu3bszf/58pk+fTn19PePGjWP37t184QtfYPDgwdTW1nLTTTdx1llnFa27T58+fOMb32DYsGGMHz/+4CEggEcffZR7772Xuro6Ro8ezXvvvQdA//79GTJkCNdcc03Zfn9qPgPcWeRyufAXj5hV3urVqxkyZEily/jY2bVrF2eeeSavvvoqJ510UtExxf42kpZHRK7YeO/Rm5l1EosWLWLIkCFMnz79sCF/NHwy1sysk7jgggv4zW8O+42AR8179GZ2WJ3t0K4d3d/EQW9mRfXs2ZMtW7Y47DuR5vvRN193XyofujGzompqamhsbMRf79m5NH/DVFs46M2sqOOPP75N32JknZcP3ZiZJc5Bb2aWOAe9mVniHPRmZolz0JuZJc5Bb2aWOAe9mVniHPRmZolz0JuZJc5Bb2aWOAe9mVniHPRmZolz0JuZJc5Bb2aWOAe9mVniHPRmZolz0JuZJc5Bb2aWOAe9mVniSgp6SRdKWiNpnaSZRfoHSnpe0huSlkiqadF/oqRGST8uV+FmZlaaVoNeUjfgPuAioBa4QlJti2F3Ao9ERB0wB/hBi/7vAy+2v1wzM2urUvboRwHrImJ9ROwB5gGXthhTC7yQLS8u7Jd0NtAf+Mf2l2tmZm1VStCfCmwseN6YtRV6HbgsW54I9JbUV9JxwF3AjCO9gKTrJTVIamhqaiqtcjMzK0m5TsbOAMZIeg0YA2wC9gM3Ar+OiMYjrRwRcyMiFxG56urqMpVkZmYAVSWM2QScVvC8Jms7KCLeJdujl9QLmBQR2ySdC/yppBuBXkB3STsj4iMndM3MrGOUEvTLgDMkDSYf8JOBrxUOkNQP2BoRB4BbgAcAImJKwZirgZxD3szs2Gr10E1E7AOmAc8Cq4EnImKVpDmSLsmGjQXWSHqL/InXWzuoXjMzayNFRKVrOEQul4uGhoZKl2Fm1qVIWh4RuWJ9/mSsmVniHPRmZolz0JuZJc5Bb2aWOAe9mVniHPRmZolz0JuZJc5Bb2aWOAe9mVniHPRmZolz0JuZJc5Bb2aWOAe9mVniHPRmZolz0JuZJc5Bb2aWOAe9mVniHPRmZolz0JuZJc5Bb2aWOAe9mVniHPRmZolz0JuZJc5Bb2aWOAe9mVniHPRmZolz0JuZJc5Bb2aWOAe9mVniHPRmZolz0JuZJc5Bb2aWOAe9mVniHPRmZokrKeglXShpjaR1kmYW6R8o6XlJb0haIqkmax8u6RVJq7K+r5Z7AmZmdmStBr2kbsB9wEVALXCFpNoWw+4EHomIOmAO8IOsfRcwNSKGAhcC90jqU67izcysdaXs0Y8C1kXE+ojYA8wDLm0xphZ4IVte3NwfEW9FxNps+V3gd0B1OQo3M7PSlBL0pwIbC543Zm2FXgcuy5YnAr0l9S0cIGkU0B14++hKNTOzo1Guk7EzgDGSXgPGAJuA/c2dkgYAjwLXRMSBlitLul5Sg6SGpqamMpVkZmZQWtBvAk4reF6TtR0UEe9GxGURMQKYlbVtA5B0IvAMMCsi/qXYC0TE3IjIRUSuutpHdszMyqmUoF8GnCFpsKTuwGTg6cIBkvpJat7WLcADWXt3YCH5E7ULyle2mZmVqtWgj4h9wDTgWWA18ERErJI0R9Il2bCxwBpJbwH9gVuz9j8H/jNwtaQV2WN4uSdhZmaHp4iodA2HyOVy0dDQUOkyzMy6FEnLIyJXrM+fjDUzS5yD3swscQ56M7PEOejNzBLnoDczS5yD3swscQ56M7PEOejNzBLnoDczS5yD3swscQ56M7PEOejNzBLnoDczS5yD3swscQ56M7PEOejNzBLnoDczS5yD3swscQ56M7PEOejNzBLnoDczS5yD3swscQ56M7PEOejNzBLnoDczS5yD3swscQ56M7PEOejNzBLnoDczS5yD3swscQ56M7PEOejNzBLnoDczS5yD3swscQ56M7PElRT0ki6UtEbSOkkzi/QPlPS8pDckLZFUU9B3laS12eOqchZvZmatazXoJXUD7gMuAmqBKyTVthh2J/BIRNQBc4AfZOueAvwNcA4wCvgbSSeXr3wzM2tNKXv0o4B1EbE+IvYA84BLW4ypBV7IlhcX9I8HnouIrRHxe+A54ML2l21mZqUqJehPBTYWPG/M2gq9DlyWLU8EekvqW+K6SLpeUoOkhqamplJrNzOzEpTrZOwMYIyk14AxwCZgf6krR8TciMhFRK66urpMJZmZGUBVCWM2AacVPK/J2g6KiHfJ9ugl9QImRcQ2SZuAsS3WXdKOes3MrI1K2aNfBpwhabCk7sBk4OnCAZL6SWre1i3AA9nys8CXJJ2cnYT9UtZmZmbHSKtBHxH7gGnkA3o18ERErJI0R9Il2bCxwBpJbwH9gVuzdbcC3yf/z2IZMCdrMzOzY0QRUekaDpHL5aKhoaHSZZiZdSmSlkdErlifPxlrZpY4B72ZWeIc9GZmiXPQm5klzkFvZpY4B72ZWeIc9GZmiXPQm5klzkFvZpY4B72ZWeIc9GZmiXPQm5klzkFvZpY4B72ZWeIc9GZmiXPQm5klzkFvZpY4B72ZWeIc9GZmiXPQm5klzkFvZpY4B72ZWeIc9GZmiXPQm5klzkFvZpY4B72ZWeIc9GZmiXPQm5klzkFvZpY4B72ZWeIc9GZmiXPQm5klzkFvZpY4B72ZWeJKCnpJF0paI2mdpJlF+k+XtFjSa5LekPTlrP14SQ9LWilptaRbyj0BMzM7slaDXlI34D7gIqAWuEJSbYthfw08EREjgMnA32Xtfwb0iIgzgbOBv5Q0qDylm5lZKUrZox8FrIuI9RGxB5gHXNpiTAAnZssnAe8WtH9SUhVwArAHeL/dVZuZWclKCfpTgY0FzxuztkKzgSslNQK/BqZn7QuAD4DNwG+BOyNia8sXkHS9pAZJDU1NTW2bgZmZHVG5TsZeATwUETXAl4FHJR1H/t3AfuAzwGDgZkmfbblyRMyNiFxE5Kqrq8tUkpmZQWlBvwk4reB5TdZW6FrgCYCIeAXoCfQDvgb8Q0TsjYjfAf8M5NpbtJmZla6UoF8GnCFpsKTu5E+2Pt1izG+B8wEkDSEf9E1Z+xez9k8CfwL8W3lKNzOzUrQa9BGxD5gGPAusJn91zSpJcyRdkg27GfiGpNeBnwFXR0SQv1qnl6RV5P9hPBgRb3TERMzMrDjl87jzyOVy0dDQUOkyzMy6FEnLI6LooXF/MtbMLHEOejOzxDnozcwS56A3M0ucg97MLHEOejOzxDnozcwS56A3M0ucg97MLHEOejOzxDnozcwS56A3M0ucg97MLHEOejOzxDnozcwS56A3M0ucg97MLHEOejOzxDnozcwS56A3M0ucg97MLHEOejOzxDnozcwS56A3M0ucg97MLHEOejOzxDnozcwS56A3M0ucg97MLHEOejOzxDnozcwS56A3M0ucg97MLHEOejOzxDnozcwSp4iodA2HkNQE/KbSdRyFfsB/VLqIY8xz/njwnLuGgRFRXayj0wV9VyWpISJyla7jWPKcPx48567Ph27MzBLnoDczS5yDvnzmVrqACvCcPx485y7Ox+jNzBLnPXozs8Q56M3MEuegbwNJp0h6TtLa7OfJhxl3VTZmraSrivQ/LenNjq+4/dozZ0mfkPSMpH+TtErSbce2+tJJulDSGknrJM0s0t9D0vys/18lDSrouyVrXyNp/LGsuz2Ods6SxklaLmll9vOLx7r2o9Wev3PWf7qknZJmHKuayyIi/CjxAdwBzMyWZwK3FxlzCrA++3lytnxyQf9lwOPAm5WeT0fPGfgEcF42pjvwEnBRpedUpP5uwNvAZ7M6XwdqW4y5Ebg/W54MzM+Wa7PxPYDB2Xa6VXpOHTznEcBnsuVhwKZKz6ej51zQvwD438CMSs+nLQ/v0bfNpcDD2fLDwIQiY8YDz0XE1oj4PfAccCGApF7Afwf+9hjUWi5HPeeI2BURiwEiYg/wKlBzDGpuq1HAuohYn9U5j/y8CxX+HhYA50tS1j4vIj6MiHeAddn2OrujnnNEvBYR72btq4ATJPU4JlW3T3v+zkiaALxDfs5dioO+bfpHxOZs+T2gf5ExpwIbC543Zm0A3wfuAnZ1WIXl1945AyCpD3Ax8HxHFNlOrdZfOCYi9gHbgb4lrtsZtWfOhSYBr0bEhx1UZzkd9ZyznbRvAd87BnWWXVWlC+hsJC0CPl2ka1bhk4gISSVfmyppOPBHEfHfWh73q7SOmnPB9quAnwH3RsT6o6vSOhtJQ4HbgS9VupZjYDZwd0TszHbwuxQHfQsRccHh+iT9u6QBEbFZ0gDgd0WGbQLGFjyvAZYA5wI5SRvI/94/JWlJRIylwjpwzs3mAmsj4p4ylNsRNgGnFTyvydqKjWnM/nGdBGwpcd3OqD1zRlINsBCYGhFvd3y5ZdGeOZ8DXC7pDqAPcEDS7oj4cceXXQaVPknQlR7ADzn0xOQdRcacQv443snZ4x3glBZjBtF1Tsa2a87kz0c8CRxX6bkcYY5V5E8gD+b/n6Qb2mLMf+XQk3RPZMtDOfRk7Hq6xsnY9sy5Tzb+skrP41jNucWY2XSxk7EVL6ArPcgfn3weWAssKgizHPDTgnF/Qf6k3DrgmiLb6UpBf9RzJr/HFMBqYEX2uK7SczrMPL8MvEX+qoxZWdsc4JJsuSf5qy3WAUuBzxasOytbbw2d8Kqics8Z+Gvgg4K/6QrgU5WeT0f/nQu20eWC3rdAMDNLnK+6MTNLnIPezCxxDnozs8Q56M3MEuegNzNLnIPezCxxDnozs8T9P0Ym6loFfRsJAAAAAElFTkSuQmCC\n","text/plain":["<Figure size 432x288 with 1 Axes>"]},"metadata":{"tags":[],"needs_background":"light"}},{"output_type":"stream","text":["\n","DONE\n"],"name":"stdout"}]}]}