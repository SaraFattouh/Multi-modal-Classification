{"nbformat":4,"nbformat_minor":0,"metadata":{"accelerator":"GPU","colab":{"name":"TEST Mean 80-20 (FINAL) Hirerical Fusion.ipynb","provenance":[{"file_id":"16T2J5BOltd1RT58Mr4O81xPwRckKNTvO","timestamp":1619285234665},{"file_id":"10HG_Jj6e-CNcUe_Ng3H6hAbd0crPZTLT","timestamp":1618388020139},{"file_id":"1Iwp1sNEfG6ly73_MK_bi3EUEZAC05-lQ","timestamp":1617026135135},{"file_id":"1OCoA3nYHVEcUF5r3Sxaq9PuAqT1U9Lx7","timestamp":1616632198364},{"file_id":"1GEPelG8M4pwx_llUYLlmUO6jfjnUgMPl","timestamp":1616587813712},{"file_id":"1Px7sog6jBh8jhBFgrHAKeqt1maPE48jM","timestamp":1614749164976}],"collapsed_sections":[],"toc_visible":true,"machine_shape":"hm"},"kernelspec":{"display_name":"Python 3","name":"python3"},"widgets":{"application/vnd.jupyter.widget-state+json":{"014f925fc8da4ac0a39f7f8d9492bb5e":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_8f7ba449617443c28b070094dddd31f3","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_afd17b52771e41e6a10107d5d6a11f3b","IPY_MODEL_f3508a41adc749628755fd248ba7b858"]}},"8f7ba449617443c28b070094dddd31f3":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"afd17b52771e41e6a10107d5d6a11f3b":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_ecd4e16d696a46f488f918fdc8815d1a","_dom_classes":[],"description":"100%","_model_name":"FloatProgressModel","bar_style":"success","max":55025,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":55025,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_1b3834f417454cf7b3cdc6bcfbadaaf8"}},"f3508a41adc749628755fd248ba7b858":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_43c4c006766a494bb74ff59c012c332e","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 55025/55025 [00:11&lt;00:00, 4861.09it/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_6c862f0ed686478885bf33bed99f576e"}},"ecd4e16d696a46f488f918fdc8815d1a":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"1b3834f417454cf7b3cdc6bcfbadaaf8":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"43c4c006766a494bb74ff59c012c332e":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"6c862f0ed686478885bf33bed99f576e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}}}}},"cells":[{"cell_type":"markdown","metadata":{"id":"1fCPKaFCgw23"},"source":["# Multimodal Calssification on Rakuten France Dataset\n","# Multi Modal Addition Fusion"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"feRlWWySuDCt","executionInfo":{"status":"ok","timestamp":1620603099976,"user_tz":-120,"elapsed":1631,"user":{"displayName":"Sara Fattouh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjGrdvZo2x_8K8abskRubIgMK3SdtS4cG-aQE01nA=s64","userId":"09360594903764573322"}},"outputId":"d814786e-6924-4fb8-fe38-884cc20f8d58"},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"BZouy12lm_Xv","executionInfo":{"status":"ok","timestamp":1620603100949,"user_tz":-120,"elapsed":2589,"user":{"displayName":"Sara Fattouh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjGrdvZo2x_8K8abskRubIgMK3SdtS4cG-aQE01nA=s64","userId":"09360594903764573322"}},"outputId":"6d6c888a-5420-4207-ce4d-31881534e407"},"source":["!ls"],"execution_count":2,"outputs":[{"output_type":"stream","text":["drive  Rakuten\tsample_data\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"id":"oG6LhxvoLTKt","executionInfo":{"status":"ok","timestamp":1620603100951,"user_tz":-120,"elapsed":2578,"user":{"displayName":"Sara Fattouh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjGrdvZo2x_8K8abskRubIgMK3SdtS4cG-aQE01nA=s64","userId":"09360594903764573322"}},"outputId":"6aad2fc6-e55c-40e1-d92c-c438fb241c7d"},"source":["pwd"],"execution_count":3,"outputs":[{"output_type":"execute_result","data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'/content'"]},"metadata":{"tags":[]},"execution_count":3}]},{"cell_type":"code","metadata":{"id":"BhBoZ1r7tBSd","executionInfo":{"status":"ok","timestamp":1620603100952,"user_tz":-120,"elapsed":2567,"user":{"displayName":"Sara Fattouh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjGrdvZo2x_8K8abskRubIgMK3SdtS4cG-aQE01nA=s64","userId":"09360594903764573322"}}},"source":["# mkdir Rakuten"],"execution_count":4,"outputs":[]},{"cell_type":"code","metadata":{"id":"rtpdRXBytE0X","executionInfo":{"status":"ok","timestamp":1620603100953,"user_tz":-120,"elapsed":2560,"user":{"displayName":"Sara Fattouh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjGrdvZo2x_8K8abskRubIgMK3SdtS4cG-aQE01nA=s64","userId":"09360594903764573322"}}},"source":["# cd './Rakuten'"],"execution_count":5,"outputs":[]},{"cell_type":"code","metadata":{"id":"Im900y65tWcT","executionInfo":{"status":"ok","timestamp":1620603100955,"user_tz":-120,"elapsed":2550,"user":{"displayName":"Sara Fattouh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjGrdvZo2x_8K8abskRubIgMK3SdtS4cG-aQE01nA=s64","userId":"09360594903764573322"}}},"source":["# mkdir models data "],"execution_count":6,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"id":"Jzp_vuw1MuNS","executionInfo":{"status":"ok","timestamp":1620603100956,"user_tz":-120,"elapsed":2543,"user":{"displayName":"Sara Fattouh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjGrdvZo2x_8K8abskRubIgMK3SdtS4cG-aQE01nA=s64","userId":"09360594903764573322"}},"outputId":"4b9534ef-f1e4-40c8-bad5-f6617fea69a5"},"source":["pwd"],"execution_count":7,"outputs":[{"output_type":"execute_result","data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'/content'"]},"metadata":{"tags":[]},"execution_count":7}]},{"cell_type":"code","metadata":{"id":"pxUWT_9hzEVv","executionInfo":{"status":"ok","timestamp":1620603100957,"user_tz":-120,"elapsed":2531,"user":{"displayName":"Sara Fattouh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjGrdvZo2x_8K8abskRubIgMK3SdtS4cG-aQE01nA=s64","userId":"09360594903764573322"}}},"source":["# cp '/content/drive/My Drive/Rakuten/models/CamemBERT_best_model_split_title.pt' models"],"execution_count":8,"outputs":[]},{"cell_type":"code","metadata":{"id":"IwsYn7UatdVV","executionInfo":{"status":"ok","timestamp":1620603100957,"user_tz":-120,"elapsed":2524,"user":{"displayName":"Sara Fattouh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjGrdvZo2x_8K8abskRubIgMK3SdtS4cG-aQE01nA=s64","userId":"09360594903764573322"}}},"source":["# cp '/content/drive/My Drive/Rakuten/models/CamemBERT_best_model_split_description.pt' models\n"],"execution_count":9,"outputs":[]},{"cell_type":"code","metadata":{"id":"tFVZ-GX6SDCp","executionInfo":{"status":"ok","timestamp":1620603100958,"user_tz":-120,"elapsed":2518,"user":{"displayName":"Sara Fattouh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjGrdvZo2x_8K8abskRubIgMK3SdtS4cG-aQE01nA=s64","userId":"09360594903764573322"}}},"source":["# cp '/content/drive/My Drive/Rakuten/models/FlauBERT_best_model_split_description.pt' models"],"execution_count":10,"outputs":[]},{"cell_type":"code","metadata":{"id":"OrrpaqRTui48","executionInfo":{"status":"ok","timestamp":1620603100959,"user_tz":-120,"elapsed":2512,"user":{"displayName":"Sara Fattouh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjGrdvZo2x_8K8abskRubIgMK3SdtS4cG-aQE01nA=s64","userId":"09360594903764573322"}}},"source":["# cp '/content/drive/My Drive/Rakuten/models/FlauBERT_best_model_split_title.pt' models\n"],"execution_count":11,"outputs":[]},{"cell_type":"code","metadata":{"id":"XEeo6IJHukVz","executionInfo":{"status":"ok","timestamp":1620603100959,"user_tz":-120,"elapsed":2504,"user":{"displayName":"Sara Fattouh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjGrdvZo2x_8K8abskRubIgMK3SdtS4cG-aQE01nA=s64","userId":"09360594903764573322"}}},"source":["# cp '/content/drive/My Drive/Rakuten/models/RESNET_baseline_model.pt' models"],"execution_count":12,"outputs":[]},{"cell_type":"code","metadata":{"id":"NEebtBL70gi6","executionInfo":{"status":"ok","timestamp":1620603100960,"user_tz":-120,"elapsed":2498,"user":{"displayName":"Sara Fattouh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjGrdvZo2x_8K8abskRubIgMK3SdtS4cG-aQE01nA=s64","userId":"09360594903764573322"}}},"source":["# pwd"],"execution_count":13,"outputs":[]},{"cell_type":"code","metadata":{"id":"4kTwB_kSusK2","executionInfo":{"status":"ok","timestamp":1620603100961,"user_tz":-120,"elapsed":2487,"user":{"displayName":"Sara Fattouh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjGrdvZo2x_8K8abskRubIgMK3SdtS4cG-aQE01nA=s64","userId":"09360594903764573322"}}},"source":["# !cp  '/content/drive/My Drive/Rakuten/image.zip' './'"],"execution_count":14,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7iY2GDPw1MZh","executionInfo":{"status":"ok","timestamp":1620603101200,"user_tz":-120,"elapsed":2719,"user":{"displayName":"Sara Fattouh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjGrdvZo2x_8K8abskRubIgMK3SdtS4cG-aQE01nA=s64","userId":"09360594903764573322"}},"outputId":"89ea7ef6-1969-4095-c7d5-cf65b06ad87a"},"source":["ls -la"],"execution_count":15,"outputs":[{"output_type":"stream","text":["total 24\n","drwxr-xr-x 1 root root 4096 May  9 23:12 \u001b[0m\u001b[01;34m.\u001b[0m/\n","drwxr-xr-x 1 root root 4096 May  9 22:46 \u001b[01;34m..\u001b[0m/\n","drwxr-xr-x 4 root root 4096 May  6 13:43 \u001b[01;34m.config\u001b[0m/\n","drwx------ 5 root root 4096 May  9 23:12 \u001b[01;34mdrive\u001b[0m/\n","drwxr-xr-x 5 root root 4096 May  9 23:20 \u001b[01;34mRakuten\u001b[0m/\n","drwxr-xr-x 1 root root 4096 May  6 13:44 \u001b[01;34msample_data\u001b[0m/\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"JhdiQHVFvSj3","executionInfo":{"status":"ok","timestamp":1620603101201,"user_tz":-120,"elapsed":2708,"user":{"displayName":"Sara Fattouh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjGrdvZo2x_8K8abskRubIgMK3SdtS4cG-aQE01nA=s64","userId":"09360594903764573322"}}},"source":["# !unzip  ./image.zip"],"execution_count":16,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"id":"uWx79doRO-pp","executionInfo":{"status":"ok","timestamp":1620603101203,"user_tz":-120,"elapsed":2696,"user":{"displayName":"Sara Fattouh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjGrdvZo2x_8K8abskRubIgMK3SdtS4cG-aQE01nA=s64","userId":"09360594903764573322"}},"outputId":"fde6de2a-bf46-4dba-f70f-a9a07ad57d59"},"source":["pwd"],"execution_count":17,"outputs":[{"output_type":"execute_result","data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'/content'"]},"metadata":{"tags":[]},"execution_count":17}]},{"cell_type":"code","metadata":{"id":"v1kbpdFQwrir","executionInfo":{"status":"ok","timestamp":1620603105036,"user_tz":-120,"elapsed":6517,"user":{"displayName":"Sara Fattouh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjGrdvZo2x_8K8abskRubIgMK3SdtS4cG-aQE01nA=s64","userId":"09360594903764573322"}}},"source":[" !cp '/content/drive/My Drive/Rakuten/data/NewTest.csv' data \n"," !cp '/content/drive/My Drive/Rakuten/data/NewTraining.csv' data \n"," !cp '/content/drive/My Drive/Rakuten/data/catalog_english_taxonomy.tsv' data \n"," !cp '/content/drive/My Drive/Rakuten/data/Y_train.tsv' data \n"," !cp '/content/drive/My Drive/Rakuten/data/X_train.tsv' data"],"execution_count":18,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"id":"USx_EdDXMeU4","executionInfo":{"status":"ok","timestamp":1620603105038,"user_tz":-120,"elapsed":6511,"user":{"displayName":"Sara Fattouh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjGrdvZo2x_8K8abskRubIgMK3SdtS4cG-aQE01nA=s64","userId":"09360594903764573322"}},"outputId":"73ab0069-0924-4e77-f65a-706d31cc7395"},"source":["pwd"],"execution_count":19,"outputs":[{"output_type":"execute_result","data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'/content'"]},"metadata":{"tags":[]},"execution_count":19}]},{"cell_type":"markdown","metadata":{"id":"LXpLoRd1h5xu"},"source":["# 1. Setup"]},{"cell_type":"markdown","metadata":{"id":"FIGO5jehh8-2"},"source":["# 1.1 Using Colab GPU for Training\n","\n","Since we’ll be training a large neural network it’s best to take advantage of the free GPUs and TPUs that Google offers (in this case we’ll attach a GPU), otherwise training will take a very long time.\n","\n","A GPU can be added by going to the menu and selecting:\n","\n","Edit 🡒 Notebook Settings 🡒 Hardware accelerator 🡒 (GPU)"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-hgdtmoPf2al","executionInfo":{"status":"ok","timestamp":1620603106561,"user_tz":-120,"elapsed":8021,"user":{"displayName":"Sara Fattouh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjGrdvZo2x_8K8abskRubIgMK3SdtS4cG-aQE01nA=s64","userId":"09360594903764573322"}},"outputId":"88dd9ba7-138c-4920-a65e-6263ad6292db"},"source":["import os, time, datetime\n","import numpy as np\n","import pandas as pd\n","from tqdm.notebook import tqdm\n","import random\n","import logging\n","tqdm.pandas()\n","import seaborn as sns\n","from sklearn.model_selection import train_test_split\n","from google.colab import drive\n","\n","#NN Packages\n","import torch\n","import torch.nn as nn\n","from torch.utils.data import TensorDataset, random_split,DataLoader, RandomSampler, SequentialSampler\n","\n","logger = logging.getLogger(__name__)"],"execution_count":20,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/tqdm/std.py:658: FutureWarning: The Panel class is removed from pandas. Accessing it from the top-level namespace will also be removed in the next version\n","  from pandas import Panel\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"kEwI57OheXSi","executionInfo":{"status":"ok","timestamp":1620603106564,"user_tz":-120,"elapsed":8012,"user":{"displayName":"Sara Fattouh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjGrdvZo2x_8K8abskRubIgMK3SdtS4cG-aQE01nA=s64","userId":"09360594903764573322"}}},"source":["torch.manual_seed(123)\n","torch.cuda.manual_seed(123)\n","torch.backends.cudnn.enabled=False\n","torch.backends.cudnn.deterministic=True"],"execution_count":21,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7-DN_gCoh_pG","executionInfo":{"status":"ok","timestamp":1620603106564,"user_tz":-120,"elapsed":8004,"user":{"displayName":"Sara Fattouh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjGrdvZo2x_8K8abskRubIgMK3SdtS4cG-aQE01nA=s64","userId":"09360594903764573322"}},"outputId":"f7faf4fd-62f7-44ee-b86a-699805f3526c"},"source":["if torch.cuda.is_available():    \n","\n","    # Tell PyTorch to use the GPU.    \n","    device = torch.device(\"cuda\")\n","\n","    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n","\n","    print('We will use the GPU:', torch.cuda.get_device_name(0))\n","\n","# If not...\n","else:\n","    print('No GPU available, using the CPU instead.')\n","    device = torch.device(\"cpu\")"],"execution_count":22,"outputs":[{"output_type":"stream","text":["There are 1 GPU(s) available.\n","We will use the GPU: Tesla P100-PCIE-16GB\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"8dnUWVTZM8Wc","executionInfo":{"status":"ok","timestamp":1620603106565,"user_tz":-120,"elapsed":7992,"user":{"displayName":"Sara Fattouh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjGrdvZo2x_8K8abskRubIgMK3SdtS4cG-aQE01nA=s64","userId":"09360594903764573322"}},"outputId":"5672dd64-1eb2-4b1a-f932-3b35153080f7"},"source":["gpu_info = !nvidia-smi\n","gpu_info = '\\n'.join(gpu_info)\n","if gpu_info.find('failed') >= 0:\n","  print('Select the Runtime > \"Change runtime type\" menu to enable a GPU accelerator, ')\n","  print('and then re-execute this cell.')\n","else:\n","  print(gpu_info)"],"execution_count":23,"outputs":[{"output_type":"stream","text":["Sun May  9 23:31:46 2021       \n","+-----------------------------------------------------------------------------+\n","| NVIDIA-SMI 465.19.01    Driver Version: 460.32.03    CUDA Version: 11.2     |\n","|-------------------------------+----------------------+----------------------+\n","| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n","| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n","|                               |                      |               MIG M. |\n","|===============================+======================+======================|\n","|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n","| N/A   48C    P0    30W / 250W |      2MiB / 16280MiB |      0%      Default |\n","|                               |                      |                  N/A |\n","+-------------------------------+----------------------+----------------------+\n","                                                                               \n","+-----------------------------------------------------------------------------+\n","| Processes:                                                                  |\n","|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n","|        ID   ID                                                   Usage      |\n","|=============================================================================|\n","|  No running processes found                                                 |\n","+-----------------------------------------------------------------------------+\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"tibnb-j2mVPo"},"source":["# 1.2. Installing the Hugging Face Library - Image Pretrained Models\n","\n","Install the transformers package from **Hugging Face** which will give us a pytorch interface for working with BERT. This library contains interfaces for other pretrained language models.\n","\n","We’ve selected the pytorch interface because it strikes a nice balance between the high-level APIs (which are easy to use but don’t provide insight into how things work) and tensorflow code (which contains lots of details but often sidetracks us into lessons about tensorflow, when the purpose here is BERT!).\n","\n","At the moment, the Hugging Face library seems to be the most widely accepted and powerful pytorch interface for working with BERT. In addition to supporting a variety of different pre-trained transformer models, the library also includes pre-built modifications of these models suited to your specific task.\n","E.g \"BertForSequenceClassification\" that we will be using.\n","\n","The goal  of the **pretrainedmodels** is to:\n","\n","-  help to reproduce transfer learning setups\n","\n","-  access pretrained ConvNets with a unique interface/API inspired by torchvision."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"j_umLuXpmYwk","executionInfo":{"status":"ok","timestamp":1620603116039,"user_tz":-120,"elapsed":17455,"user":{"displayName":"Sara Fattouh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjGrdvZo2x_8K8abskRubIgMK3SdtS4cG-aQE01nA=s64","userId":"09360594903764573322"}},"outputId":"3b94cc1d-8183-44b1-cdc5-06fed306af18"},"source":["!pip install transformers\n","!pip install sentencepiece\n","!pip install pretrainedmodels"],"execution_count":24,"outputs":[{"output_type":"stream","text":["Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (4.5.1)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.41.1)\n","Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from transformers) (3.10.1)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers) (20.9)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.0.12)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n","Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers) (0.0.45)\n","Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.10.2)\n","Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.4.1)\n","Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.7.4.3)\n","Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers) (2.4.7)\n","Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n","Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.0.1)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2020.12.5)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n","Requirement already satisfied: sentencepiece in /usr/local/lib/python3.7/dist-packages (0.1.95)\n","Requirement already satisfied: pretrainedmodels in /usr/local/lib/python3.7/dist-packages (0.7.4)\n","Requirement already satisfied: torchvision in /usr/local/lib/python3.7/dist-packages (from pretrainedmodels) (0.9.1+cu101)\n","Requirement already satisfied: munch in /usr/local/lib/python3.7/dist-packages (from pretrainedmodels) (2.5.0)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from pretrainedmodels) (4.41.1)\n","Requirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (from pretrainedmodels) (1.8.1+cu101)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torchvision->pretrainedmodels) (1.19.5)\n","Requirement already satisfied: pillow>=4.1.1 in /usr/local/lib/python3.7/dist-packages (from torchvision->pretrainedmodels) (7.1.2)\n","Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from munch->pretrainedmodels) (1.15.0)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch->pretrainedmodels) (3.7.4.3)\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"LLbB2aTdigvz"},"source":["# 2. Dataset Loading and Preprocessing"]},{"cell_type":"markdown","metadata":{"id":"WDHFkRyDjxpP"},"source":["# 2.1 Dataset Loading"]},{"cell_type":"code","metadata":{"id":"wJ0jmI1xxQ7j","executionInfo":{"status":"ok","timestamp":1620603116040,"user_tz":-120,"elapsed":17441,"user":{"displayName":"Sara Fattouh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjGrdvZo2x_8K8abskRubIgMK3SdtS4cG-aQE01nA=s64","userId":"09360594903764573322"}}},"source":["text_data_path = '/content/Rakuten/data'\n","image_data_path = '/content/Rakuten/image' "],"execution_count":25,"outputs":[]},{"cell_type":"code","metadata":{"id":"9QzA9onxAnhv","executionInfo":{"status":"ok","timestamp":1620603116040,"user_tz":-120,"elapsed":17431,"user":{"displayName":"Sara Fattouh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjGrdvZo2x_8K8abskRubIgMK3SdtS4cG-aQE01nA=s64","userId":"09360594903764573322"}}},"source":["class SigirPreprocess():\n","\n","  def __init__(self, text_data_path):\n","    \n","        self.text_data_path = text_data_path\n","        self.train = None # Merged X_train and Y_train\n","        self.dict_code_to_id = {}\n","        self.dict_id_to_code = {}\n","        self.list_tags = {} #unique type code\n","        self.sentences = []\n","        self.labels = []\n","        self.text_col = None\n","        self.X_test = None\n","\n","  def prepare_data(self):\n","        \n","        #loading the Merged, preprocessed text data and test data\n","        train = pd.read_csv(self.text_data_path+\"/NewTraining.csv\")\n","        # new_train =  train[train['Description'] != \" \"]\n","        # new_train = new_train[new_train['Description'].notna()]\n","        self.train = train\n","\n","        \n","  def get_sentences(self, text_col, remove_null_rows=True):\n","\n","       #get values of a specific column\n","        self.text_col = text_col        \n","\n","        new_train = self.train.copy()  \n","        self.sentences = new_train[text_col].values\n","        self.labels = new_train['labels'].values\n","\n","\n","  def prepare_test(self, text_col):\n","    \n","        X_test = pd.read_csv(self.text_data_path + \"/NewTest.csv\")\n","        self.X_test = X_test\n","        X_test['title_desc'] = X_test['Title'] + \" \" + X_test['Description']\n","        self.test_sentences = X_test[text_col].values\n","        return self.test_sentences\n","        "],"execution_count":26,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"8triqwglkYZO"},"source":["# 2.2 Drop Records With No Description"]},{"cell_type":"code","metadata":{"id":"Q_BYtGYwkSPk","executionInfo":{"status":"ok","timestamp":1620603118174,"user_tz":-120,"elapsed":19558,"user":{"displayName":"Sara Fattouh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjGrdvZo2x_8K8abskRubIgMK3SdtS4cG-aQE01nA=s64","userId":"09360594903764573322"}}},"source":["#Load train and test data (test for specific column)\n","\n","text_col = 'title_desc'\n","\n","max_len = 256\n","\n","num_classes = 27\n","\n","Preprocess = SigirPreprocess(text_data_path)\n","\n","Preprocess.prepare_data()\n","train = Preprocess.train\n","# print(\"Trian:  \", len(Preprocess.train))\n","\n","\n","Preprocess.get_sentences(text_col)\n","# print(\"Labels: \", len(Preprocess.labels))\n","\n","# X_test = Preprocess.prepare_test(text_col)\n","# print(\"Test:   \", len(Preprocess.X_test))"],"execution_count":27,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"D1MDFu47PX6-","executionInfo":{"status":"ok","timestamp":1620603164890,"user_tz":-120,"elapsed":66266,"user":{"displayName":"Sara Fattouh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjGrdvZo2x_8K8abskRubIgMK3SdtS4cG-aQE01nA=s64","userId":"09360594903764573322"}},"outputId":"2f2ee38a-5087-421a-b183-fe6a49f31900"},"source":["from bs4 import BeautifulSoup\n","Preprocess.train['Description'] = [BeautifulSoup(text).get_text() for text in  Preprocess.train['Description'] ]\n","Preprocess.train['Title'] = [BeautifulSoup(text).get_text() for text in  Preprocess.train['Title'] ]\n","Preprocess.train['title_desc'] = [BeautifulSoup(text).get_text() for text in  Preprocess.train['title_desc'] ]\n"],"execution_count":28,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/bs4/__init__.py:336: UserWarning: \"https://placehold.it/100x70\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n","  ' that document to Beautiful Soup.' % decoded_markup\n","/usr/local/lib/python3.7/dist-packages/bs4/__init__.py:336: UserWarning: \"https://www.pro-bems.com/IMAGES/images_1/FIGJJCT0000117/m/FIGJJCT0000117_5.jpg\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n","  ' that document to Beautiful Soup.' % decoded_markup\n","/usr/local/lib/python3.7/dist-packages/bs4/__init__.py:273: UserWarning: \"b'.'\" looks like a filename, not markup. You should probably open this file and pass the filehandle into Beautiful Soup.\n","  ' Beautiful Soup.' % markup)\n","/usr/local/lib/python3.7/dist-packages/bs4/__init__.py:336: UserWarning: \"https://www.pro-bems.com/IMAGES/images_1/FIG83X17001/m/FIG83X17001_5.jpg\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n","  ' that document to Beautiful Soup.' % decoded_markup\n","/usr/local/lib/python3.7/dist-packages/bs4/__init__.py:336: UserWarning: \"https://www.pro-bems.com/IMAGES/images/BOOKPNLIGMAG19/m/BOOKPNLIGMAG19_5.jpg\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n","  ' that document to Beautiful Soup.' % decoded_markup\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":741},"id":"P9jF_ASbMB2A","executionInfo":{"status":"ok","timestamp":1620603164892,"user_tz":-120,"elapsed":66256,"user":{"displayName":"Sara Fattouh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjGrdvZo2x_8K8abskRubIgMK3SdtS4cG-aQE01nA=s64","userId":"09360594903764573322"}},"outputId":"42af31b2-83cf-4f15-9614-79305cc663ae"},"source":["from collections import Counter\n","import matplotlib.pyplot as plt\n","\n","\n","counter = Counter(Preprocess.labels)\n","for k,v in counter.items():\n","\tper = v / len(Preprocess.labels) * 100\n","\tprint('Class=%d, n=%d (%.3f%%)' % (k, v, per))\n","\n","print(\"\")\n","# plot the distribution\n","plt.bar(counter.keys(), counter.values())\n","plt.show()"],"execution_count":29,"outputs":[{"output_type":"stream","text":["Class=2, n=1205 (2.190%)\n","Class=4, n=1736 (3.155%)\n","Class=5, n=3792 (6.891%)\n","Class=6, n=2526 (4.591%)\n","Class=7, n=4896 (8.898%)\n","Class=8, n=1578 (2.868%)\n","Class=9, n=4096 (7.444%)\n","Class=3, n=3802 (6.910%)\n","Class=11, n=945 (1.717%)\n","Class=14, n=3872 (7.037%)\n","Class=17, n=4707 (8.554%)\n","Class=12, n=9304 (16.909%)\n","Class=19, n=699 (1.270%)\n","Class=20, n=2140 (3.889%)\n","Class=1, n=320 (0.582%)\n","Class=21, n=2214 (4.024%)\n","Class=22, n=758 (1.378%)\n","Class=18, n=866 (1.574%)\n","Class=23, n=872 (1.585%)\n","Class=24, n=2296 (4.173%)\n","Class=25, n=681 (1.238%)\n","Class=16, n=349 (0.634%)\n","Class=26, n=701 (1.274%)\n","Class=0, n=338 (0.614%)\n","Class=13, n=153 (0.278%)\n","Class=10, n=126 (0.229%)\n","Class=15, n=53 (0.096%)\n","\n"],"name":"stdout"},{"output_type":"display_data","data":{"image/png":"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\n","text/plain":["<Figure size 432x288 with 1 Axes>"]},"metadata":{"tags":[],"needs_background":"light"}}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"qguSL6pMHQve","executionInfo":{"status":"ok","timestamp":1620603165401,"user_tz":-120,"elapsed":66751,"user":{"displayName":"Sara Fattouh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjGrdvZo2x_8K8abskRubIgMK3SdtS4cG-aQE01nA=s64","userId":"09360594903764573322"}},"outputId":"9c8b6c02-59ce-4885-a3d6-eff5ad0bd608"},"source":["print(Preprocess.train['Title'].isnull().sum())\n","\n","print(Preprocess.train['Description'].isnull().sum())\n","\n","print(Preprocess.train['Image_id'].isnull().sum())\n","\n","print(Preprocess.train['Product_id'].isnull().sum())\n","\n","print(Preprocess.train['Prdtypecode'].isnull().sum())\n","\n","print(Preprocess.train['labels'].isnull().sum())\n","\n","print(Preprocess.train['product'].isnull().sum()) #top level category\n","\n","print(Preprocess.train['title_desc'].isnull().sum())\n","\n","\n"],"execution_count":30,"outputs":[{"output_type":"stream","text":["0\n","0\n","0\n","0\n","0\n","0\n","0\n","0\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9gGj4NDbINdl","executionInfo":{"status":"ok","timestamp":1620603165402,"user_tz":-120,"elapsed":66741,"user":{"displayName":"Sara Fattouh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjGrdvZo2x_8K8abskRubIgMK3SdtS4cG-aQE01nA=s64","userId":"09360594903764573322"}},"outputId":"f3f8b515-c1ac-43bd-f8e4-f8c368f66c38"},"source":["Preprocess.train.isnull().values.any()"],"execution_count":31,"outputs":[{"output_type":"execute_result","data":{"text/plain":["False"]},"metadata":{"tags":[]},"execution_count":31}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"8797SZsJIVnP","executionInfo":{"status":"ok","timestamp":1620603165403,"user_tz":-120,"elapsed":66730,"user":{"displayName":"Sara Fattouh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjGrdvZo2x_8K8abskRubIgMK3SdtS4cG-aQE01nA=s64","userId":"09360594903764573322"}},"outputId":"7695eebf-6aa0-484b-9228-2a28cfdd61c3"},"source":["Preprocess.train.isnull().sum().sum()"],"execution_count":32,"outputs":[{"output_type":"execute_result","data":{"text/plain":["0"]},"metadata":{"tags":[]},"execution_count":32}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"jBCwmgodH5wL","executionInfo":{"status":"ok","timestamp":1620603165403,"user_tz":-120,"elapsed":66719,"user":{"displayName":"Sara Fattouh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjGrdvZo2x_8K8abskRubIgMK3SdtS4cG-aQE01nA=s64","userId":"09360594903764573322"}},"outputId":"4cd13ab3-3751-4c01-cdbf-fa73a6f045f3"},"source":["Preprocess.train"],"execution_count":33,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Unnamed: 0</th>\n","      <th>Unnamed: 0.1</th>\n","      <th>Integer_id</th>\n","      <th>Title</th>\n","      <th>Description</th>\n","      <th>Image_id</th>\n","      <th>Product_id</th>\n","      <th>Prdtypecode</th>\n","      <th>labels</th>\n","      <th>product</th>\n","      <th>title_len</th>\n","      <th>desc_len</th>\n","      <th>title_desc_len</th>\n","      <th>title_desc</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0</td>\n","      <td>2</td>\n","      <td>2</td>\n","      <td>Grand Stylet Ergonomique Bleu Gamepad Nintendo...</td>\n","      <td>PILOT STYLE Touch Pen de marque Speedlink est ...</td>\n","      <td>938777978</td>\n","      <td>201115110</td>\n","      <td>50</td>\n","      <td>2</td>\n","      <td>Entertainment</td>\n","      <td>12</td>\n","      <td>109</td>\n","      <td>121</td>\n","      <td>Grand Stylet Ergonomique Bleu Gamepad Nintendo...</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>1</td>\n","      <td>4</td>\n","      <td>4</td>\n","      <td>La Guerre Des Tuques</td>\n","      <td>Luc a des idées de grandeur. Il veut organiser...</td>\n","      <td>1077757786</td>\n","      <td>278535884</td>\n","      <td>2705</td>\n","      <td>4</td>\n","      <td>Books</td>\n","      <td>4</td>\n","      <td>34</td>\n","      <td>38</td>\n","      <td>La Guerre Des Tuques Luc a des idées de grande...</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>2</td>\n","      <td>7</td>\n","      <td>7</td>\n","      <td>Conquérant Sept Cahier Couverture Polypro 240 ...</td>\n","      <td>CONQUERANT CLASSIQUE Cahier 240 x 320 mm seyès...</td>\n","      <td>999581347</td>\n","      <td>344240059</td>\n","      <td>2522</td>\n","      <td>5</td>\n","      <td>Books</td>\n","      <td>14</td>\n","      <td>18</td>\n","      <td>32</td>\n","      <td>Conquérant Sept Cahier Couverture Polypro 240 ...</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>3</td>\n","      <td>9</td>\n","      <td>9</td>\n","      <td>Tente Pliante V3s5-Pro Pvc Blanc - 3 X 4m50 - ...</td>\n","      <td>Tente pliante V3S5 Pro PVC 500 gr/m² - 3 x 4m5...</td>\n","      <td>1245644185</td>\n","      <td>3793572222</td>\n","      <td>2582</td>\n","      <td>6</td>\n","      <td>Household</td>\n","      <td>19</td>\n","      <td>293</td>\n","      <td>312</td>\n","      <td>Tente Pliante V3s5-Pro Pvc Blanc - 3 X 4m50 - ...</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>4</td>\n","      <td>10</td>\n","      <td>10</td>\n","      <td>Eames Inspired Sxw Chair - Pink - Black</td>\n","      <td>The timeless DSW seat can now be paired with m...</td>\n","      <td>1111840281</td>\n","      <td>1915836983</td>\n","      <td>1560</td>\n","      <td>7</td>\n","      <td>Household</td>\n","      <td>8</td>\n","      <td>94</td>\n","      <td>102</td>\n","      <td>Eames Inspired Sxw Chair - Pink - Black The ti...</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>55020</th>\n","      <td>55111</td>\n","      <td>84908</td>\n","      <td>84908</td>\n","      <td>Dimmerable Usb Led Lampe De Bureau Réglable Ch...</td>\n","      <td>Nom de la marque:oobestAmpoules incluses:OuiCe...</td>\n","      <td>1313620762</td>\n","      <td>4198481300</td>\n","      <td>2060</td>\n","      <td>17</td>\n","      <td>Household</td>\n","      <td>18</td>\n","      <td>37</td>\n","      <td>55</td>\n","      <td>Dimmerable Usb Led Lampe De Bureau Réglable Ch...</td>\n","    </tr>\n","    <tr>\n","      <th>55021</th>\n","      <td>55112</td>\n","      <td>84909</td>\n","      <td>84909</td>\n","      <td>espa - kit complet de nage à contre courant 39...</td>\n","      <td>espa espa - kit complet de nage à contre coura...</td>\n","      <td>1043841028</td>\n","      <td>853455937</td>\n","      <td>2583</td>\n","      <td>12</td>\n","      <td>Household</td>\n","      <td>17</td>\n","      <td>173</td>\n","      <td>190</td>\n","      <td>espa - kit complet de nage à contre courant 39...</td>\n","    </tr>\n","    <tr>\n","      <th>55022</th>\n","      <td>55113</td>\n","      <td>84910</td>\n","      <td>84910</td>\n","      <td>Vêtements Pour Animaux Mode Style Chiens Rayé ...</td>\n","      <td>le t - shirt rayé mode chiens  petits chiots v...</td>\n","      <td>1158527239</td>\n","      <td>2699568414</td>\n","      <td>2220</td>\n","      <td>22</td>\n","      <td>Household</td>\n","      <td>12</td>\n","      <td>168</td>\n","      <td>180</td>\n","      <td>Vêtements Pour Animaux Mode Style Chiens Rayé ...</td>\n","    </tr>\n","    <tr>\n","      <th>55023</th>\n","      <td>55114</td>\n","      <td>84912</td>\n","      <td>84912</td>\n","      <td>Kit piscine acier NEVADA déco pierre Ø 3.50m x...</td>\n","      <td>Description complète :Kit piscine hors-sol Toi...</td>\n","      <td>1188462883</td>\n","      <td>3065095706</td>\n","      <td>2583</td>\n","      <td>12</td>\n","      <td>Household</td>\n","      <td>10</td>\n","      <td>190</td>\n","      <td>200</td>\n","      <td>Kit piscine acier NEVADA déco pierre Ø 3.50m x...</td>\n","    </tr>\n","    <tr>\n","      <th>55024</th>\n","      <td>55115</td>\n","      <td>84914</td>\n","      <td>84914</td>\n","      <td>Table Basse Bois De Récupération Massif Base B...</td>\n","      <td>Cette table basse a un design unique et consti...</td>\n","      <td>1267353403</td>\n","      <td>3942400296</td>\n","      <td>1560</td>\n","      <td>7</td>\n","      <td>Household</td>\n","      <td>9</td>\n","      <td>262</td>\n","      <td>271</td>\n","      <td>Table Basse Bois De Récupération Massif Base B...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>55025 rows × 14 columns</p>\n","</div>"],"text/plain":["       Unnamed: 0  ...                                         title_desc\n","0               0  ...  Grand Stylet Ergonomique Bleu Gamepad Nintendo...\n","1               1  ...  La Guerre Des Tuques Luc a des idées de grande...\n","2               2  ...  Conquérant Sept Cahier Couverture Polypro 240 ...\n","3               3  ...  Tente Pliante V3s5-Pro Pvc Blanc - 3 X 4m50 - ...\n","4               4  ...  Eames Inspired Sxw Chair - Pink - Black The ti...\n","...           ...  ...                                                ...\n","55020       55111  ...  Dimmerable Usb Led Lampe De Bureau Réglable Ch...\n","55021       55112  ...  espa - kit complet de nage à contre courant 39...\n","55022       55113  ...  Vêtements Pour Animaux Mode Style Chiens Rayé ...\n","55023       55114  ...  Kit piscine acier NEVADA déco pierre Ø 3.50m x...\n","55024       55115  ...  Table Basse Bois De Récupération Massif Base B...\n","\n","[55025 rows x 14 columns]"]},"metadata":{"tags":[]},"execution_count":33}]},{"cell_type":"markdown","metadata":{"id":"VAqoRpq9zMS4"},"source":["# View Tokenizer Input "]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"lhxfSqOKlR94","executionInfo":{"status":"ok","timestamp":1620603165404,"user_tz":-120,"elapsed":66707,"user":{"displayName":"Sara Fattouh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjGrdvZo2x_8K8abskRubIgMK3SdtS4cG-aQE01nA=s64","userId":"09360594903764573322"}},"outputId":"0d0f59df-1244-4578-f0a2-63cbd220ad2f"},"source":["Preprocess.get_sentences(text_col,True)\n","sentences = Preprocess.sentences\n","\n","labels = Preprocess.labels\n","print(sentences)\n"],"execution_count":34,"outputs":[{"output_type":"stream","text":["[\"Grand Stylet Ergonomique Bleu Gamepad Nintendo Wii U - Speedlink Pilot Style PILOT STYLE Touch Pen de marque Speedlink est 1 stylet ergonomique pour GamePad Nintendo Wii U. Pour un confort optimal et une précision maximale sur le GamePad de la Wii U: ce grand stylet hautement ergonomique est non seulement parfaitement adapté à votre main mais aussi très élégant. Il est livré avec un support qui se fixe sans adhésif à l'arrière du GamePad  Caractéristiques: Modèle: Speedlink PILOT STYLE Touch Pen Couleur: Bleu Ref. Fabricant: SL-3468-BE Compatibilité: GamePad Nintendo Wii U Forme particulièrement ergonomique excellente tenue en main Pointe à revêtement longue durée conçue pour ne pas abîmer l'écran tactile En bonus : Support inclu pour GamePad \"\n"," \"La Guerre Des Tuques Luc a des idées de grandeur. Il veut organiser un jeu de guerre de boules de neige et s'arranger pour en être le vainqueur incontesté. Mais Sophie s'en mêle et chambarde tous ses plans...\"\n"," 'Conquérant Sept Cahier Couverture Polypro 240 X 320 Mm 96 Pages 90g Seyès Incolore CONQUERANT CLASSIQUE Cahier 240 x 320 mm seyès incolorecouverture en Polypro 96 pages agrafé papier de 90 g/m2(400006764)'\n"," ...\n"," 'Vêtements Pour Animaux Mode Style Chiens Rayé T-Shirt Costume Petit Chiot Rouge le t - shirt rayé mode chiens  petits chiots vêtementsnote: veuillez comparer le détail tailles avec toi avant d acheter.!!utiliser les mêmesles vêtements de comparer avec la taille.description:100% brand new la qualité élevéequantité: 1matériel: cotonmotif: levotre chien est plus élégante coolparfait pour la marche  joggingattention: comme différents ordinateurs afficher les couleurs différemment  la couleur de la poste peut varier légèrement d images ci - dessus  merci pour votre compréhension.toutes les dimensions sont mesurées à la main  il y a peut - être 2-3cm déviations  merci pour ta compréhensiontaille des détails:taille: scou: 24 cm / 9 h 45  dos: 23cm / 9.06  bust: 34cm / 13.39  taille: m cou: 28 / 11.02  dos: isbn / 10 63  bust: 40 cm / 15.75taille: lcou: 34cm / 13.39  retour: 31cm / 12.20  buste: 44 / 17.32 potaille: xlcou: 38 cm / 14.96  : 37cm / 14.57 pobust: 51cm / 20.08  taille: xxlcou: 42cm / 16.54 po: 41cm / 16.14  buste: 60 cm / 23.62 poforfait comprend:le t - shirt 1pcs pet'\n"," 'Kit piscine acier NEVADA déco pierre Ø 3.50m x 0.90m Description complète :Kit piscine hors-sol Toi PIEDRA GRIS ronde Ø 3.50m hauteur 0.90m. Parois acier liner 30/100eme uni bleu revêtement breveté exclusif imitant la pierre échelle profilés en PVC. Kit piscine complet.Caractéristiques détaillées :- Forme : Ronde- Type : Kit piscine acier hors-sol- Dimensions extérieures : Ø3.50 x 0.90m- Surface installation : 3.60m x 3.60m- Hauteur avec margelle : 0.90m- Utilisation : Hors-sol- Capacité : 8m3- Kit complet : Oui- Liner : Uni bleu 30/100e avec fixation overlap- Largeur des margelles : Sans- Structure : Parois acier anti-corrosion- Epaisseur des parois : Acier 35/100eme- Jambes de force : Sans jambes de forces apparentes- Revêtement extérieur : Parois laquées avec décoration pierre \"Stone Effect\"- Type de filtration : Filtration à cartouche- Débit : 2m³ / heure- Pompe : 46W- Garantie structure : 2 ans- Garantie liner : 2 ans- Garantie filtration : 2 ans- Echelle : Symétrique acier 2 marches- Notice de montage : Oui- Livraison : 1 palette- Garantie : 2 ans'\n"," \"Table Basse Bois De Récupération Massif Base Blanche 60x60x33cm Cette table basse a un design unique et constituera un ajout intemporel à votre maison. Son dessus de table en bois massif est idéal pour ranger vos boissons panier de fruits ou objets décoratifs et sa base en acier solide ajoute à la robustesse de la table d'appoint. La table basse est faite de bois de récupération massif provenant de solives de planchers et de poutres de soutien de vieux bâtiments en cours de démolition et peut être composée de différents types de bois comme le Sesham (bois de rose) le pin le teck le hêtre le chêne le cèdre le bois de manguier l'acacia etc. Cela signifie que le bois de récupération conserve les caractéristiques de ces différents types de bois. Le bois récupéré est déjà vieilli patiné et séché de sorte qu'il ne rétrécit pas ne se plie pas et n'a pas besoin d'une finition. Chaque étape du processus est réalisée avec le plus grand soin que ce soit le ponçage la peinture ou le laquage. Les belles fibres de bois rendent chaque meuble unique et légèrement différent du suivant. Les signes d'usure et la structure fibreuse visible donnent à chaque pièce son histoire et un aspect unique. L'article est déjà assemblé ; aucun assemblage n'est requis. Remarque importante : les couleurs varient d'un meuble à l'autre rendant chacune de nos tables basses unique la livraison est aléatoire. Couleur de base : BlancMatériau : dessus de table en bois massif de récupération + base en acierDimensions : 60 x 33 cm (Diam. x H)Produit poncé peint et laquéAucun assemblage requis\"]\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"XXlPVw5U1pos","executionInfo":{"status":"ok","timestamp":1620603165405,"user_tz":-120,"elapsed":66696,"user":{"displayName":"Sara Fattouh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjGrdvZo2x_8K8abskRubIgMK3SdtS4cG-aQE01nA=s64","userId":"09360594903764573322"}},"outputId":"ab3e6fe6-92cf-412e-913b-fec7c0f63efc"},"source":["print (type(sentences))\n","print()\n","# print(\"Total number of sentences:{}, labels:{}\".format(len(sentences), len(labels)))"],"execution_count":35,"outputs":[{"output_type":"stream","text":["<class 'numpy.ndarray'>\n","\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"xwZDh0jf9FRc"},"source":["# View Test Sentences"]},{"cell_type":"code","metadata":{"id":"Sjmz7l6rnefT","executionInfo":{"status":"ok","timestamp":1620603165405,"user_tz":-120,"elapsed":66682,"user":{"displayName":"Sara Fattouh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjGrdvZo2x_8K8abskRubIgMK3SdtS4cG-aQE01nA=s64","userId":"09360594903764573322"}}},"source":["# batch_size = 32  \n","\n","# test_sentences = Preprocess.test_sentences\n","\n","# X_test_phase1  = Preprocess.X_test"],"execution_count":36,"outputs":[]},{"cell_type":"code","metadata":{"id":"8QV2kcCt9Oz6","executionInfo":{"status":"ok","timestamp":1620603165406,"user_tz":-120,"elapsed":66676,"user":{"displayName":"Sara Fattouh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjGrdvZo2x_8K8abskRubIgMK3SdtS4cG-aQE01nA=s64","userId":"09360594903764573322"}}},"source":["# print(len(test_sentences))"],"execution_count":37,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"oVemBMlCk4xW"},"source":["**Helper Function**"]},{"cell_type":"code","metadata":{"id":"WV-Xbdjxk3Ii","executionInfo":{"status":"ok","timestamp":1620603165406,"user_tz":-120,"elapsed":66668,"user":{"displayName":"Sara Fattouh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjGrdvZo2x_8K8abskRubIgMK3SdtS4cG-aQE01nA=s64","userId":"09360594903764573322"}}},"source":["def format_time(elapsed):\n","    '''\n","    Takes a time in seconds and returns a string hh:mm:ss\n","    '''\n","    # Round to the nearest second.\n","    elapsed_rounded = int(round((elapsed)))\n","    \n","    # Format as hh:mm:ss\n","    return str(datetime.timedelta(seconds=elapsed_rounded))"],"execution_count":38,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ZIbKzQwxlrZm"},"source":["# 3. Tokenization & Input Formatting\n"," Transform our dataset into the format that BERT can be trained on."]},{"cell_type":"markdown","metadata":{"id":"Ud5qFfdmlwsk"},"source":["# 3.1. BERT Tokenizer\n","To feed our text to BERT, it must be split into tokens, and then these tokens must be mapped to their index in the tokenizer vocabulary.\n","\n","The tokenization must be performed by the tokenizer included within BERT"]},{"cell_type":"code","metadata":{"id":"AbEg-aB2l61a","executionInfo":{"status":"ok","timestamp":1620603167745,"user_tz":-120,"elapsed":68999,"user":{"displayName":"Sara Fattouh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjGrdvZo2x_8K8abskRubIgMK3SdtS4cG-aQE01nA=s64","userId":"09360594903764573322"}}},"source":["from transformers import XLMForSequenceClassification\n","from transformers import FlaubertModel, FlaubertTokenizer,FlaubertForSequenceClassification,AdamW, FlaubertConfig \n","\n","from torch.nn import Dropout,Conv1d, Linear\n","from transformers.modeling_utils import SequenceSummary\n","\n","#from transformers.modeling_roberta import RobertaClassificationHead\n"],"execution_count":39,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"0l3oZ1Tims3q"},"source":["# 3.2. Required Formatting\n","We are required to give it a number of pieces of information\n","\n","We need to:\n","\n","Add special tokens to the start and end of each sentence.\n","Pad & truncate all sentences to a single constant length.\n","Explicitly differentiate real tokens from padding tokens with the “attention mask”."]},{"cell_type":"markdown","metadata":{"id":"fZvQKKcxmu4L"},"source":["# 3.3. Tokenize Dataset\n","We will use \"encode_plus\":\n","\n","returns a dictionary containing the encoded sequence or sequence pair and additional information: the mask for sequence classification and the overflowing elements if a max_length is specified."]},{"cell_type":"code","metadata":{"id":"lSBvH82Amx_3","executionInfo":{"status":"ok","timestamp":1620603167749,"user_tz":-120,"elapsed":68996,"user":{"displayName":"Sara Fattouh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjGrdvZo2x_8K8abskRubIgMK3SdtS4cG-aQE01nA=s64","userId":"09360594903764573322"}}},"source":["def prep_input(sentences,labels, max_len,tokenizer):\n","    input_ids = []\n","    attention_masks = []\n","\n","    # For every sentence...\n","    for sent in sentences:\n","        # `encode_plus` will:\n","        #   (1) Tokenize the sentence.\n","        #   (2) Prepend the `[CLS]` token to the start.\n","        #   (3) Append the `[SEP]` token to the end.\n","        #   (4) Map tokens to their IDs.\n","        #   (5) Pad or truncate the sentence to `max_length`\n","        #   (6) Create attention masks for [PAD] tokens.\n","        encoded_dict = tokenizer.encode_plus(\n","                            sent,                           # Sentence to encode.\n","                            add_special_tokens = True,      # Add '[CLS]' and '[SEP]'\n","                            max_length = max_len,           # Pad & truncate all sentences.\n","                            pad_to_max_length = True,\n","                            return_attention_mask = True,   # Construct attn. masks.\n","                            return_tensors = 'pt',     # Return pytorch tensors.\n","                       )\n","\n","        # Add the encoded sentence to the list.    \n","        input_ids.append(encoded_dict['input_ids'])       # IDs of the the vocabularies in the Model's dictionary\n","\n","        # And its attention mask (simply differentiates padding from non-padding).\n","        attention_masks.append(encoded_dict['attention_mask'])\n","\n","    # Convert the lists into tensors. \n","    input_ids = torch.cat(input_ids, dim=0)             # Concatenates the given sequence of seq tensors in the given dimension. \n","                                                        # All tensors must  have the same shape \n","    attention_masks = torch.cat(attention_masks, dim=0)\n","\n","    if labels is not None:\n","        labels = torch.tensor(labels)\n","        return input_ids, attention_masks, labels\n","    else:\n","        return input_ids, attention_masks"],"execution_count":40,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"KVKdFMMmP8aT"},"source":["# 3.4. Importing Tokenizers and Input Preparation\n","\n","- Now it is time to import both Camembert and FlauBERT tokenizers from  pretained package and prepare the input using them. \n","\n","- Calling prep_input() for each model will result in the corresponding:\n","     \n","\n","1.   **input ids**\n","2.   **attention maks**\n","3.   **labels**\n","\n"," "]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-BYzJChjw5AG","executionInfo":{"status":"ok","timestamp":1620603170497,"user_tz":-120,"elapsed":71736,"user":{"displayName":"Sara Fattouh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjGrdvZo2x_8K8abskRubIgMK3SdtS4cG-aQE01nA=s64","userId":"09360594903764573322"}},"outputId":"4322e92e-6aa5-48cd-e66d-c48c22411a26"},"source":["from transformers import CamembertConfig, CamembertTokenizer, CamembertModel, CamembertForSequenceClassification, AdamW\n","from transformers import FlaubertModel, FlaubertTokenizer,FlaubertForSequenceClassification,AdamW, FlaubertConfig \n","\n","print('Using Camembert')\n","tokenizer_cam = CamembertTokenizer.from_pretrained('camembert-base', do_lowercase=False)\n","print('Using Flaubert')\n","tokenizer_flau = FlaubertTokenizer.from_pretrained('flaubert/flaubert_base_cased', do_lowercase=False)"],"execution_count":41,"outputs":[{"output_type":"stream","text":["Using Camembert\n","Using Flaubert\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"vAvaWiVEwq54","executionInfo":{"status":"ok","timestamp":1620603230024,"user_tz":-120,"elapsed":131251,"user":{"displayName":"Sara Fattouh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjGrdvZo2x_8K8abskRubIgMK3SdtS4cG-aQE01nA=s64","userId":"09360594903764573322"}},"outputId":"6fa4344f-fb75-4f31-ad32-f4c3ff056e2b"},"source":["input_ids_cam, attention_masks_cam, labels_cam = prep_input (sentences, labels, max_len, tokenizer_cam)\n"],"execution_count":42,"outputs":[{"output_type":"stream","text":["Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"o-4ArTutx-Cx","executionInfo":{"status":"ok","timestamp":1620603412139,"user_tz":-120,"elapsed":313354,"user":{"displayName":"Sara Fattouh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjGrdvZo2x_8K8abskRubIgMK3SdtS4cG-aQE01nA=s64","userId":"09360594903764573322"}},"outputId":"8671e486-fc48-4884-8a62-44e3d1b0db95"},"source":["input_ids_flau, attention_masks_flau, labels_flau  = prep_input(sentences,labels, max_len,tokenizer_flau)"],"execution_count":43,"outputs":[{"output_type":"stream","text":["Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"],"name":"stderr"}]},{"cell_type":"markdown","metadata":{"id":"7XrGAY_zRX--"},"source":["# 3.5. Training & Validation Split\n","Divide up our training randomly select **10%** as a validation set off of the training set.\n","\n","While splitting, we used the following parameters:\n","\n","\n","1.   **stratify**: \n","in this context, stratification means that the train_test_split method returns training and test subsets that have the same proportions of class labels as the input dataset.\n","2.   **random_state**: \n","simply sets a seed to the random generator, so that your train-test splits are always deterministic. If you don't set a seed, it is different each time."]},{"cell_type":"code","metadata":{"id":"nZQ0mB1NyeWa","executionInfo":{"status":"ok","timestamp":1620603412143,"user_tz":-120,"elapsed":313345,"user":{"displayName":"Sara Fattouh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjGrdvZo2x_8K8abskRubIgMK3SdtS4cG-aQE01nA=s64","userId":"09360594903764573322"}}},"source":["# val_size = 0.15\n","#basic ---------------\n","\n","# tr_inputs_cam, val_inputs_cam, _,_ = train_test_split (input_ids_cam, labels_cam, stratify = labels_cam,    \n","#                                                             random_state=2020, test_size = val_size)\n","\n","# tr_masks_cam, val_masks_cam, _,_ =   train_test_split (attention_masks_cam, labels, stratify = labels,        # labels: Preprocess.labels\n","#                                                             random_state=2020, test_size = val_size)\n","\n","\n","# tr_inputs_flau, val_inputs_flau, _,_ = train_test_split (input_ids_flau, labels_flau, stratify=labels,\n","#                                                             random_state=2020, test_size = val_size)\n","\n","# tr_masks_flau, val_masks_flau, _,_   = train_test_split (attention_masks_flau, labels,stratify=labels_flau,  # labels: Preprocess.labels\n","#                                                             random_state=2020, test_size = val_size)"],"execution_count":44,"outputs":[]},{"cell_type":"code","metadata":{"id":"R9_fzCmV2JBI","executionInfo":{"status":"ok","timestamp":1620603412143,"user_tz":-120,"elapsed":313338,"user":{"displayName":"Sara Fattouh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjGrdvZo2x_8K8abskRubIgMK3SdtS4cG-aQE01nA=s64","userId":"09360594903764573322"}}},"source":["# tr_inputs, test_inputs_cam, tr_labels, test_labels_cam = train_test_split(input_ids_cam, labels_cam, stratify=labels_cam, random_state=2020,\n","#                                                                 test_size = 0.2)\n","\n","# tr_inputs_cam, val_inputs_cam, train_labels, val_labels = train_test_split(tr_inputs, tr_labels, stratify=tr_labels, random_state=2020,\n","#                                                                 test_size = 0.15)\n","\n","# tr_masks, test_masks_cam, tr_masks_labels, _ =   train_test_split(attention_masks_cam, labels, stratify=labels, random_state=2020,\n","#                                                                  test_size=0.2)\n","\n","# tr_masks_cam, val_masks_cam, u,v =   train_test_split(tr_masks, tr_masks_labels, stratify=tr_masks_labels, random_state=2020,\n","#                                                                 test_size=0.15 )\n","\n","\n","\n","\n","# tr_inputs, test_inputs_flau, tr_labels, test_labels_flau = train_test_split(input_ids_flau, labels_flau, stratify=labels_cam, random_state=2020,\n","#                                                                 test_size = 0.2)\n","\n","# tr_inputs_flau, val_inputs_flau, train_labels, val_labels = train_test_split(tr_inputs, tr_labels, stratify=tr_labels, random_state=2020,\n","#                                                                 test_size = 0.15)\n","\n","# tr_masks, test_masks_flau, tr_masks_labels, _ =   train_test_split(attention_masks_flau, labels, stratify=labels, random_state=2020,\n","#                                                                  test_size=0.2)\n","\n","# tr_masks_flau, val_masks_flau, u,v =   train_test_split(tr_masks, tr_masks_labels, stratify=tr_masks_labels, random_state=2020,\n","#                                                                 test_size=0.15 )"],"execution_count":45,"outputs":[]},{"cell_type":"code","metadata":{"id":"nn6cpPDt2_fW","executionInfo":{"status":"ok","timestamp":1620603412542,"user_tz":-120,"elapsed":313729,"user":{"displayName":"Sara Fattouh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjGrdvZo2x_8K8abskRubIgMK3SdtS4cG-aQE01nA=s64","userId":"09360594903764573322"}}},"source":["tr_inputs, val_inputs_cam, tr_labels, val_labels_cam = train_test_split(input_ids_cam, labels_cam, stratify=labels_cam, random_state=2020,\n","                                                                test_size = 0.2)\n","\n","tr_inputs_cam, test_inputs_cam, train_labels, test_labels = train_test_split(tr_inputs, tr_labels, stratify=tr_labels, random_state=2020,\n","                                                                test_size = 0.15)\n","\n","tr_masks, val_masks_cam, tr_masks_labels, _ =   train_test_split(attention_masks_cam, labels, stratify=labels, random_state=2020,\n","                                                                 test_size=0.2)\n","\n","tr_masks_cam, test_masks_cam, u,v =   train_test_split(tr_masks, tr_masks_labels, stratify=tr_masks_labels, random_state=2020,\n","                                                                test_size=0.15 )\n","\n","\n","\n","\n","tr_inputs, val_inputs_flau, tr_labels, val_labels_flau = train_test_split(input_ids_flau, labels_flau, stratify=labels_cam, random_state=2020,\n","                                                                test_size = 0.2)\n","\n","tr_inputs_flau, test_inputs_flau, train_labels, test_labels = train_test_split(tr_inputs, tr_labels, stratify=tr_labels, random_state=2020,\n","                                                                test_size = 0.15)\n","\n","tr_masks, val_masks_flau, tr_masks_labels, _ =   train_test_split(attention_masks_flau, labels, stratify=labels, random_state=2020,\n","                                                                 test_size=0.2)\n","\n","tr_masks_flau, test_masks_flau, u,v =   train_test_split(tr_masks, tr_masks_labels, stratify=tr_masks_labels, random_state=2020,\n","                                                                test_size=0.15 )"],"execution_count":46,"outputs":[]},{"cell_type":"code","metadata":{"id":"SfiE2yDIzuRO","executionInfo":{"status":"ok","timestamp":1620603413713,"user_tz":-120,"elapsed":314892,"user":{"displayName":"Sara Fattouh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjGrdvZo2x_8K8abskRubIgMK3SdtS4cG-aQE01nA=s64","userId":"09360594903764573322"}}},"source":["torch.save(tr_inputs_cam, \"tr_inputs_cam.pt\")\n","torch.save(val_inputs_cam, \"val_inputs_cam.pt\")\n","torch.save(tr_masks_cam, \"tr_masks_cam.pt\")\n","torch.save(val_masks_cam, \"val_masks_cam.pt\")\n","torch.save(test_inputs_cam, \"test_inputs_cam.pt\")\n","torch.save(test_masks_cam, \"test_masks_cam.pt\")\n","\n","\n","torch.save(tr_inputs_flau, \"tr_inputs_flau.pt\")\n","torch.save(val_inputs_flau, \"val_inputs_flau.pt\")\n","torch.save(tr_masks_flau, \"tr_masks_flau.pt\")\n","torch.save(val_masks_flau, \"val_masks_flau.pt\")\n","torch.save(test_inputs_flau, \"test_inputs_flau.pt\")\n","torch.save(test_masks_flau, \"test_masks_flau.pt\")\n","\n"],"execution_count":47,"outputs":[]},{"cell_type":"code","metadata":{"id":"mdkITHXh0Ahs","executionInfo":{"status":"ok","timestamp":1620603413714,"user_tz":-120,"elapsed":314886,"user":{"displayName":"Sara Fattouh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjGrdvZo2x_8K8abskRubIgMK3SdtS4cG-aQE01nA=s64","userId":"09360594903764573322"}}},"source":["text_input='./'\n","\n","tr_inputs_cam = torch.load(text_input + \"tr_inputs_cam.pt\")\n","val_inputs_cam = torch.load(text_input +\"val_inputs_cam.pt\")\n","tr_masks_cam = torch.load(text_input + \"tr_masks_cam.pt\")\n","val_masks_cam = torch.load(text_input + \"val_masks_cam.pt\")\n","input_ids_test_cam = torch.load(text_input + \"test_inputs_cam.pt\") \n","attention_masks_test_cam = torch.load(text_input + \"test_masks_cam.pt\") \n","\n","tr_inputs_flau = torch.load(text_input + \"tr_inputs_flau.pt\")\n","val_inputs_flau = torch.load(text_input + \"val_inputs_flau.pt\")\n","tr_masks_flau = torch.load(text_input + \"tr_masks_flau.pt\")\n","val_masks_flau = torch.load(text_input + \"val_masks_flau.pt\")\n","input_ids_test_flau = torch.load(text_input + \"test_inputs_flau.pt\")\n","attention_masks_test_flau = torch.load(text_input + \"test_masks_flau.pt\")"],"execution_count":48,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"i1O4EyDzCpsS"},"source":["# 4. Defining Models to be Fused\n","\n","Now, as our  data has been preprocessed, cleaned and text data was tokenzied , it is ready to be fed to the models. \n","- As a first step,  first we need to define and configure the models. \n"]},{"cell_type":"markdown","metadata":{"id":"rlj0Dz1FW7cM"},"source":["# 4.1. RESNet Model for Image Processing. \n","\n","In PyTorch, you always need to define a forward method for your neural network model. But you never have to call it explicitly.\n","Here we are defining our image processing class is subclass of nn.Module and is inheriting all methods. In the super class, nn.Module, there is a __call__ method which obtains the forward function from the subclass and calls it."]},{"cell_type":"markdown","metadata":{"id":"rnnJlj2Y3iii"},"source":["# 4.1.1.  Image Processing Model - RESNet50\n","\n","\n"]},{"cell_type":"code","metadata":{"id":"QWvMo_nV1XCb","executionInfo":{"status":"ok","timestamp":1620603415140,"user_tz":-120,"elapsed":316305,"user":{"displayName":"Sara Fattouh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjGrdvZo2x_8K8abskRubIgMK3SdtS4cG-aQE01nA=s64","userId":"09360594903764573322"}}},"source":["from torch.nn import functional as F\n","import torch.nn as nn\n","import pretrainedmodels\n","class SEResnext50_32x4d(nn.Module):\n","    def __init__(self, pretrained='imagenet'):\n","        super(SEResnext50_32x4d, self).__init__()\n","        \n","        self.base_model = pretrainedmodels.__dict__[\"se_resnext50_32x4d\"](pretrained=None)\n","        if pretrained is not None:\n","            self.base_model.load_state_dict(\n","                self.base_model.load_state_dict(torch.load(resnet_model_path))\n","                )\n","            \n","        self.l0 = nn.Linear(2048, 27)  # Applies a linear transformation to the incoming data\n","        # batch_size = 2048\n","    \n","    def forward(self, image):\n","        batch_size, _, _, _ = image.shape\n","\n","        # During the training you will get batches of images, \n","        # so your shape in the forward method will get an additional batch dimension at dim0: \n","        # [batch_size, channels, height, width].\n","        \n","        x = self.base_model.features(image) \n","\n","        #extracting feature vector from network after feature leaning \n","        #This is the flatten vector \n","\n","        x = F.adaptive_avg_pool2d(x, 1).reshape(batch_size, -1) \n","        #adaptive_avg_pool2d : Kernel size = (input_size+target_size-1) // target_size rounded up\n","        #Then the positions of where to apply the kernel are computed as rounded equidistant points between 0 and input_size - kernel_size\n","        \n","        out = self.l0(x)\n","\n","        return out"],"execution_count":49,"outputs":[]},{"cell_type":"code","metadata":{"id":"NaAYA3QY1cbO","executionInfo":{"status":"ok","timestamp":1620603415141,"user_tz":-120,"elapsed":316297,"user":{"displayName":"Sara Fattouh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjGrdvZo2x_8K8abskRubIgMK3SdtS4cG-aQE01nA=s64","userId":"09360594903764573322"}}},"source":["class Identity(nn.Module):\n","  \n","    def __init__(self):\n","        super(Identity, self).__init__()\n","        \n","    def forward(self, x):\n","        return x"],"execution_count":50,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"vr8ciZGg1Ee4"},"source":["# 4.1.2. Instaniating the Image Processing Network \n"," Now we create an instance from the SEResnext50_32x4d class that we defined and load the weights from a pretrained model, since the training is done previously. "]},{"cell_type":"code","metadata":{"id":"ZGGor-mS1zJe","executionInfo":{"status":"ok","timestamp":1620603415142,"user_tz":-120,"elapsed":316291,"user":{"displayName":"Sara Fattouh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjGrdvZo2x_8K8abskRubIgMK3SdtS4cG-aQE01nA=s64","userId":"09360594903764573322"}}},"source":["#data_path = '/content/drive/My Drive/Rakuten/'\n","\n","img_model = SEResnext50_32x4d(pretrained=None)\n","# img_model.load_state_dict(torch.load(os.path.join(data_path, 'models/RESNET_best_model.pt')))\n","\n","# img_model.cuda()"],"execution_count":51,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"P9gfXO9k21-o"},"source":["# 4.1.3. Prinitng Model's Params"]},{"cell_type":"code","metadata":{"id":"WOiwcopd8m8B","executionInfo":{"status":"ok","timestamp":1620603415142,"user_tz":-120,"elapsed":316284,"user":{"displayName":"Sara Fattouh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjGrdvZo2x_8K8abskRubIgMK3SdtS4cG-aQE01nA=s64","userId":"09360594903764573322"}}},"source":["img_model.l0 = Identity()"],"execution_count":52,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Jb7I6qBq2c0E","executionInfo":{"status":"ok","timestamp":1620603415613,"user_tz":-120,"elapsed":316747,"user":{"displayName":"Sara Fattouh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjGrdvZo2x_8K8abskRubIgMK3SdtS4cG-aQE01nA=s64","userId":"09360594903764573322"}},"outputId":"a9733afb-2d99-4a83-aa07-17ded00036b0"},"source":["for param in img_model.parameters():\n","     print(type(param), param.size())"],"execution_count":53,"outputs":[{"output_type":"stream","text":["<class 'torch.nn.parameter.Parameter'> torch.Size([64, 3, 7, 7])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([64])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([64])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([128, 64, 1, 1])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([128])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([128])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([128, 4, 3, 3])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([128])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([128])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([256, 128, 1, 1])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([256])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([256])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([16, 256, 1, 1])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([16])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([256, 16, 1, 1])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([256])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([256, 64, 1, 1])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([256])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([256])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([128, 256, 1, 1])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([128])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([128])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([128, 4, 3, 3])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([128])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([128])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([256, 128, 1, 1])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([256])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([256])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([16, 256, 1, 1])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([16])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([256, 16, 1, 1])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([256])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([128, 256, 1, 1])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([128])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([128])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([128, 4, 3, 3])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([128])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([128])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([256, 128, 1, 1])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([256])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([256])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([16, 256, 1, 1])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([16])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([256, 16, 1, 1])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([256])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([256, 256, 1, 1])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([256])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([256])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([256, 8, 3, 3])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([256])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([256])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([512, 256, 1, 1])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([512])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([512])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([32, 512, 1, 1])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([32])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([512, 32, 1, 1])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([512])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([512, 256, 1, 1])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([512])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([512])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([256, 512, 1, 1])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([256])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([256])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([256, 8, 3, 3])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([256])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([256])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([512, 256, 1, 1])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([512])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([512])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([32, 512, 1, 1])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([32])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([512, 32, 1, 1])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([512])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([256, 512, 1, 1])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([256])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([256])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([256, 8, 3, 3])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([256])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([256])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([512, 256, 1, 1])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([512])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([512])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([32, 512, 1, 1])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([32])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([512, 32, 1, 1])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([512])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([256, 512, 1, 1])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([256])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([256])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([256, 8, 3, 3])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([256])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([256])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([512, 256, 1, 1])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([512])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([512])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([32, 512, 1, 1])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([32])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([512, 32, 1, 1])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([512])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([512, 512, 1, 1])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([512])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([512])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([512, 16, 3, 3])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([512])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([512])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([1024, 512, 1, 1])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([1024])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([1024])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([64, 1024, 1, 1])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([64])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([1024, 64, 1, 1])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([1024])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([1024, 512, 1, 1])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([1024])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([1024])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([512, 1024, 1, 1])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([512])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([512])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([512, 16, 3, 3])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([512])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([512])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([1024, 512, 1, 1])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([1024])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([1024])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([64, 1024, 1, 1])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([64])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([1024, 64, 1, 1])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([1024])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([512, 1024, 1, 1])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([512])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([512])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([512, 16, 3, 3])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([512])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([512])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([1024, 512, 1, 1])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([1024])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([1024])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([64, 1024, 1, 1])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([64])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([1024, 64, 1, 1])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([1024])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([512, 1024, 1, 1])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([512])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([512])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([512, 16, 3, 3])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([512])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([512])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([1024, 512, 1, 1])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([1024])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([1024])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([64, 1024, 1, 1])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([64])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([1024, 64, 1, 1])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([1024])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([512, 1024, 1, 1])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([512])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([512])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([512, 16, 3, 3])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([512])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([512])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([1024, 512, 1, 1])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([1024])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([1024])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([64, 1024, 1, 1])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([64])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([1024, 64, 1, 1])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([1024])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([512, 1024, 1, 1])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([512])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([512])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([512, 16, 3, 3])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([512])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([512])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([1024, 512, 1, 1])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([1024])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([1024])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([64, 1024, 1, 1])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([64])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([1024, 64, 1, 1])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([1024])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([1024, 1024, 1, 1])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([1024])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([1024])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([1024, 32, 3, 3])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([1024])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([1024])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([2048, 1024, 1, 1])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([2048])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([2048])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([128, 2048, 1, 1])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([128])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([2048, 128, 1, 1])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([2048])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([2048, 1024, 1, 1])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([2048])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([2048])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([1024, 2048, 1, 1])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([1024])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([1024])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([1024, 32, 3, 3])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([1024])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([1024])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([2048, 1024, 1, 1])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([2048])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([2048])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([128, 2048, 1, 1])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([128])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([2048, 128, 1, 1])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([2048])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([1024, 2048, 1, 1])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([1024])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([1024])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([1024, 32, 3, 3])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([1024])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([1024])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([2048, 1024, 1, 1])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([2048])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([2048])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([128, 2048, 1, 1])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([128])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([2048, 128, 1, 1])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([2048])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([1000, 2048])\n","<class 'torch.nn.parameter.Parameter'> torch.Size([1000])\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"9iqaXH5A3Esl"},"source":["# 4.1.4. Model's Params Require No Grads\n","\n","These are just regular tensors, with one very special addition: we tell PyTorch that they require a gradient. This causes PyTorch to record all of the operations done on the tensor, so that it can calculate the gradient during back-propagation automatically!\n","As our model is already trained and weights are assigned, then there is no need to calculate the gradients so no need to send them to the optimizer."]},{"cell_type":"code","metadata":{"id":"c0XJcLgW8u09","executionInfo":{"status":"ok","timestamp":1620603415614,"user_tz":-120,"elapsed":316736,"user":{"displayName":"Sara Fattouh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjGrdvZo2x_8K8abskRubIgMK3SdtS4cG-aQE01nA=s64","userId":"09360594903764573322"}}},"source":["# params are iterators which contain model's parameters. Usually passed to the optimizer\n","for params in img_model.parameters():\n","      params.requires_grad = False"],"execution_count":54,"outputs":[]},{"cell_type":"code","metadata":{"id":"vaXULFfzkdB8","executionInfo":{"status":"ok","timestamp":1620603415615,"user_tz":-120,"elapsed":316729,"user":{"displayName":"Sara Fattouh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjGrdvZo2x_8K8abskRubIgMK3SdtS4cG-aQE01nA=s64","userId":"09360594903764573322"}}},"source":["img_model.out_proj = Identity()"],"execution_count":55,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"JWgGQqbU4nNR"},"source":["# Image Data Preparation"]},{"cell_type":"code","metadata":{"id":"4yNKxOJ2-QrA","executionInfo":{"status":"ok","timestamp":1620603415615,"user_tz":-120,"elapsed":316722,"user":{"displayName":"Sara Fattouh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjGrdvZo2x_8K8abskRubIgMK3SdtS4cG-aQE01nA=s64","userId":"09360594903764573322"}}},"source":["# # Data path\n","# text_data_path = os.path.join('/content/drive/My Drive/Rakuten')\n","# image_data_path = os.path.join('')\n"],"execution_count":56,"outputs":[]},{"cell_type":"code","metadata":{"id":"SM6bWAoKTsx7","executionInfo":{"status":"ok","timestamp":1620603415616,"user_tz":-120,"elapsed":316715,"user":{"displayName":"Sara Fattouh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjGrdvZo2x_8K8abskRubIgMK3SdtS4cG-aQE01nA=s64","userId":"09360594903764573322"}}},"source":["def get_img_path(img_id, prd_id, path):\n","    \n","    pattern = 'image'+'_'+str(img_id)+'_'+'product'+'_'+str(prd_id)+'.jpg'\n","    return path + pattern"],"execution_count":57,"outputs":[]},{"cell_type":"code","metadata":{"id":"oYomnrt7wKWw","executionInfo":{"status":"ok","timestamp":1620603415616,"user_tz":-120,"elapsed":316690,"user":{"displayName":"Sara Fattouh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjGrdvZo2x_8K8abskRubIgMK3SdtS4cG-aQE01nA=s64","userId":"09360594903764573322"}}},"source":["# print(len(Preprocess.train), len(train))"],"execution_count":58,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"QzgyQu0aY17J"},"source":["# Obtaining & Splitting Images "]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":185,"referenced_widgets":["014f925fc8da4ac0a39f7f8d9492bb5e","8f7ba449617443c28b070094dddd31f3","afd17b52771e41e6a10107d5d6a11f3b","f3508a41adc749628755fd248ba7b858","ecd4e16d696a46f488f918fdc8815d1a","1b3834f417454cf7b3cdc6bcfbadaaf8","43c4c006766a494bb74ff59c012c332e","6c862f0ed686478885bf33bed99f576e"]},"id":"emmAR45klxoi","executionInfo":{"status":"ok","timestamp":1620603416754,"user_tz":-120,"elapsed":317812,"user":{"displayName":"Sara Fattouh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjGrdvZo2x_8K8abskRubIgMK3SdtS4cG-aQE01nA=s64","userId":"09360594903764573322"}},"outputId":"176f0ae8-6095-4fe2-a554-f1c625d79c08"},"source":["from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n","\n","#  train 65%\n","#  validation 15%\n","#  test 20%\n","# (original training) =>  train, test => 80, 20  val_size = 0.2\n","# (train) => train, val =>  85, 15   val_size = 0.15 \n","\n","\n","train_img = train[['Image_id','Product_id','labels','product']]\n","\n","train_img['image_path'] = Preprocess.train.progress_apply(lambda x: get_img_path(x['Image_id'], x['Product_id'],\n","                                                      path = os.path.join(image_data_path, 'image_training/')),axis=1)\n","\n","\n","\n","\n","tr_df, val_df, tr_labels, val_labels = train_test_split(train_img, train_img['labels'], \n","                                           random_state=2020,\n","                                           test_size = 0.2,\n","                                           stratify=train_img['labels'])\n","\n","\n","train_df, test_df, train_labels, test_labels = train_test_split(tr_df, tr_labels, \n","                                           random_state=2020,\n","                                           test_size = 0.15,\n","                                           stratify=tr_labels)\n","\n","\n","\n","# print(\"Train: \", len(train_df))\n","# print(\"Val:   \", len(val_df))\n","# print(\"Test:  \", len(test_df))\n","print (\"\")"],"execution_count":59,"outputs":[{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"014f925fc8da4ac0a39f7f8d9492bb5e","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=0.0, max=55025.0), HTML(value='')))"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["\n","\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:13: SettingWithCopyWarning: \n","A value is trying to be set on a copy of a slice from a DataFrame.\n","Try using .loc[row_indexer,col_indexer] = value instead\n","\n","See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n","  del sys.path[0]\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"B_h5_e_Gw-M9","executionInfo":{"status":"ok","timestamp":1620603416755,"user_tz":-120,"elapsed":317788,"user":{"displayName":"Sara Fattouh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjGrdvZo2x_8K8abskRubIgMK3SdtS4cG-aQE01nA=s64","userId":"09360594903764573322"}}},"source":["# print(\"Original Images Df:   \",  len(train_img))\n","# print(\"Train Images DF:      \" , len(train_df))\n","# print(\"Validation Images DF: \",  len(val_df))"],"execution_count":60,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"aIAzQAsCTzBs","executionInfo":{"status":"ok","timestamp":1620603416755,"user_tz":-120,"elapsed":317774,"user":{"displayName":"Sara Fattouh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjGrdvZo2x_8K8abskRubIgMK3SdtS4cG-aQE01nA=s64","userId":"09360594903764573322"}},"outputId":"9955d21a-d2c3-4366-9040-2e10efb763f1"},"source":["print (train_img['image_path'][0])\n"],"execution_count":61,"outputs":[{"output_type":"stream","text":["/content/Rakuten/image/image_training/image_938777978_product_201115110.jpg\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"lRigVJwVBOcz"},"source":["# Image Data Augmentation\n","\n","We're going to be making use of Pytorch's transforms for preparing the input images to be used by our model. \n","\n","\n","\n","\n","\n","*   We'll need to make sure the images in the training set and validation set are the same size, so we'll be using transforms.Resize\n","*   We'll also be doing a little data augmentation, trying to improve the performance of our model by forcing it to learn about images at different angles and crops, so we'll randomly crop and rotate the images.\n","\n","*    we'll make tensors out of the images, as PyTorch works with tensors. \n","*   Finally, we'll normalize the images, which helps the network work with values that may be have a wide range of different values.\n","\n","\n","*   We then compose all our chosen transforms.\n","\n","It worth mentioning that validation transforms don't have any of the flipping or rotating, as they aren't part of our training set, so the network isn't learning about them\n","\n","\n","\n","\n","\n"]},{"cell_type":"code","metadata":{"id":"-Togfn7XmpW5","executionInfo":{"status":"ok","timestamp":1620603416756,"user_tz":-120,"elapsed":317752,"user":{"displayName":"Sara Fattouh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjGrdvZo2x_8K8abskRubIgMK3SdtS4cG-aQE01nA=s64","userId":"09360594903764573322"}}},"source":["input_size = 224 # for Resnt\n","\n","# Applying Transforms to the Data\n","\n","from torchvision import datasets, models, transforms\n","\n","image_transforms = { \n","    'train': transforms.Compose([\n","        transforms.RandomResizedCrop(size=256, scale=(0.8, 1.0)),\n","        transforms.RandomRotation(degrees=15),\n","        transforms.RandomHorizontalFlip(),\n","        transforms.Resize(size=256),\n","        transforms.CenterCrop(size=input_size),\n","        transforms.ToTensor(),\n","        transforms.Normalize([0.485, 0.456, 0.406],\n","                             [0.229, 0.224, 0.225])\n","    ]),\n","    'valid': transforms.Compose([\n","        transforms.Resize(size=256),\n","        transforms.CenterCrop(size=input_size),\n","        transforms.ToTensor(),\n","        transforms.Normalize([0.485, 0.456, 0.406],\n","                             [0.229, 0.224, 0.225])\n","    ]),\n","    'test': transforms.Compose([\n","        transforms.Resize(size=256),\n","        transforms.CenterCrop(size=input_size),\n","        transforms.ToTensor(),\n","        transforms.Normalize([0.485, 0.456, 0.406],\n","                             [0.229, 0.224, 0.225])\n","    ])\n","}"],"execution_count":62,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"nB9-K4lUNoqT"},"source":["# Text Processing Models - BertForSequenceClassification\n","\n","Thankfully, the huggingface pytorch implementation includes a set of interfaces designed for a variety of NLP tasks. Though these interfaces are all built on top of a trained BERT model, each has different top layers and output types designed to accomodate their specific NLP task.\n","\n","We first want to modify the pre-trained BERT model to give outputs for classification, and then we want to continue training the model on our dataset until that the entire model, end-to-end, is well-suited for our task.\n","\n","**BertForSequenceClassification** is one of the current of classes provided for fine-tuning.\n","\n","This is the normal BERT model with an added single linear layer on top for classification that we will use as a sentence classifier. As we feed input data, the entire pre-trained BERT model and the additional untrained classification layer is trained on our specific task.\n","\n","- Not to forget that Camembet model inherits RobertaModel"]},{"cell_type":"markdown","metadata":{"id":"I_vkDiuRC70z"},"source":["# 4.2 CamemBERT Model"]},{"cell_type":"code","metadata":{"id":"imOtgakyCtFe","executionInfo":{"status":"ok","timestamp":1620603416756,"user_tz":-120,"elapsed":317739,"user":{"displayName":"Sara Fattouh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjGrdvZo2x_8K8abskRubIgMK3SdtS4cG-aQE01nA=s64","userId":"09360594903764573322"}}},"source":["class vec_output_CamembertForSequenceClassification(CamembertModel):\n","  \n","    config_class = CamembertConfig\n","\n","    def __init__(self, config):\n","        super().__init__(config)\n","        self.num_labels = config.num_labels\n","\n","        self.roberta = CamembertModel(config)\n","        self.dense = nn.Linear(256*config.hidden_size, config.hidden_size)\n","        self.dropout = nn.Dropout(0.1)\n","        self.out_proj = nn.Linear(config.hidden_size, config.num_labels)\n","        self.init_weights()\n","\n","\n","    def forward(\n","        self,\n","        input_ids=None,\n","        attention_mask=None,\n","        token_type_ids=None,\n","        position_ids=None,\n","        head_mask=None,\n","        inputs_embeds=None,\n","        labels=None,\n","        output_attentions=None,\n","        output_hidden_states=None,\n","    ):\n","        outputs = self.roberta(\n","            input_ids,\n","            attention_mask = attention_mask,\n","            token_type_ids = token_type_ids,\n","            position_ids = position_ids,\n","            head_mask = head_mask,\n","            inputs_embeds=inputs_embeds,\n","#           output_attentions=output_attentions,\n","#           output_hidden_states=output_hidden_states,\n","        )\n","\n","        sequence_output = outputs[0] #(B,256,768)\n","\n","        x = sequence_output.view(sequence_output.shape[0], 256*768)\n","\n","#       x = sequence_output[:, 0, :]  # take <s> token (equiv. to [CLS])-> #(B,768) Image -> (B,2048)\n","\n","        x = self.dense(x)  # 768 -> 768\n","\n","        feat= torch.tanh(x) \n","\n","        logits = self.out_proj(feat) # 768 -> 27\n","\n","        outputs = (logits,) + outputs[2:] #3rd element onwards\n","\n","        return outputs,feat  # (loss), logits, (hidden_states), (attentions)"],"execution_count":63,"outputs":[]},{"cell_type":"code","metadata":{"id":"6KstmQqyd04x","executionInfo":{"status":"ok","timestamp":1620603416757,"user_tz":-120,"elapsed":317727,"user":{"displayName":"Sara Fattouh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjGrdvZo2x_8K8abskRubIgMK3SdtS4cG-aQE01nA=s64","userId":"09360594903764573322"}}},"source":["camembert_model_path = '/content/Rakuten/models/CamemBERT_best_model_description.pt'"],"execution_count":64,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"OtPoFtaqDAj6"},"source":["# FlauBERT Model"]},{"cell_type":"code","metadata":{"id":"a16smoYhDCmn","executionInfo":{"status":"ok","timestamp":1620603417104,"user_tz":-120,"elapsed":318061,"user":{"displayName":"Sara Fattouh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjGrdvZo2x_8K8abskRubIgMK3SdtS4cG-aQE01nA=s64","userId":"09360594903764573322"}}},"source":["num_classes = 27\n","\n","class vec_output_FlaubertForSequenceClassification(FlaubertModel):\n","    \n","    config_class = FlaubertConfig\n","    \n","\n","    def __init__(self, config):\n","        super().__init__(config)\n","        self.transformer = FlaubertModel(config)\n","        self.sequence_summary = SequenceSummary(config)\n","        self.init_weights()\n","        self.dropout =  torch.nn.Dropout(0.1)\n","        self.classifier = torch.nn.Linear(config.hidden_size, num_classes)\n","\n","\n","    def forward(\n","        self,\n","        input_ids=None,\n","        attention_mask=None,\n","        langs=None,\n","        token_type_ids=None,\n","        position_ids=None,\n","        lengths=None,\n","        cache=None,\n","        head_mask=None,\n","        inputs_embeds=None,\n","        labels=None,\n","    ):\n","        \n","        \n","        transformer_outputs = self.transformer(\n","            input_ids,\n","            attention_mask = attention_mask,\n","            langs = langs,\n","            token_type_ids = token_type_ids,\n","            position_ids = position_ids,\n","            lengths = lengths,\n","            cache = cache,\n","            head_mask = head_mask,\n","            inputs_embeds = inputs_embeds,\n","        )\n","\n","        #output = self.dropout(output)\n","        output = transformer_outputs[0] \n","        vec = output[:,0]\n","        \n","        \n","        #logits\n","        dense = self.dropout(vec)\n","        \n","        #classifier\n","        logits = self.classifier(dense)\n","        \n","        outputs = (logits,) + transformer_outputs[1:]  # Keep new_mems and attention/hidden states if they are here\n","       \n","        \n","        return outputs,dense"],"execution_count":65,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"b2krGO8BnVfd"},"source":["# Dataset Fusion"]},{"cell_type":"code","metadata":{"id":"qIIQ5-g3gU85","executionInfo":{"status":"ok","timestamp":1620603417105,"user_tz":-120,"elapsed":318048,"user":{"displayName":"Sara Fattouh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjGrdvZo2x_8K8abskRubIgMK3SdtS4cG-aQE01nA=s64","userId":"09360594903764573322"}}},"source":["# TODO DELELTE IMAGES WITH NO DESCRIPTION\n","# From the preprocesssed file"],"execution_count":66,"outputs":[]},{"cell_type":"code","metadata":{"id":"RRq24YrsnU9X","executionInfo":{"status":"ok","timestamp":1620603417106,"user_tz":-120,"elapsed":318037,"user":{"displayName":"Sara Fattouh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjGrdvZo2x_8K8abskRubIgMK3SdtS4cG-aQE01nA=s64","userId":"09360594903764573322"}}},"source":["from torch.utils.data import Dataset, DataLoader, Subset\n","from PIL import Image\n","import matplotlib.pyplot as plt\n","import cv2\n","\n","class FusionDataset(Dataset):\n","    \n","    def __init__(self, df, inputs_cam, masks_cam, inputs_flau, masks_flau, transform=None, mode='train'):\n","        self.df = df\n","        self.transform   = transform\n","        self.mode = mode\n","\n","        self.inputs_cam  = inputs_cam\n","        self.masks_cam   = masks_cam\n","\n","        self.inputs_flau  = inputs_flau\n","        self.masks_flau   = masks_flau\n","         \n","    def __len__(self):\n","        return len(self.df)\n","    \n","    def __getitem__(self,idx):\n","        \n","        im_path = self.df.iloc[idx]['image_path']\n","        img= plt.imread(im_path)\n","        #img = cv2.imread(im_path)\n","        #img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n","        img = Image.fromarray(img)\n","\n","        if self.transform is not None:\n","            img = self.transform(img)\n","\n","        img              = img.cuda()\n","        input_id_cam     = self.inputs_cam[idx].cuda()\n","        input_mask_cam   = self.masks_cam[idx].cuda()\n","        input_id_flau    = self.inputs_flau[idx].cuda()\n","        input_mask_flau  = self.masks_flau[idx].cuda()\n","        \n","        if self.mode =='test':\n","            return img,input_id_cam,input_mask_cam,input_id_flau,input_mask_flau\n","            \n","        else:\n","            labels = torch.tensor(self.df.iloc[idx]['labels']).cuda()             \n","            return img,input_id_cam,input_mask_cam,input_id_flau,input_mask_flau,labels"],"execution_count":67,"outputs":[]},{"cell_type":"code","metadata":{"id":"O5RRyCzd4XSd","executionInfo":{"status":"ok","timestamp":1620603417106,"user_tz":-120,"elapsed":318023,"user":{"displayName":"Sara Fattouh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjGrdvZo2x_8K8abskRubIgMK3SdtS4cG-aQE01nA=s64","userId":"09360594903764573322"}}},"source":["a1 = torch.randn(3,10,10)"],"execution_count":68,"outputs":[]},{"cell_type":"code","metadata":{"id":"o_51PvBY4Zpr","executionInfo":{"status":"ok","timestamp":1620603417106,"user_tz":-120,"elapsed":318008,"user":{"displayName":"Sara Fattouh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjGrdvZo2x_8K8abskRubIgMK3SdtS4cG-aQE01nA=s64","userId":"09360594903764573322"}}},"source":["reduce_dim = nn.Conv1d(in_channels = 10 , out_channels = 1 , kernel_size= 1)"],"execution_count":69,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"wryxtip-4b0m","executionInfo":{"status":"ok","timestamp":1620603417107,"user_tz":-120,"elapsed":317996,"user":{"displayName":"Sara Fattouh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjGrdvZo2x_8K8abskRubIgMK3SdtS4cG-aQE01nA=s64","userId":"09360594903764573322"}},"outputId":"04c357ec-2fb7-4636-910e-4eb3c5a7b9ed"},"source":["reduce_dim(a1).view(3,10).shape"],"execution_count":70,"outputs":[{"output_type":"execute_result","data":{"text/plain":["torch.Size([3, 10])"]},"metadata":{"tags":[]},"execution_count":70}]},{"cell_type":"markdown","metadata":{"id":"Uvw1I4sNSJ9_"},"source":["# Test Sentences Tokenization"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"hTUa-CbEnluo","executionInfo":{"status":"ok","timestamp":1620603419808,"user_tz":-120,"elapsed":320682,"user":{"displayName":"Sara Fattouh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjGrdvZo2x_8K8abskRubIgMK3SdtS4cG-aQE01nA=s64","userId":"09360594903764573322"}},"outputId":"d63210c7-e022-4f09-f141-86c9283cfd2f"},"source":["print('Using Camembert')\n","tokenizer_cam = CamembertTokenizer.from_pretrained('camembert-base', do_lowercase=False)\n","\n","print('Using Flaubert')\n","tokenizer_flau = FlaubertTokenizer.from_pretrained('flaubert/flaubert_base_cased', do_lowercase=False)\n","\n","# input_ids_test_flau,attention_masks_test_flau = prep_input(test_sentences, labels=None, max_len=max_len,tokenizer = tokenizer_flau)\n","\n","# input_ids_test_cam,attention_masks_test_cam = prep_input(test_sentences , labels=None, max_len=max_len,tokenizer = tokenizer_cam)"],"execution_count":71,"outputs":[{"output_type":"stream","text":["Using Camembert\n","Using Flaubert\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"e8gijJIM5pQX","executionInfo":{"status":"ok","timestamp":1620603419811,"user_tz":-120,"elapsed":320672,"user":{"displayName":"Sara Fattouh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjGrdvZo2x_8K8abskRubIgMK3SdtS4cG-aQE01nA=s64","userId":"09360594903764573322"}}},"source":["# print(type(Preprocess.test_sentences))\n","# print(len(Preprocess.test_sentences))"],"execution_count":72,"outputs":[]},{"cell_type":"code","metadata":{"id":"HXbLhSkyCilV","executionInfo":{"status":"ok","timestamp":1620603419811,"user_tz":-120,"elapsed":320664,"user":{"displayName":"Sara Fattouh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjGrdvZo2x_8K8abskRubIgMK3SdtS4cG-aQE01nA=s64","userId":"09360594903764573322"}}},"source":["# # Moodels path \n","# resnet_model_path = '/content/Rakuten/models/RESNET_best_model.pt'\n","# camembert_model_path = '/content/Rakuten/models/CamemBERT_best_model_title_description.pt' ###### TODO Change with the updated model!!!\n","# flaubert_model_path = '/content/Rakuten/models/FlauBERT_best_model_title_description.pt'\n","\n","#my_flau_path  = '/content/Rakuten/models/FlauBERT_best_model_description.pt'\n"],"execution_count":73,"outputs":[]},{"cell_type":"code","metadata":{"id":"LzlptrSui4af","executionInfo":{"status":"ok","timestamp":1620603419812,"user_tz":-120,"elapsed":320657,"user":{"displayName":"Sara Fattouh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjGrdvZo2x_8K8abskRubIgMK3SdtS4cG-aQE01nA=s64","userId":"09360594903764573322"}}},"source":["# Moodels path \n","resnet_model_path = '/content/Rakuten/models/RESNET_baseline_model.pt'\n","\n","camembert_model_path_title = '/content/Rakuten/models/CamemBERT_best_model_split_title.pt'\n","camembert_model_path_desc = '/content/Rakuten/models/CamemBERT_best_model_split_description.pt'\n","\n","flaubert_model_path_title = '/content/Rakuten/models/FlauBERT_best_model_split_title.pt'\n","flaubert_model_path_desc = '/content/Rakuten/models/FlauBERT_best_model_split_description.pt'\n"],"execution_count":74,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"09NiYnUxfi94"},"source":["# Fuse\n"," When using pretrained models, PyTorch sets the model to be unfrozen (will have its weights adjusted) by default"]},{"cell_type":"code","metadata":{"id":"TxW8Ups_nr8O","executionInfo":{"status":"ok","timestamp":1620603419812,"user_tz":-120,"elapsed":320650,"user":{"displayName":"Sara Fattouh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjGrdvZo2x_8K8abskRubIgMK3SdtS4cG-aQE01nA=s64","userId":"09360594903764573322"}}},"source":["class vector_fusion(nn.Module):    \n","    def __init__(self):\n","        super(vector_fusion, self).__init__()\n","\n","        self.img_model = SEResnext50_32x4d(pretrained=None)\n","        self.img_model.load_state_dict(torch.load(resnet_model_path))\n","        self.img_model.l0=Identity()\n","        for params in self.img_model.parameters():\n","            params.requires_grad=False\n","\n","# ------ CamamBERT ------\n","\n","        self.cam_model_title = vec_output_CamembertForSequenceClassification.from_pretrained(\n","         'camembert-base', # Use the 12-layer BERT model, with an uncased vocab.\n","          num_labels = 27, # The number of output labels--2 for binary classification.\n","                    # You can increase this for multi-class tasks.   \n","          output_attentions = False, # Whether the model returns attentions weights.\n","          output_hidden_states = False,) # Whether the model returns all hidden-states.\n","        \n","        \n","        checkpoint = torch.load(camembert_model_path_title)\n","        self.cam_model_title.load_state_dict(checkpoint)\n","\n","        for param in self.cam_model_title.parameters():\n","            param.requires_grad=False\n","\n","        self.cam_model_title.out_proj = Identity()\n","\n","\n","        self.cam_model_desc = vec_output_CamembertForSequenceClassification.from_pretrained(\n","         'camembert-base', # Use the 12-layer BERT model, with an uncased vocab.\n","          num_labels = 27, # The number of output labels--2 for binary classification.\n","                    # You can increase this for multi-class tasks.   \n","          output_attentions = False, # Whether the model returns attentions weights.\n","          output_hidden_states = False,) # Whether the model returns all hidden-states.\n","        \n","        \n","        checkpoint = torch.load(camembert_model_path_desc)\n","        self.cam_model_desc.load_state_dict(checkpoint)\n","\n","        for param in self.cam_model_desc.parameters():\n","            param.requires_grad=False\n","\n","        self.cam_model_desc.out_proj = Identity()\n","\n"," # ----  FlauBERT ----- \n","\n","        \n","        self.flau_model_title = vec_output_FlaubertForSequenceClassification.from_pretrained(\n","        'flaubert/flaubert_base_cased', \n","        num_labels = 27, \n","        output_attentions = False,\n","        output_hidden_states = False,)\n","        checkpoint = torch.load(flaubert_model_path_title)\n","\n","        self.flau_model_title.load_state_dict(checkpoint)\n","\n","        for param in self.flau_model_title.parameters():\n","            param.requires_grad=False\n","\n","        self.flau_model_title.classifier=Identity()\n","\n","\n","      \n","        self.flau_model_desc = vec_output_FlaubertForSequenceClassification.from_pretrained(\n","        'flaubert/flaubert_base_cased', \n","        num_labels = 27, \n","        output_attentions = False,\n","        output_hidden_states = False,)\n","        checkpoint = torch.load(flaubert_model_path_desc)\n","\n","        self.flau_model_desc.load_state_dict(checkpoint)\n","\n","        for param in self.flau_model_desc.parameters():\n","            param.requires_grad=False\n","\n","        self.flau_model_desc.classifier=Identity()\n","\n","\n","# ------------------------------------------------------------\n","\n","        self.reduce_dim = nn.Conv1d(in_channels = 2048 , out_channels = 768 , kernel_size= 1)\n","        self.reduce_dim2 = nn.Conv1d(in_channels = 768 , out_channels = 1 , kernel_size= 1)\n","        self.out = nn.Linear(768, 27)\n","        \n","        #gamma\n","#         self.w1 = nn.Parameter(torch.zeros(1))\n","#         self.w2 = nn.Parameter(torch.zeros(1))\n","#         self.w3 = nn.Parameter(torch.zeros(1))\n","        \n","    def forward(self,img,input_id_cam,input_mask_cam,input_id_flau,input_mask_flau):\n","        \n","        cam_emb_title,vec1_title = self.cam_model_title(input_id_cam, \n","                     token_type_ids=None, \n","                     attention_mask=input_mask_cam)\n","        \n","        cam_emb_desc,vec1_desc = self.cam_model_desc(input_id_cam, \n","                     token_type_ids=None, \n","                     attention_mask=input_mask_cam)\n","        \n","\n","#---------------------------------\n","        \n","        flau_emb_title,vec2 =self.flau_model_title(input_id_flau, \n","                     token_type_ids=None, \n","                     attention_mask=input_mask_flau)\n","        \n","        flau_emb_desc,vec2_desc =self.flau_model_desc(input_id_flau, \n","                     token_type_ids=None, \n","                     attention_mask=input_mask_flau)\n","        \n","# ---------------------------------\n","        \n","        #Projecting the image embedding to lower dimension\n","        img_emb = self.img_model(img)\n","        \n","        img_emb = img_emb.view(img_emb.shape[0],img_emb.shape[1],1)\n","        img_emb = self.reduce_dim(img_emb)\n","        img_emb = img_emb.view(img_emb.shape[0],img_emb.shape[1]) ###### bs * 768 \n","# --------------------------------\n","\n","        #summing up the vectors\n","        cam_emb  = cam_emb_title + cam_emb_desc\n","        flau_emb = flau_emb_title + flau_emb_desc\n","        \n","        #Bilinear\n","        #text_emb = text_emb.view(text_emb.shape[0],1,text_emb.shape[1])  ##### bs * 1 * 768\n","        \n","        #Bilinear Pooling\n","        #pool_emb = torch.bmm(img_emb,text_emb) ### bs * 768 * 768\n","        #pool_emb = self.reduce_dim2(pool_emb).view(text_emb.shape[0],768)  #### bs * 1 * 768\n","\n","        fuse= img_emb + cam_emb[0] + flau_emb[0]\n","        fuse =   fuse /3\n","        # print(\"fusion size\", fuse.shape)  # 128\n","       \n","        logits = self.out(fuse)\n","\n","        # print(\"returned logits shape: \", logits.shape)\n","        return logits"],"execution_count":75,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"J1VU1vrSQFzD","executionInfo":{"status":"ok","timestamp":1620603422866,"user_tz":-120,"elapsed":323695,"user":{"displayName":"Sara Fattouh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjGrdvZo2x_8K8abskRubIgMK3SdtS4cG-aQE01nA=s64","userId":"09360594903764573322"}},"outputId":"459b71a9-239c-4e67-e37c-b7a71dc22943"},"source":[" img_model = SEResnext50_32x4d(pretrained=None)\n"," img_model.load_state_dict(torch.load(resnet_model_path))"],"execution_count":76,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<All keys matched successfully>"]},"metadata":{"tags":[]},"execution_count":76}]},{"cell_type":"code","metadata":{"id":"n4LM7TANsMqJ","executionInfo":{"status":"ok","timestamp":1620603422867,"user_tz":-120,"elapsed":323683,"user":{"displayName":"Sara Fattouh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjGrdvZo2x_8K8abskRubIgMK3SdtS4cG-aQE01nA=s64","userId":"09360594903764573322"}}},"source":["# cam_title= vec_output_CamembertForSequenceClassification.from_pretrained(\n","#          'camembert-base', # Use the 12-layer BERT model, with an uncased vocab.\n","#           num_labels = 27, # The number of output labels--2 for binary classification.\n","#                     # You can increase this for multi-class tasks.   \n","#           output_attentions = False, # Whether the model returns attentions weights.\n","#           output_hidden_states = False,) # Whether the model returns all hidden-states.\n","        \n","        \n","# checkpoint = torch.load(camembert_model_path_title)\n","# cam_title.load_state_dict(checkpoint)"],"execution_count":77,"outputs":[]},{"cell_type":"code","metadata":{"id":"W0iQCIXYsTlf","executionInfo":{"status":"ok","timestamp":1620603422868,"user_tz":-120,"elapsed":323676,"user":{"displayName":"Sara Fattouh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjGrdvZo2x_8K8abskRubIgMK3SdtS4cG-aQE01nA=s64","userId":"09360594903764573322"}}},"source":["# cam_desc= vec_output_CamembertForSequenceClassification.from_pretrained(\n","#          'camembert-base', # Use the 12-layer BERT model, with an uncased vocab.\n","#           num_labels = 27, # The number of output labels--2 for binary classification.\n","#                     # You can increase this for multi-class tasks.   \n","#           output_attentions = False, # Whether the model returns attentions weights.\n","#           output_hidden_states = False,) # Whether the model returns all hidden-states.\n","        \n","        \n","# checkpoint = torch.load(camembert_model_path_desc)\n","# cam_desc.load_state_dict(checkpoint)"],"execution_count":78,"outputs":[]},{"cell_type":"code","metadata":{"id":"FZfncq4PO_EY","executionInfo":{"status":"ok","timestamp":1620603422868,"user_tz":-120,"elapsed":323668,"user":{"displayName":"Sara Fattouh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjGrdvZo2x_8K8abskRubIgMK3SdtS4cG-aQE01nA=s64","userId":"09360594903764573322"}}},"source":["# flau_title = vec_output_FlaubertForSequenceClassification.from_pretrained(\n","#         'flaubert/flaubert_base_cased', \n","#         num_labels = 27, \n","#         output_attentions = False,\n","#         output_hidden_states = False,)\n","\n","\n","# checkpoint = torch.load(flaubert_model_path_title)\n","# flau_title.load_state_dict(checkpoint)"],"execution_count":79,"outputs":[]},{"cell_type":"code","metadata":{"id":"Qo090Yeusw5V","executionInfo":{"status":"ok","timestamp":1620603422868,"user_tz":-120,"elapsed":323660,"user":{"displayName":"Sara Fattouh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjGrdvZo2x_8K8abskRubIgMK3SdtS4cG-aQE01nA=s64","userId":"09360594903764573322"}}},"source":["# flau_desc = vec_output_FlaubertForSequenceClassification.from_pretrained(\n","#         'flaubert/flaubert_base_cased', \n","#         num_labels = 27, \n","#         output_attentions = False,\n","#         output_hidden_states = False,)\n","\n","\n","# checkpoint = torch.load(flaubert_model_path_desc)\n","# flau_desc.load_state_dict(checkpoint)"],"execution_count":80,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"eU5pHNZKNKr3"},"source":["#  Instantiation  & Training of Fusion Model "]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"8Qpj2jQYu0P-","executionInfo":{"status":"ok","timestamp":1620603489085,"user_tz":-120,"elapsed":389869,"user":{"displayName":"Sara Fattouh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjGrdvZo2x_8K8abskRubIgMK3SdtS4cG-aQE01nA=s64","userId":"09360594903764573322"}},"outputId":"19f85cbb-8ceb-4045-c910-e2a1fb810680"},"source":["model = vector_fusion() "],"execution_count":81,"outputs":[{"output_type":"stream","text":["Some weights of the model checkpoint at camembert-base were not used when initializing vec_output_CamembertForSequenceClassification: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight']\n","- This IS expected if you are initializing vec_output_CamembertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing vec_output_CamembertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of vec_output_CamembertForSequenceClassification were not initialized from the model checkpoint at camembert-base and are newly initialized: ['embeddings.word_embeddings.weight', 'embeddings.position_embeddings.weight', 'embeddings.token_type_embeddings.weight', 'embeddings.LayerNorm.weight', 'embeddings.LayerNorm.bias', 'encoder.layer.0.attention.self.query.weight', 'encoder.layer.0.attention.self.query.bias', 'encoder.layer.0.attention.self.key.weight', 'encoder.layer.0.attention.self.key.bias', 'encoder.layer.0.attention.self.value.weight', 'encoder.layer.0.attention.self.value.bias', 'encoder.layer.0.attention.output.dense.weight', 'encoder.layer.0.attention.output.dense.bias', 'encoder.layer.0.attention.output.LayerNorm.weight', 'encoder.layer.0.attention.output.LayerNorm.bias', 'encoder.layer.0.intermediate.dense.weight', 'encoder.layer.0.intermediate.dense.bias', 'encoder.layer.0.output.dense.weight', 'encoder.layer.0.output.dense.bias', 'encoder.layer.0.output.LayerNorm.weight', 'encoder.layer.0.output.LayerNorm.bias', 'encoder.layer.1.attention.self.query.weight', 'encoder.layer.1.attention.self.query.bias', 'encoder.layer.1.attention.self.key.weight', 'encoder.layer.1.attention.self.key.bias', 'encoder.layer.1.attention.self.value.weight', 'encoder.layer.1.attention.self.value.bias', 'encoder.layer.1.attention.output.dense.weight', 'encoder.layer.1.attention.output.dense.bias', 'encoder.layer.1.attention.output.LayerNorm.weight', 'encoder.layer.1.attention.output.LayerNorm.bias', 'encoder.layer.1.intermediate.dense.weight', 'encoder.layer.1.intermediate.dense.bias', 'encoder.layer.1.output.dense.weight', 'encoder.layer.1.output.dense.bias', 'encoder.layer.1.output.LayerNorm.weight', 'encoder.layer.1.output.LayerNorm.bias', 'encoder.layer.2.attention.self.query.weight', 'encoder.layer.2.attention.self.query.bias', 'encoder.layer.2.attention.self.key.weight', 'encoder.layer.2.attention.self.key.bias', 'encoder.layer.2.attention.self.value.weight', 'encoder.layer.2.attention.self.value.bias', 'encoder.layer.2.attention.output.dense.weight', 'encoder.layer.2.attention.output.dense.bias', 'encoder.layer.2.attention.output.LayerNorm.weight', 'encoder.layer.2.attention.output.LayerNorm.bias', 'encoder.layer.2.intermediate.dense.weight', 'encoder.layer.2.intermediate.dense.bias', 'encoder.layer.2.output.dense.weight', 'encoder.layer.2.output.dense.bias', 'encoder.layer.2.output.LayerNorm.weight', 'encoder.layer.2.output.LayerNorm.bias', 'encoder.layer.3.attention.self.query.weight', 'encoder.layer.3.attention.self.query.bias', 'encoder.layer.3.attention.self.key.weight', 'encoder.layer.3.attention.self.key.bias', 'encoder.layer.3.attention.self.value.weight', 'encoder.layer.3.attention.self.value.bias', 'encoder.layer.3.attention.output.dense.weight', 'encoder.layer.3.attention.output.dense.bias', 'encoder.layer.3.attention.output.LayerNorm.weight', 'encoder.layer.3.attention.output.LayerNorm.bias', 'encoder.layer.3.intermediate.dense.weight', 'encoder.layer.3.intermediate.dense.bias', 'encoder.layer.3.output.dense.weight', 'encoder.layer.3.output.dense.bias', 'encoder.layer.3.output.LayerNorm.weight', 'encoder.layer.3.output.LayerNorm.bias', 'encoder.layer.4.attention.self.query.weight', 'encoder.layer.4.attention.self.query.bias', 'encoder.layer.4.attention.self.key.weight', 'encoder.layer.4.attention.self.key.bias', 'encoder.layer.4.attention.self.value.weight', 'encoder.layer.4.attention.self.value.bias', 'encoder.layer.4.attention.output.dense.weight', 'encoder.layer.4.attention.output.dense.bias', 'encoder.layer.4.attention.output.LayerNorm.weight', 'encoder.layer.4.attention.output.LayerNorm.bias', 'encoder.layer.4.intermediate.dense.weight', 'encoder.layer.4.intermediate.dense.bias', 'encoder.layer.4.output.dense.weight', 'encoder.layer.4.output.dense.bias', 'encoder.layer.4.output.LayerNorm.weight', 'encoder.layer.4.output.LayerNorm.bias', 'encoder.layer.5.attention.self.query.weight', 'encoder.layer.5.attention.self.query.bias', 'encoder.layer.5.attention.self.key.weight', 'encoder.layer.5.attention.self.key.bias', 'encoder.layer.5.attention.self.value.weight', 'encoder.layer.5.attention.self.value.bias', 'encoder.layer.5.attention.output.dense.weight', 'encoder.layer.5.attention.output.dense.bias', 'encoder.layer.5.attention.output.LayerNorm.weight', 'encoder.layer.5.attention.output.LayerNorm.bias', 'encoder.layer.5.intermediate.dense.weight', 'encoder.layer.5.intermediate.dense.bias', 'encoder.layer.5.output.dense.weight', 'encoder.layer.5.output.dense.bias', 'encoder.layer.5.output.LayerNorm.weight', 'encoder.layer.5.output.LayerNorm.bias', 'encoder.layer.6.attention.self.query.weight', 'encoder.layer.6.attention.self.query.bias', 'encoder.layer.6.attention.self.key.weight', 'encoder.layer.6.attention.self.key.bias', 'encoder.layer.6.attention.self.value.weight', 'encoder.layer.6.attention.self.value.bias', 'encoder.layer.6.attention.output.dense.weight', 'encoder.layer.6.attention.output.dense.bias', 'encoder.layer.6.attention.output.LayerNorm.weight', 'encoder.layer.6.attention.output.LayerNorm.bias', 'encoder.layer.6.intermediate.dense.weight', 'encoder.layer.6.intermediate.dense.bias', 'encoder.layer.6.output.dense.weight', 'encoder.layer.6.output.dense.bias', 'encoder.layer.6.output.LayerNorm.weight', 'encoder.layer.6.output.LayerNorm.bias', 'encoder.layer.7.attention.self.query.weight', 'encoder.layer.7.attention.self.query.bias', 'encoder.layer.7.attention.self.key.weight', 'encoder.layer.7.attention.self.key.bias', 'encoder.layer.7.attention.self.value.weight', 'encoder.layer.7.attention.self.value.bias', 'encoder.layer.7.attention.output.dense.weight', 'encoder.layer.7.attention.output.dense.bias', 'encoder.layer.7.attention.output.LayerNorm.weight', 'encoder.layer.7.attention.output.LayerNorm.bias', 'encoder.layer.7.intermediate.dense.weight', 'encoder.layer.7.intermediate.dense.bias', 'encoder.layer.7.output.dense.weight', 'encoder.layer.7.output.dense.bias', 'encoder.layer.7.output.LayerNorm.weight', 'encoder.layer.7.output.LayerNorm.bias', 'encoder.layer.8.attention.self.query.weight', 'encoder.layer.8.attention.self.query.bias', 'encoder.layer.8.attention.self.key.weight', 'encoder.layer.8.attention.self.key.bias', 'encoder.layer.8.attention.self.value.weight', 'encoder.layer.8.attention.self.value.bias', 'encoder.layer.8.attention.output.dense.weight', 'encoder.layer.8.attention.output.dense.bias', 'encoder.layer.8.attention.output.LayerNorm.weight', 'encoder.layer.8.attention.output.LayerNorm.bias', 'encoder.layer.8.intermediate.dense.weight', 'encoder.layer.8.intermediate.dense.bias', 'encoder.layer.8.output.dense.weight', 'encoder.layer.8.output.dense.bias', 'encoder.layer.8.output.LayerNorm.weight', 'encoder.layer.8.output.LayerNorm.bias', 'encoder.layer.9.attention.self.query.weight', 'encoder.layer.9.attention.self.query.bias', 'encoder.layer.9.attention.self.key.weight', 'encoder.layer.9.attention.self.key.bias', 'encoder.layer.9.attention.self.value.weight', 'encoder.layer.9.attention.self.value.bias', 'encoder.layer.9.attention.output.dense.weight', 'encoder.layer.9.attention.output.dense.bias', 'encoder.layer.9.attention.output.LayerNorm.weight', 'encoder.layer.9.attention.output.LayerNorm.bias', 'encoder.layer.9.intermediate.dense.weight', 'encoder.layer.9.intermediate.dense.bias', 'encoder.layer.9.output.dense.weight', 'encoder.layer.9.output.dense.bias', 'encoder.layer.9.output.LayerNorm.weight', 'encoder.layer.9.output.LayerNorm.bias', 'encoder.layer.10.attention.self.query.weight', 'encoder.layer.10.attention.self.query.bias', 'encoder.layer.10.attention.self.key.weight', 'encoder.layer.10.attention.self.key.bias', 'encoder.layer.10.attention.self.value.weight', 'encoder.layer.10.attention.self.value.bias', 'encoder.layer.10.attention.output.dense.weight', 'encoder.layer.10.attention.output.dense.bias', 'encoder.layer.10.attention.output.LayerNorm.weight', 'encoder.layer.10.attention.output.LayerNorm.bias', 'encoder.layer.10.intermediate.dense.weight', 'encoder.layer.10.intermediate.dense.bias', 'encoder.layer.10.output.dense.weight', 'encoder.layer.10.output.dense.bias', 'encoder.layer.10.output.LayerNorm.weight', 'encoder.layer.10.output.LayerNorm.bias', 'encoder.layer.11.attention.self.query.weight', 'encoder.layer.11.attention.self.query.bias', 'encoder.layer.11.attention.self.key.weight', 'encoder.layer.11.attention.self.key.bias', 'encoder.layer.11.attention.self.value.weight', 'encoder.layer.11.attention.self.value.bias', 'encoder.layer.11.attention.output.dense.weight', 'encoder.layer.11.attention.output.dense.bias', 'encoder.layer.11.attention.output.LayerNorm.weight', 'encoder.layer.11.attention.output.LayerNorm.bias', 'encoder.layer.11.intermediate.dense.weight', 'encoder.layer.11.intermediate.dense.bias', 'encoder.layer.11.output.dense.weight', 'encoder.layer.11.output.dense.bias', 'encoder.layer.11.output.LayerNorm.weight', 'encoder.layer.11.output.LayerNorm.bias', 'pooler.dense.weight', 'pooler.dense.bias', 'dense.weight', 'dense.bias', 'out_proj.weight', 'out_proj.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of the model checkpoint at camembert-base were not used when initializing vec_output_CamembertForSequenceClassification: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight']\n","- This IS expected if you are initializing vec_output_CamembertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing vec_output_CamembertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of vec_output_CamembertForSequenceClassification were not initialized from the model checkpoint at camembert-base and are newly initialized: ['embeddings.word_embeddings.weight', 'embeddings.position_embeddings.weight', 'embeddings.token_type_embeddings.weight', 'embeddings.LayerNorm.weight', 'embeddings.LayerNorm.bias', 'encoder.layer.0.attention.self.query.weight', 'encoder.layer.0.attention.self.query.bias', 'encoder.layer.0.attention.self.key.weight', 'encoder.layer.0.attention.self.key.bias', 'encoder.layer.0.attention.self.value.weight', 'encoder.layer.0.attention.self.value.bias', 'encoder.layer.0.attention.output.dense.weight', 'encoder.layer.0.attention.output.dense.bias', 'encoder.layer.0.attention.output.LayerNorm.weight', 'encoder.layer.0.attention.output.LayerNorm.bias', 'encoder.layer.0.intermediate.dense.weight', 'encoder.layer.0.intermediate.dense.bias', 'encoder.layer.0.output.dense.weight', 'encoder.layer.0.output.dense.bias', 'encoder.layer.0.output.LayerNorm.weight', 'encoder.layer.0.output.LayerNorm.bias', 'encoder.layer.1.attention.self.query.weight', 'encoder.layer.1.attention.self.query.bias', 'encoder.layer.1.attention.self.key.weight', 'encoder.layer.1.attention.self.key.bias', 'encoder.layer.1.attention.self.value.weight', 'encoder.layer.1.attention.self.value.bias', 'encoder.layer.1.attention.output.dense.weight', 'encoder.layer.1.attention.output.dense.bias', 'encoder.layer.1.attention.output.LayerNorm.weight', 'encoder.layer.1.attention.output.LayerNorm.bias', 'encoder.layer.1.intermediate.dense.weight', 'encoder.layer.1.intermediate.dense.bias', 'encoder.layer.1.output.dense.weight', 'encoder.layer.1.output.dense.bias', 'encoder.layer.1.output.LayerNorm.weight', 'encoder.layer.1.output.LayerNorm.bias', 'encoder.layer.2.attention.self.query.weight', 'encoder.layer.2.attention.self.query.bias', 'encoder.layer.2.attention.self.key.weight', 'encoder.layer.2.attention.self.key.bias', 'encoder.layer.2.attention.self.value.weight', 'encoder.layer.2.attention.self.value.bias', 'encoder.layer.2.attention.output.dense.weight', 'encoder.layer.2.attention.output.dense.bias', 'encoder.layer.2.attention.output.LayerNorm.weight', 'encoder.layer.2.attention.output.LayerNorm.bias', 'encoder.layer.2.intermediate.dense.weight', 'encoder.layer.2.intermediate.dense.bias', 'encoder.layer.2.output.dense.weight', 'encoder.layer.2.output.dense.bias', 'encoder.layer.2.output.LayerNorm.weight', 'encoder.layer.2.output.LayerNorm.bias', 'encoder.layer.3.attention.self.query.weight', 'encoder.layer.3.attention.self.query.bias', 'encoder.layer.3.attention.self.key.weight', 'encoder.layer.3.attention.self.key.bias', 'encoder.layer.3.attention.self.value.weight', 'encoder.layer.3.attention.self.value.bias', 'encoder.layer.3.attention.output.dense.weight', 'encoder.layer.3.attention.output.dense.bias', 'encoder.layer.3.attention.output.LayerNorm.weight', 'encoder.layer.3.attention.output.LayerNorm.bias', 'encoder.layer.3.intermediate.dense.weight', 'encoder.layer.3.intermediate.dense.bias', 'encoder.layer.3.output.dense.weight', 'encoder.layer.3.output.dense.bias', 'encoder.layer.3.output.LayerNorm.weight', 'encoder.layer.3.output.LayerNorm.bias', 'encoder.layer.4.attention.self.query.weight', 'encoder.layer.4.attention.self.query.bias', 'encoder.layer.4.attention.self.key.weight', 'encoder.layer.4.attention.self.key.bias', 'encoder.layer.4.attention.self.value.weight', 'encoder.layer.4.attention.self.value.bias', 'encoder.layer.4.attention.output.dense.weight', 'encoder.layer.4.attention.output.dense.bias', 'encoder.layer.4.attention.output.LayerNorm.weight', 'encoder.layer.4.attention.output.LayerNorm.bias', 'encoder.layer.4.intermediate.dense.weight', 'encoder.layer.4.intermediate.dense.bias', 'encoder.layer.4.output.dense.weight', 'encoder.layer.4.output.dense.bias', 'encoder.layer.4.output.LayerNorm.weight', 'encoder.layer.4.output.LayerNorm.bias', 'encoder.layer.5.attention.self.query.weight', 'encoder.layer.5.attention.self.query.bias', 'encoder.layer.5.attention.self.key.weight', 'encoder.layer.5.attention.self.key.bias', 'encoder.layer.5.attention.self.value.weight', 'encoder.layer.5.attention.self.value.bias', 'encoder.layer.5.attention.output.dense.weight', 'encoder.layer.5.attention.output.dense.bias', 'encoder.layer.5.attention.output.LayerNorm.weight', 'encoder.layer.5.attention.output.LayerNorm.bias', 'encoder.layer.5.intermediate.dense.weight', 'encoder.layer.5.intermediate.dense.bias', 'encoder.layer.5.output.dense.weight', 'encoder.layer.5.output.dense.bias', 'encoder.layer.5.output.LayerNorm.weight', 'encoder.layer.5.output.LayerNorm.bias', 'encoder.layer.6.attention.self.query.weight', 'encoder.layer.6.attention.self.query.bias', 'encoder.layer.6.attention.self.key.weight', 'encoder.layer.6.attention.self.key.bias', 'encoder.layer.6.attention.self.value.weight', 'encoder.layer.6.attention.self.value.bias', 'encoder.layer.6.attention.output.dense.weight', 'encoder.layer.6.attention.output.dense.bias', 'encoder.layer.6.attention.output.LayerNorm.weight', 'encoder.layer.6.attention.output.LayerNorm.bias', 'encoder.layer.6.intermediate.dense.weight', 'encoder.layer.6.intermediate.dense.bias', 'encoder.layer.6.output.dense.weight', 'encoder.layer.6.output.dense.bias', 'encoder.layer.6.output.LayerNorm.weight', 'encoder.layer.6.output.LayerNorm.bias', 'encoder.layer.7.attention.self.query.weight', 'encoder.layer.7.attention.self.query.bias', 'encoder.layer.7.attention.self.key.weight', 'encoder.layer.7.attention.self.key.bias', 'encoder.layer.7.attention.self.value.weight', 'encoder.layer.7.attention.self.value.bias', 'encoder.layer.7.attention.output.dense.weight', 'encoder.layer.7.attention.output.dense.bias', 'encoder.layer.7.attention.output.LayerNorm.weight', 'encoder.layer.7.attention.output.LayerNorm.bias', 'encoder.layer.7.intermediate.dense.weight', 'encoder.layer.7.intermediate.dense.bias', 'encoder.layer.7.output.dense.weight', 'encoder.layer.7.output.dense.bias', 'encoder.layer.7.output.LayerNorm.weight', 'encoder.layer.7.output.LayerNorm.bias', 'encoder.layer.8.attention.self.query.weight', 'encoder.layer.8.attention.self.query.bias', 'encoder.layer.8.attention.self.key.weight', 'encoder.layer.8.attention.self.key.bias', 'encoder.layer.8.attention.self.value.weight', 'encoder.layer.8.attention.self.value.bias', 'encoder.layer.8.attention.output.dense.weight', 'encoder.layer.8.attention.output.dense.bias', 'encoder.layer.8.attention.output.LayerNorm.weight', 'encoder.layer.8.attention.output.LayerNorm.bias', 'encoder.layer.8.intermediate.dense.weight', 'encoder.layer.8.intermediate.dense.bias', 'encoder.layer.8.output.dense.weight', 'encoder.layer.8.output.dense.bias', 'encoder.layer.8.output.LayerNorm.weight', 'encoder.layer.8.output.LayerNorm.bias', 'encoder.layer.9.attention.self.query.weight', 'encoder.layer.9.attention.self.query.bias', 'encoder.layer.9.attention.self.key.weight', 'encoder.layer.9.attention.self.key.bias', 'encoder.layer.9.attention.self.value.weight', 'encoder.layer.9.attention.self.value.bias', 'encoder.layer.9.attention.output.dense.weight', 'encoder.layer.9.attention.output.dense.bias', 'encoder.layer.9.attention.output.LayerNorm.weight', 'encoder.layer.9.attention.output.LayerNorm.bias', 'encoder.layer.9.intermediate.dense.weight', 'encoder.layer.9.intermediate.dense.bias', 'encoder.layer.9.output.dense.weight', 'encoder.layer.9.output.dense.bias', 'encoder.layer.9.output.LayerNorm.weight', 'encoder.layer.9.output.LayerNorm.bias', 'encoder.layer.10.attention.self.query.weight', 'encoder.layer.10.attention.self.query.bias', 'encoder.layer.10.attention.self.key.weight', 'encoder.layer.10.attention.self.key.bias', 'encoder.layer.10.attention.self.value.weight', 'encoder.layer.10.attention.self.value.bias', 'encoder.layer.10.attention.output.dense.weight', 'encoder.layer.10.attention.output.dense.bias', 'encoder.layer.10.attention.output.LayerNorm.weight', 'encoder.layer.10.attention.output.LayerNorm.bias', 'encoder.layer.10.intermediate.dense.weight', 'encoder.layer.10.intermediate.dense.bias', 'encoder.layer.10.output.dense.weight', 'encoder.layer.10.output.dense.bias', 'encoder.layer.10.output.LayerNorm.weight', 'encoder.layer.10.output.LayerNorm.bias', 'encoder.layer.11.attention.self.query.weight', 'encoder.layer.11.attention.self.query.bias', 'encoder.layer.11.attention.self.key.weight', 'encoder.layer.11.attention.self.key.bias', 'encoder.layer.11.attention.self.value.weight', 'encoder.layer.11.attention.self.value.bias', 'encoder.layer.11.attention.output.dense.weight', 'encoder.layer.11.attention.output.dense.bias', 'encoder.layer.11.attention.output.LayerNorm.weight', 'encoder.layer.11.attention.output.LayerNorm.bias', 'encoder.layer.11.intermediate.dense.weight', 'encoder.layer.11.intermediate.dense.bias', 'encoder.layer.11.output.dense.weight', 'encoder.layer.11.output.dense.bias', 'encoder.layer.11.output.LayerNorm.weight', 'encoder.layer.11.output.LayerNorm.bias', 'pooler.dense.weight', 'pooler.dense.bias', 'dense.weight', 'dense.bias', 'out_proj.weight', 'out_proj.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of the model checkpoint at flaubert/flaubert_base_cased were not used when initializing vec_output_FlaubertForSequenceClassification: ['pred_layer.proj.bias', 'pred_layer.proj.weight']\n","- This IS expected if you are initializing vec_output_FlaubertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing vec_output_FlaubertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of vec_output_FlaubertForSequenceClassification were not initialized from the model checkpoint at flaubert/flaubert_base_cased and are newly initialized: ['position_embeddings.weight', 'embeddings.weight', 'layer_norm_emb.weight', 'layer_norm_emb.bias', 'attentions.0.q_lin.weight', 'attentions.0.q_lin.bias', 'attentions.0.k_lin.weight', 'attentions.0.k_lin.bias', 'attentions.0.v_lin.weight', 'attentions.0.v_lin.bias', 'attentions.0.out_lin.weight', 'attentions.0.out_lin.bias', 'attentions.1.q_lin.weight', 'attentions.1.q_lin.bias', 'attentions.1.k_lin.weight', 'attentions.1.k_lin.bias', 'attentions.1.v_lin.weight', 'attentions.1.v_lin.bias', 'attentions.1.out_lin.weight', 'attentions.1.out_lin.bias', 'attentions.2.q_lin.weight', 'attentions.2.q_lin.bias', 'attentions.2.k_lin.weight', 'attentions.2.k_lin.bias', 'attentions.2.v_lin.weight', 'attentions.2.v_lin.bias', 'attentions.2.out_lin.weight', 'attentions.2.out_lin.bias', 'attentions.3.q_lin.weight', 'attentions.3.q_lin.bias', 'attentions.3.k_lin.weight', 'attentions.3.k_lin.bias', 'attentions.3.v_lin.weight', 'attentions.3.v_lin.bias', 'attentions.3.out_lin.weight', 'attentions.3.out_lin.bias', 'attentions.4.q_lin.weight', 'attentions.4.q_lin.bias', 'attentions.4.k_lin.weight', 'attentions.4.k_lin.bias', 'attentions.4.v_lin.weight', 'attentions.4.v_lin.bias', 'attentions.4.out_lin.weight', 'attentions.4.out_lin.bias', 'attentions.5.q_lin.weight', 'attentions.5.q_lin.bias', 'attentions.5.k_lin.weight', 'attentions.5.k_lin.bias', 'attentions.5.v_lin.weight', 'attentions.5.v_lin.bias', 'attentions.5.out_lin.weight', 'attentions.5.out_lin.bias', 'attentions.6.q_lin.weight', 'attentions.6.q_lin.bias', 'attentions.6.k_lin.weight', 'attentions.6.k_lin.bias', 'attentions.6.v_lin.weight', 'attentions.6.v_lin.bias', 'attentions.6.out_lin.weight', 'attentions.6.out_lin.bias', 'attentions.7.q_lin.weight', 'attentions.7.q_lin.bias', 'attentions.7.k_lin.weight', 'attentions.7.k_lin.bias', 'attentions.7.v_lin.weight', 'attentions.7.v_lin.bias', 'attentions.7.out_lin.weight', 'attentions.7.out_lin.bias', 'attentions.8.q_lin.weight', 'attentions.8.q_lin.bias', 'attentions.8.k_lin.weight', 'attentions.8.k_lin.bias', 'attentions.8.v_lin.weight', 'attentions.8.v_lin.bias', 'attentions.8.out_lin.weight', 'attentions.8.out_lin.bias', 'attentions.9.q_lin.weight', 'attentions.9.q_lin.bias', 'attentions.9.k_lin.weight', 'attentions.9.k_lin.bias', 'attentions.9.v_lin.weight', 'attentions.9.v_lin.bias', 'attentions.9.out_lin.weight', 'attentions.9.out_lin.bias', 'attentions.10.q_lin.weight', 'attentions.10.q_lin.bias', 'attentions.10.k_lin.weight', 'attentions.10.k_lin.bias', 'attentions.10.v_lin.weight', 'attentions.10.v_lin.bias', 'attentions.10.out_lin.weight', 'attentions.10.out_lin.bias', 'attentions.11.q_lin.weight', 'attentions.11.q_lin.bias', 'attentions.11.k_lin.weight', 'attentions.11.k_lin.bias', 'attentions.11.v_lin.weight', 'attentions.11.v_lin.bias', 'attentions.11.out_lin.weight', 'attentions.11.out_lin.bias', 'layer_norm1.0.weight', 'layer_norm1.0.bias', 'layer_norm1.1.weight', 'layer_norm1.1.bias', 'layer_norm1.2.weight', 'layer_norm1.2.bias', 'layer_norm1.3.weight', 'layer_norm1.3.bias', 'layer_norm1.4.weight', 'layer_norm1.4.bias', 'layer_norm1.5.weight', 'layer_norm1.5.bias', 'layer_norm1.6.weight', 'layer_norm1.6.bias', 'layer_norm1.7.weight', 'layer_norm1.7.bias', 'layer_norm1.8.weight', 'layer_norm1.8.bias', 'layer_norm1.9.weight', 'layer_norm1.9.bias', 'layer_norm1.10.weight', 'layer_norm1.10.bias', 'layer_norm1.11.weight', 'layer_norm1.11.bias', 'ffns.0.lin1.weight', 'ffns.0.lin1.bias', 'ffns.0.lin2.weight', 'ffns.0.lin2.bias', 'ffns.1.lin1.weight', 'ffns.1.lin1.bias', 'ffns.1.lin2.weight', 'ffns.1.lin2.bias', 'ffns.2.lin1.weight', 'ffns.2.lin1.bias', 'ffns.2.lin2.weight', 'ffns.2.lin2.bias', 'ffns.3.lin1.weight', 'ffns.3.lin1.bias', 'ffns.3.lin2.weight', 'ffns.3.lin2.bias', 'ffns.4.lin1.weight', 'ffns.4.lin1.bias', 'ffns.4.lin2.weight', 'ffns.4.lin2.bias', 'ffns.5.lin1.weight', 'ffns.5.lin1.bias', 'ffns.5.lin2.weight', 'ffns.5.lin2.bias', 'ffns.6.lin1.weight', 'ffns.6.lin1.bias', 'ffns.6.lin2.weight', 'ffns.6.lin2.bias', 'ffns.7.lin1.weight', 'ffns.7.lin1.bias', 'ffns.7.lin2.weight', 'ffns.7.lin2.bias', 'ffns.8.lin1.weight', 'ffns.8.lin1.bias', 'ffns.8.lin2.weight', 'ffns.8.lin2.bias', 'ffns.9.lin1.weight', 'ffns.9.lin1.bias', 'ffns.9.lin2.weight', 'ffns.9.lin2.bias', 'ffns.10.lin1.weight', 'ffns.10.lin1.bias', 'ffns.10.lin2.weight', 'ffns.10.lin2.bias', 'ffns.11.lin1.weight', 'ffns.11.lin1.bias', 'ffns.11.lin2.weight', 'ffns.11.lin2.bias', 'layer_norm2.0.weight', 'layer_norm2.0.bias', 'layer_norm2.1.weight', 'layer_norm2.1.bias', 'layer_norm2.2.weight', 'layer_norm2.2.bias', 'layer_norm2.3.weight', 'layer_norm2.3.bias', 'layer_norm2.4.weight', 'layer_norm2.4.bias', 'layer_norm2.5.weight', 'layer_norm2.5.bias', 'layer_norm2.6.weight', 'layer_norm2.6.bias', 'layer_norm2.7.weight', 'layer_norm2.7.bias', 'layer_norm2.8.weight', 'layer_norm2.8.bias', 'layer_norm2.9.weight', 'layer_norm2.9.bias', 'layer_norm2.10.weight', 'layer_norm2.10.bias', 'layer_norm2.11.weight', 'layer_norm2.11.bias', 'sequence_summary.summary.weight', 'sequence_summary.summary.bias', 'classifier.weight', 'classifier.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of the model checkpoint at flaubert/flaubert_base_cased were not used when initializing vec_output_FlaubertForSequenceClassification: ['pred_layer.proj.bias', 'pred_layer.proj.weight']\n","- This IS expected if you are initializing vec_output_FlaubertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing vec_output_FlaubertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of vec_output_FlaubertForSequenceClassification were not initialized from the model checkpoint at flaubert/flaubert_base_cased and are newly initialized: ['position_embeddings.weight', 'embeddings.weight', 'layer_norm_emb.weight', 'layer_norm_emb.bias', 'attentions.0.q_lin.weight', 'attentions.0.q_lin.bias', 'attentions.0.k_lin.weight', 'attentions.0.k_lin.bias', 'attentions.0.v_lin.weight', 'attentions.0.v_lin.bias', 'attentions.0.out_lin.weight', 'attentions.0.out_lin.bias', 'attentions.1.q_lin.weight', 'attentions.1.q_lin.bias', 'attentions.1.k_lin.weight', 'attentions.1.k_lin.bias', 'attentions.1.v_lin.weight', 'attentions.1.v_lin.bias', 'attentions.1.out_lin.weight', 'attentions.1.out_lin.bias', 'attentions.2.q_lin.weight', 'attentions.2.q_lin.bias', 'attentions.2.k_lin.weight', 'attentions.2.k_lin.bias', 'attentions.2.v_lin.weight', 'attentions.2.v_lin.bias', 'attentions.2.out_lin.weight', 'attentions.2.out_lin.bias', 'attentions.3.q_lin.weight', 'attentions.3.q_lin.bias', 'attentions.3.k_lin.weight', 'attentions.3.k_lin.bias', 'attentions.3.v_lin.weight', 'attentions.3.v_lin.bias', 'attentions.3.out_lin.weight', 'attentions.3.out_lin.bias', 'attentions.4.q_lin.weight', 'attentions.4.q_lin.bias', 'attentions.4.k_lin.weight', 'attentions.4.k_lin.bias', 'attentions.4.v_lin.weight', 'attentions.4.v_lin.bias', 'attentions.4.out_lin.weight', 'attentions.4.out_lin.bias', 'attentions.5.q_lin.weight', 'attentions.5.q_lin.bias', 'attentions.5.k_lin.weight', 'attentions.5.k_lin.bias', 'attentions.5.v_lin.weight', 'attentions.5.v_lin.bias', 'attentions.5.out_lin.weight', 'attentions.5.out_lin.bias', 'attentions.6.q_lin.weight', 'attentions.6.q_lin.bias', 'attentions.6.k_lin.weight', 'attentions.6.k_lin.bias', 'attentions.6.v_lin.weight', 'attentions.6.v_lin.bias', 'attentions.6.out_lin.weight', 'attentions.6.out_lin.bias', 'attentions.7.q_lin.weight', 'attentions.7.q_lin.bias', 'attentions.7.k_lin.weight', 'attentions.7.k_lin.bias', 'attentions.7.v_lin.weight', 'attentions.7.v_lin.bias', 'attentions.7.out_lin.weight', 'attentions.7.out_lin.bias', 'attentions.8.q_lin.weight', 'attentions.8.q_lin.bias', 'attentions.8.k_lin.weight', 'attentions.8.k_lin.bias', 'attentions.8.v_lin.weight', 'attentions.8.v_lin.bias', 'attentions.8.out_lin.weight', 'attentions.8.out_lin.bias', 'attentions.9.q_lin.weight', 'attentions.9.q_lin.bias', 'attentions.9.k_lin.weight', 'attentions.9.k_lin.bias', 'attentions.9.v_lin.weight', 'attentions.9.v_lin.bias', 'attentions.9.out_lin.weight', 'attentions.9.out_lin.bias', 'attentions.10.q_lin.weight', 'attentions.10.q_lin.bias', 'attentions.10.k_lin.weight', 'attentions.10.k_lin.bias', 'attentions.10.v_lin.weight', 'attentions.10.v_lin.bias', 'attentions.10.out_lin.weight', 'attentions.10.out_lin.bias', 'attentions.11.q_lin.weight', 'attentions.11.q_lin.bias', 'attentions.11.k_lin.weight', 'attentions.11.k_lin.bias', 'attentions.11.v_lin.weight', 'attentions.11.v_lin.bias', 'attentions.11.out_lin.weight', 'attentions.11.out_lin.bias', 'layer_norm1.0.weight', 'layer_norm1.0.bias', 'layer_norm1.1.weight', 'layer_norm1.1.bias', 'layer_norm1.2.weight', 'layer_norm1.2.bias', 'layer_norm1.3.weight', 'layer_norm1.3.bias', 'layer_norm1.4.weight', 'layer_norm1.4.bias', 'layer_norm1.5.weight', 'layer_norm1.5.bias', 'layer_norm1.6.weight', 'layer_norm1.6.bias', 'layer_norm1.7.weight', 'layer_norm1.7.bias', 'layer_norm1.8.weight', 'layer_norm1.8.bias', 'layer_norm1.9.weight', 'layer_norm1.9.bias', 'layer_norm1.10.weight', 'layer_norm1.10.bias', 'layer_norm1.11.weight', 'layer_norm1.11.bias', 'ffns.0.lin1.weight', 'ffns.0.lin1.bias', 'ffns.0.lin2.weight', 'ffns.0.lin2.bias', 'ffns.1.lin1.weight', 'ffns.1.lin1.bias', 'ffns.1.lin2.weight', 'ffns.1.lin2.bias', 'ffns.2.lin1.weight', 'ffns.2.lin1.bias', 'ffns.2.lin2.weight', 'ffns.2.lin2.bias', 'ffns.3.lin1.weight', 'ffns.3.lin1.bias', 'ffns.3.lin2.weight', 'ffns.3.lin2.bias', 'ffns.4.lin1.weight', 'ffns.4.lin1.bias', 'ffns.4.lin2.weight', 'ffns.4.lin2.bias', 'ffns.5.lin1.weight', 'ffns.5.lin1.bias', 'ffns.5.lin2.weight', 'ffns.5.lin2.bias', 'ffns.6.lin1.weight', 'ffns.6.lin1.bias', 'ffns.6.lin2.weight', 'ffns.6.lin2.bias', 'ffns.7.lin1.weight', 'ffns.7.lin1.bias', 'ffns.7.lin2.weight', 'ffns.7.lin2.bias', 'ffns.8.lin1.weight', 'ffns.8.lin1.bias', 'ffns.8.lin2.weight', 'ffns.8.lin2.bias', 'ffns.9.lin1.weight', 'ffns.9.lin1.bias', 'ffns.9.lin2.weight', 'ffns.9.lin2.bias', 'ffns.10.lin1.weight', 'ffns.10.lin1.bias', 'ffns.10.lin2.weight', 'ffns.10.lin2.bias', 'ffns.11.lin1.weight', 'ffns.11.lin1.bias', 'ffns.11.lin2.weight', 'ffns.11.lin2.bias', 'layer_norm2.0.weight', 'layer_norm2.0.bias', 'layer_norm2.1.weight', 'layer_norm2.1.bias', 'layer_norm2.2.weight', 'layer_norm2.2.bias', 'layer_norm2.3.weight', 'layer_norm2.3.bias', 'layer_norm2.4.weight', 'layer_norm2.4.bias', 'layer_norm2.5.weight', 'layer_norm2.5.bias', 'layer_norm2.6.weight', 'layer_norm2.6.bias', 'layer_norm2.7.weight', 'layer_norm2.7.bias', 'layer_norm2.8.weight', 'layer_norm2.8.bias', 'layer_norm2.9.weight', 'layer_norm2.9.bias', 'layer_norm2.10.weight', 'layer_norm2.10.bias', 'layer_norm2.11.weight', 'layer_norm2.11.bias', 'sequence_summary.summary.weight', 'sequence_summary.summary.bias', 'classifier.weight', 'classifier.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"GN9HIANRu1_R","executionInfo":{"status":"ok","timestamp":1620603490345,"user_tz":-120,"elapsed":391115,"user":{"displayName":"Sara Fattouh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjGrdvZo2x_8K8abskRubIgMK3SdtS4cG-aQE01nA=s64","userId":"09360594903764573322"}},"outputId":"cce230ed-a11d-4dd0-c57c-94830ebd3727"},"source":["model.cuda()"],"execution_count":82,"outputs":[{"output_type":"execute_result","data":{"text/plain":["vector_fusion(\n","  (img_model): SEResnext50_32x4d(\n","    (base_model): SENet(\n","      (layer0): Sequential(\n","        (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n","        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (relu1): ReLU(inplace=True)\n","        (pool): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=True)\n","      )\n","      (layer1): Sequential(\n","        (0): SEResNeXtBottleneck(\n","          (conv1): Conv2d(64, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n","          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (conv3): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu): ReLU(inplace=True)\n","          (se_module): SEModule(\n","            (avg_pool): AdaptiveAvgPool2d(output_size=1)\n","            (fc1): Conv2d(256, 16, kernel_size=(1, 1), stride=(1, 1))\n","            (relu): ReLU(inplace=True)\n","            (fc2): Conv2d(16, 256, kernel_size=(1, 1), stride=(1, 1))\n","            (sigmoid): Sigmoid()\n","          )\n","          (downsample): Sequential(\n","            (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","            (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          )\n","        )\n","        (1): SEResNeXtBottleneck(\n","          (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n","          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (conv3): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu): ReLU(inplace=True)\n","          (se_module): SEModule(\n","            (avg_pool): AdaptiveAvgPool2d(output_size=1)\n","            (fc1): Conv2d(256, 16, kernel_size=(1, 1), stride=(1, 1))\n","            (relu): ReLU(inplace=True)\n","            (fc2): Conv2d(16, 256, kernel_size=(1, 1), stride=(1, 1))\n","            (sigmoid): Sigmoid()\n","          )\n","        )\n","        (2): SEResNeXtBottleneck(\n","          (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n","          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (conv3): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu): ReLU(inplace=True)\n","          (se_module): SEModule(\n","            (avg_pool): AdaptiveAvgPool2d(output_size=1)\n","            (fc1): Conv2d(256, 16, kernel_size=(1, 1), stride=(1, 1))\n","            (relu): ReLU(inplace=True)\n","            (fc2): Conv2d(16, 256, kernel_size=(1, 1), stride=(1, 1))\n","            (sigmoid): Sigmoid()\n","          )\n","        )\n","      )\n","      (layer2): Sequential(\n","        (0): SEResNeXtBottleneck(\n","          (conv1): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=32, bias=False)\n","          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (conv3): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu): ReLU(inplace=True)\n","          (se_module): SEModule(\n","            (avg_pool): AdaptiveAvgPool2d(output_size=1)\n","            (fc1): Conv2d(512, 32, kernel_size=(1, 1), stride=(1, 1))\n","            (relu): ReLU(inplace=True)\n","            (fc2): Conv2d(32, 512, kernel_size=(1, 1), stride=(1, 1))\n","            (sigmoid): Sigmoid()\n","          )\n","          (downsample): Sequential(\n","            (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n","            (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          )\n","        )\n","        (1): SEResNeXtBottleneck(\n","          (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n","          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (conv3): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu): ReLU(inplace=True)\n","          (se_module): SEModule(\n","            (avg_pool): AdaptiveAvgPool2d(output_size=1)\n","            (fc1): Conv2d(512, 32, kernel_size=(1, 1), stride=(1, 1))\n","            (relu): ReLU(inplace=True)\n","            (fc2): Conv2d(32, 512, kernel_size=(1, 1), stride=(1, 1))\n","            (sigmoid): Sigmoid()\n","          )\n","        )\n","        (2): SEResNeXtBottleneck(\n","          (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n","          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (conv3): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu): ReLU(inplace=True)\n","          (se_module): SEModule(\n","            (avg_pool): AdaptiveAvgPool2d(output_size=1)\n","            (fc1): Conv2d(512, 32, kernel_size=(1, 1), stride=(1, 1))\n","            (relu): ReLU(inplace=True)\n","            (fc2): Conv2d(32, 512, kernel_size=(1, 1), stride=(1, 1))\n","            (sigmoid): Sigmoid()\n","          )\n","        )\n","        (3): SEResNeXtBottleneck(\n","          (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n","          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (conv3): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu): ReLU(inplace=True)\n","          (se_module): SEModule(\n","            (avg_pool): AdaptiveAvgPool2d(output_size=1)\n","            (fc1): Conv2d(512, 32, kernel_size=(1, 1), stride=(1, 1))\n","            (relu): ReLU(inplace=True)\n","            (fc2): Conv2d(32, 512, kernel_size=(1, 1), stride=(1, 1))\n","            (sigmoid): Sigmoid()\n","          )\n","        )\n","      )\n","      (layer3): Sequential(\n","        (0): SEResNeXtBottleneck(\n","          (conv1): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=32, bias=False)\n","          (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (conv3): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu): ReLU(inplace=True)\n","          (se_module): SEModule(\n","            (avg_pool): AdaptiveAvgPool2d(output_size=1)\n","            (fc1): Conv2d(1024, 64, kernel_size=(1, 1), stride=(1, 1))\n","            (relu): ReLU(inplace=True)\n","            (fc2): Conv2d(64, 1024, kernel_size=(1, 1), stride=(1, 1))\n","            (sigmoid): Sigmoid()\n","          )\n","          (downsample): Sequential(\n","            (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n","            (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          )\n","        )\n","        (1): SEResNeXtBottleneck(\n","          (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n","          (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (conv3): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu): ReLU(inplace=True)\n","          (se_module): SEModule(\n","            (avg_pool): AdaptiveAvgPool2d(output_size=1)\n","            (fc1): Conv2d(1024, 64, kernel_size=(1, 1), stride=(1, 1))\n","            (relu): ReLU(inplace=True)\n","            (fc2): Conv2d(64, 1024, kernel_size=(1, 1), stride=(1, 1))\n","            (sigmoid): Sigmoid()\n","          )\n","        )\n","        (2): SEResNeXtBottleneck(\n","          (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n","          (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (conv3): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu): ReLU(inplace=True)\n","          (se_module): SEModule(\n","            (avg_pool): AdaptiveAvgPool2d(output_size=1)\n","            (fc1): Conv2d(1024, 64, kernel_size=(1, 1), stride=(1, 1))\n","            (relu): ReLU(inplace=True)\n","            (fc2): Conv2d(64, 1024, kernel_size=(1, 1), stride=(1, 1))\n","            (sigmoid): Sigmoid()\n","          )\n","        )\n","        (3): SEResNeXtBottleneck(\n","          (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n","          (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (conv3): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu): ReLU(inplace=True)\n","          (se_module): SEModule(\n","            (avg_pool): AdaptiveAvgPool2d(output_size=1)\n","            (fc1): Conv2d(1024, 64, kernel_size=(1, 1), stride=(1, 1))\n","            (relu): ReLU(inplace=True)\n","            (fc2): Conv2d(64, 1024, kernel_size=(1, 1), stride=(1, 1))\n","            (sigmoid): Sigmoid()\n","          )\n","        )\n","        (4): SEResNeXtBottleneck(\n","          (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n","          (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (conv3): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu): ReLU(inplace=True)\n","          (se_module): SEModule(\n","            (avg_pool): AdaptiveAvgPool2d(output_size=1)\n","            (fc1): Conv2d(1024, 64, kernel_size=(1, 1), stride=(1, 1))\n","            (relu): ReLU(inplace=True)\n","            (fc2): Conv2d(64, 1024, kernel_size=(1, 1), stride=(1, 1))\n","            (sigmoid): Sigmoid()\n","          )\n","        )\n","        (5): SEResNeXtBottleneck(\n","          (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n","          (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (conv3): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu): ReLU(inplace=True)\n","          (se_module): SEModule(\n","            (avg_pool): AdaptiveAvgPool2d(output_size=1)\n","            (fc1): Conv2d(1024, 64, kernel_size=(1, 1), stride=(1, 1))\n","            (relu): ReLU(inplace=True)\n","            (fc2): Conv2d(64, 1024, kernel_size=(1, 1), stride=(1, 1))\n","            (sigmoid): Sigmoid()\n","          )\n","        )\n","      )\n","      (layer4): Sequential(\n","        (0): SEResNeXtBottleneck(\n","          (conv1): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (conv2): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=32, bias=False)\n","          (bn2): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (conv3): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu): ReLU(inplace=True)\n","          (se_module): SEModule(\n","            (avg_pool): AdaptiveAvgPool2d(output_size=1)\n","            (fc1): Conv2d(2048, 128, kernel_size=(1, 1), stride=(1, 1))\n","            (relu): ReLU(inplace=True)\n","            (fc2): Conv2d(128, 2048, kernel_size=(1, 1), stride=(1, 1))\n","            (sigmoid): Sigmoid()\n","          )\n","          (downsample): Sequential(\n","            (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n","            (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          )\n","        )\n","        (1): SEResNeXtBottleneck(\n","          (conv1): Conv2d(2048, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (conv2): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n","          (bn2): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (conv3): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu): ReLU(inplace=True)\n","          (se_module): SEModule(\n","            (avg_pool): AdaptiveAvgPool2d(output_size=1)\n","            (fc1): Conv2d(2048, 128, kernel_size=(1, 1), stride=(1, 1))\n","            (relu): ReLU(inplace=True)\n","            (fc2): Conv2d(128, 2048, kernel_size=(1, 1), stride=(1, 1))\n","            (sigmoid): Sigmoid()\n","          )\n","        )\n","        (2): SEResNeXtBottleneck(\n","          (conv1): Conv2d(2048, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (conv2): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n","          (bn2): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (conv3): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (relu): ReLU(inplace=True)\n","          (se_module): SEModule(\n","            (avg_pool): AdaptiveAvgPool2d(output_size=1)\n","            (fc1): Conv2d(2048, 128, kernel_size=(1, 1), stride=(1, 1))\n","            (relu): ReLU(inplace=True)\n","            (fc2): Conv2d(128, 2048, kernel_size=(1, 1), stride=(1, 1))\n","            (sigmoid): Sigmoid()\n","          )\n","        )\n","      )\n","      (avg_pool): AvgPool2d(kernel_size=7, stride=1, padding=0)\n","      (last_linear): Linear(in_features=2048, out_features=1000, bias=True)\n","    )\n","    (l0): Identity()\n","  )\n","  (cam_model_title): vec_output_CamembertForSequenceClassification(\n","    (embeddings): RobertaEmbeddings(\n","      (word_embeddings): Embedding(32005, 768, padding_idx=1)\n","      (position_embeddings): Embedding(514, 768, padding_idx=1)\n","      (token_type_embeddings): Embedding(1, 768)\n","      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","      (dropout): Dropout(p=0.1, inplace=False)\n","    )\n","    (encoder): RobertaEncoder(\n","      (layer): ModuleList(\n","        (0): RobertaLayer(\n","          (attention): RobertaAttention(\n","            (self): RobertaSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): RobertaSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): RobertaIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): RobertaOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (1): RobertaLayer(\n","          (attention): RobertaAttention(\n","            (self): RobertaSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): RobertaSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): RobertaIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): RobertaOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (2): RobertaLayer(\n","          (attention): RobertaAttention(\n","            (self): RobertaSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): RobertaSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): RobertaIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): RobertaOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (3): RobertaLayer(\n","          (attention): RobertaAttention(\n","            (self): RobertaSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): RobertaSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): RobertaIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): RobertaOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (4): RobertaLayer(\n","          (attention): RobertaAttention(\n","            (self): RobertaSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): RobertaSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): RobertaIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): RobertaOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (5): RobertaLayer(\n","          (attention): RobertaAttention(\n","            (self): RobertaSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): RobertaSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): RobertaIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): RobertaOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (6): RobertaLayer(\n","          (attention): RobertaAttention(\n","            (self): RobertaSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): RobertaSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): RobertaIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): RobertaOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (7): RobertaLayer(\n","          (attention): RobertaAttention(\n","            (self): RobertaSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): RobertaSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): RobertaIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): RobertaOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (8): RobertaLayer(\n","          (attention): RobertaAttention(\n","            (self): RobertaSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): RobertaSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): RobertaIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): RobertaOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (9): RobertaLayer(\n","          (attention): RobertaAttention(\n","            (self): RobertaSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): RobertaSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): RobertaIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): RobertaOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (10): RobertaLayer(\n","          (attention): RobertaAttention(\n","            (self): RobertaSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): RobertaSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): RobertaIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): RobertaOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (11): RobertaLayer(\n","          (attention): RobertaAttention(\n","            (self): RobertaSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): RobertaSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): RobertaIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): RobertaOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","    )\n","    (pooler): RobertaPooler(\n","      (dense): Linear(in_features=768, out_features=768, bias=True)\n","      (activation): Tanh()\n","    )\n","    (roberta): CamembertModel(\n","      (embeddings): RobertaEmbeddings(\n","        (word_embeddings): Embedding(32005, 768, padding_idx=1)\n","        (position_embeddings): Embedding(514, 768, padding_idx=1)\n","        (token_type_embeddings): Embedding(1, 768)\n","        (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","        (dropout): Dropout(p=0.1, inplace=False)\n","      )\n","      (encoder): RobertaEncoder(\n","        (layer): ModuleList(\n","          (0): RobertaLayer(\n","            (attention): RobertaAttention(\n","              (self): RobertaSelfAttention(\n","                (query): Linear(in_features=768, out_features=768, bias=True)\n","                (key): Linear(in_features=768, out_features=768, bias=True)\n","                (value): Linear(in_features=768, out_features=768, bias=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","              (output): RobertaSelfOutput(\n","                (dense): Linear(in_features=768, out_features=768, bias=True)\n","                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","            )\n","            (intermediate): RobertaIntermediate(\n","              (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            )\n","            (output): RobertaOutput(\n","              (dense): Linear(in_features=3072, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (1): RobertaLayer(\n","            (attention): RobertaAttention(\n","              (self): RobertaSelfAttention(\n","                (query): Linear(in_features=768, out_features=768, bias=True)\n","                (key): Linear(in_features=768, out_features=768, bias=True)\n","                (value): Linear(in_features=768, out_features=768, bias=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","              (output): RobertaSelfOutput(\n","                (dense): Linear(in_features=768, out_features=768, bias=True)\n","                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","            )\n","            (intermediate): RobertaIntermediate(\n","              (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            )\n","            (output): RobertaOutput(\n","              (dense): Linear(in_features=3072, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (2): RobertaLayer(\n","            (attention): RobertaAttention(\n","              (self): RobertaSelfAttention(\n","                (query): Linear(in_features=768, out_features=768, bias=True)\n","                (key): Linear(in_features=768, out_features=768, bias=True)\n","                (value): Linear(in_features=768, out_features=768, bias=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","              (output): RobertaSelfOutput(\n","                (dense): Linear(in_features=768, out_features=768, bias=True)\n","                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","            )\n","            (intermediate): RobertaIntermediate(\n","              (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            )\n","            (output): RobertaOutput(\n","              (dense): Linear(in_features=3072, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (3): RobertaLayer(\n","            (attention): RobertaAttention(\n","              (self): RobertaSelfAttention(\n","                (query): Linear(in_features=768, out_features=768, bias=True)\n","                (key): Linear(in_features=768, out_features=768, bias=True)\n","                (value): Linear(in_features=768, out_features=768, bias=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","              (output): RobertaSelfOutput(\n","                (dense): Linear(in_features=768, out_features=768, bias=True)\n","                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","            )\n","            (intermediate): RobertaIntermediate(\n","              (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            )\n","            (output): RobertaOutput(\n","              (dense): Linear(in_features=3072, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (4): RobertaLayer(\n","            (attention): RobertaAttention(\n","              (self): RobertaSelfAttention(\n","                (query): Linear(in_features=768, out_features=768, bias=True)\n","                (key): Linear(in_features=768, out_features=768, bias=True)\n","                (value): Linear(in_features=768, out_features=768, bias=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","              (output): RobertaSelfOutput(\n","                (dense): Linear(in_features=768, out_features=768, bias=True)\n","                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","            )\n","            (intermediate): RobertaIntermediate(\n","              (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            )\n","            (output): RobertaOutput(\n","              (dense): Linear(in_features=3072, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (5): RobertaLayer(\n","            (attention): RobertaAttention(\n","              (self): RobertaSelfAttention(\n","                (query): Linear(in_features=768, out_features=768, bias=True)\n","                (key): Linear(in_features=768, out_features=768, bias=True)\n","                (value): Linear(in_features=768, out_features=768, bias=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","              (output): RobertaSelfOutput(\n","                (dense): Linear(in_features=768, out_features=768, bias=True)\n","                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","            )\n","            (intermediate): RobertaIntermediate(\n","              (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            )\n","            (output): RobertaOutput(\n","              (dense): Linear(in_features=3072, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (6): RobertaLayer(\n","            (attention): RobertaAttention(\n","              (self): RobertaSelfAttention(\n","                (query): Linear(in_features=768, out_features=768, bias=True)\n","                (key): Linear(in_features=768, out_features=768, bias=True)\n","                (value): Linear(in_features=768, out_features=768, bias=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","              (output): RobertaSelfOutput(\n","                (dense): Linear(in_features=768, out_features=768, bias=True)\n","                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","            )\n","            (intermediate): RobertaIntermediate(\n","              (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            )\n","            (output): RobertaOutput(\n","              (dense): Linear(in_features=3072, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (7): RobertaLayer(\n","            (attention): RobertaAttention(\n","              (self): RobertaSelfAttention(\n","                (query): Linear(in_features=768, out_features=768, bias=True)\n","                (key): Linear(in_features=768, out_features=768, bias=True)\n","                (value): Linear(in_features=768, out_features=768, bias=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","              (output): RobertaSelfOutput(\n","                (dense): Linear(in_features=768, out_features=768, bias=True)\n","                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","            )\n","            (intermediate): RobertaIntermediate(\n","              (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            )\n","            (output): RobertaOutput(\n","              (dense): Linear(in_features=3072, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (8): RobertaLayer(\n","            (attention): RobertaAttention(\n","              (self): RobertaSelfAttention(\n","                (query): Linear(in_features=768, out_features=768, bias=True)\n","                (key): Linear(in_features=768, out_features=768, bias=True)\n","                (value): Linear(in_features=768, out_features=768, bias=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","              (output): RobertaSelfOutput(\n","                (dense): Linear(in_features=768, out_features=768, bias=True)\n","                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","            )\n","            (intermediate): RobertaIntermediate(\n","              (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            )\n","            (output): RobertaOutput(\n","              (dense): Linear(in_features=3072, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (9): RobertaLayer(\n","            (attention): RobertaAttention(\n","              (self): RobertaSelfAttention(\n","                (query): Linear(in_features=768, out_features=768, bias=True)\n","                (key): Linear(in_features=768, out_features=768, bias=True)\n","                (value): Linear(in_features=768, out_features=768, bias=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","              (output): RobertaSelfOutput(\n","                (dense): Linear(in_features=768, out_features=768, bias=True)\n","                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","            )\n","            (intermediate): RobertaIntermediate(\n","              (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            )\n","            (output): RobertaOutput(\n","              (dense): Linear(in_features=3072, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (10): RobertaLayer(\n","            (attention): RobertaAttention(\n","              (self): RobertaSelfAttention(\n","                (query): Linear(in_features=768, out_features=768, bias=True)\n","                (key): Linear(in_features=768, out_features=768, bias=True)\n","                (value): Linear(in_features=768, out_features=768, bias=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","              (output): RobertaSelfOutput(\n","                (dense): Linear(in_features=768, out_features=768, bias=True)\n","                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","            )\n","            (intermediate): RobertaIntermediate(\n","              (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            )\n","            (output): RobertaOutput(\n","              (dense): Linear(in_features=3072, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (11): RobertaLayer(\n","            (attention): RobertaAttention(\n","              (self): RobertaSelfAttention(\n","                (query): Linear(in_features=768, out_features=768, bias=True)\n","                (key): Linear(in_features=768, out_features=768, bias=True)\n","                (value): Linear(in_features=768, out_features=768, bias=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","              (output): RobertaSelfOutput(\n","                (dense): Linear(in_features=768, out_features=768, bias=True)\n","                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","            )\n","            (intermediate): RobertaIntermediate(\n","              (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            )\n","            (output): RobertaOutput(\n","              (dense): Linear(in_features=3072, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","        )\n","      )\n","      (pooler): RobertaPooler(\n","        (dense): Linear(in_features=768, out_features=768, bias=True)\n","        (activation): Tanh()\n","      )\n","    )\n","    (dense): Linear(in_features=196608, out_features=768, bias=True)\n","    (dropout): Dropout(p=0.1, inplace=False)\n","    (out_proj): Identity()\n","  )\n","  (cam_model_desc): vec_output_CamembertForSequenceClassification(\n","    (embeddings): RobertaEmbeddings(\n","      (word_embeddings): Embedding(32005, 768, padding_idx=1)\n","      (position_embeddings): Embedding(514, 768, padding_idx=1)\n","      (token_type_embeddings): Embedding(1, 768)\n","      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","      (dropout): Dropout(p=0.1, inplace=False)\n","    )\n","    (encoder): RobertaEncoder(\n","      (layer): ModuleList(\n","        (0): RobertaLayer(\n","          (attention): RobertaAttention(\n","            (self): RobertaSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): RobertaSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): RobertaIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): RobertaOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (1): RobertaLayer(\n","          (attention): RobertaAttention(\n","            (self): RobertaSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): RobertaSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): RobertaIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): RobertaOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (2): RobertaLayer(\n","          (attention): RobertaAttention(\n","            (self): RobertaSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): RobertaSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): RobertaIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): RobertaOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (3): RobertaLayer(\n","          (attention): RobertaAttention(\n","            (self): RobertaSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): RobertaSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): RobertaIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): RobertaOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (4): RobertaLayer(\n","          (attention): RobertaAttention(\n","            (self): RobertaSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): RobertaSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): RobertaIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): RobertaOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (5): RobertaLayer(\n","          (attention): RobertaAttention(\n","            (self): RobertaSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): RobertaSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): RobertaIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): RobertaOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (6): RobertaLayer(\n","          (attention): RobertaAttention(\n","            (self): RobertaSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): RobertaSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): RobertaIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): RobertaOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (7): RobertaLayer(\n","          (attention): RobertaAttention(\n","            (self): RobertaSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): RobertaSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): RobertaIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): RobertaOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (8): RobertaLayer(\n","          (attention): RobertaAttention(\n","            (self): RobertaSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): RobertaSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): RobertaIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): RobertaOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (9): RobertaLayer(\n","          (attention): RobertaAttention(\n","            (self): RobertaSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): RobertaSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): RobertaIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): RobertaOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (10): RobertaLayer(\n","          (attention): RobertaAttention(\n","            (self): RobertaSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): RobertaSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): RobertaIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): RobertaOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (11): RobertaLayer(\n","          (attention): RobertaAttention(\n","            (self): RobertaSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): RobertaSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): RobertaIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): RobertaOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","    )\n","    (pooler): RobertaPooler(\n","      (dense): Linear(in_features=768, out_features=768, bias=True)\n","      (activation): Tanh()\n","    )\n","    (roberta): CamembertModel(\n","      (embeddings): RobertaEmbeddings(\n","        (word_embeddings): Embedding(32005, 768, padding_idx=1)\n","        (position_embeddings): Embedding(514, 768, padding_idx=1)\n","        (token_type_embeddings): Embedding(1, 768)\n","        (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","        (dropout): Dropout(p=0.1, inplace=False)\n","      )\n","      (encoder): RobertaEncoder(\n","        (layer): ModuleList(\n","          (0): RobertaLayer(\n","            (attention): RobertaAttention(\n","              (self): RobertaSelfAttention(\n","                (query): Linear(in_features=768, out_features=768, bias=True)\n","                (key): Linear(in_features=768, out_features=768, bias=True)\n","                (value): Linear(in_features=768, out_features=768, bias=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","              (output): RobertaSelfOutput(\n","                (dense): Linear(in_features=768, out_features=768, bias=True)\n","                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","            )\n","            (intermediate): RobertaIntermediate(\n","              (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            )\n","            (output): RobertaOutput(\n","              (dense): Linear(in_features=3072, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (1): RobertaLayer(\n","            (attention): RobertaAttention(\n","              (self): RobertaSelfAttention(\n","                (query): Linear(in_features=768, out_features=768, bias=True)\n","                (key): Linear(in_features=768, out_features=768, bias=True)\n","                (value): Linear(in_features=768, out_features=768, bias=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","              (output): RobertaSelfOutput(\n","                (dense): Linear(in_features=768, out_features=768, bias=True)\n","                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","            )\n","            (intermediate): RobertaIntermediate(\n","              (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            )\n","            (output): RobertaOutput(\n","              (dense): Linear(in_features=3072, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (2): RobertaLayer(\n","            (attention): RobertaAttention(\n","              (self): RobertaSelfAttention(\n","                (query): Linear(in_features=768, out_features=768, bias=True)\n","                (key): Linear(in_features=768, out_features=768, bias=True)\n","                (value): Linear(in_features=768, out_features=768, bias=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","              (output): RobertaSelfOutput(\n","                (dense): Linear(in_features=768, out_features=768, bias=True)\n","                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","            )\n","            (intermediate): RobertaIntermediate(\n","              (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            )\n","            (output): RobertaOutput(\n","              (dense): Linear(in_features=3072, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (3): RobertaLayer(\n","            (attention): RobertaAttention(\n","              (self): RobertaSelfAttention(\n","                (query): Linear(in_features=768, out_features=768, bias=True)\n","                (key): Linear(in_features=768, out_features=768, bias=True)\n","                (value): Linear(in_features=768, out_features=768, bias=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","              (output): RobertaSelfOutput(\n","                (dense): Linear(in_features=768, out_features=768, bias=True)\n","                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","            )\n","            (intermediate): RobertaIntermediate(\n","              (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            )\n","            (output): RobertaOutput(\n","              (dense): Linear(in_features=3072, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (4): RobertaLayer(\n","            (attention): RobertaAttention(\n","              (self): RobertaSelfAttention(\n","                (query): Linear(in_features=768, out_features=768, bias=True)\n","                (key): Linear(in_features=768, out_features=768, bias=True)\n","                (value): Linear(in_features=768, out_features=768, bias=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","              (output): RobertaSelfOutput(\n","                (dense): Linear(in_features=768, out_features=768, bias=True)\n","                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","            )\n","            (intermediate): RobertaIntermediate(\n","              (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            )\n","            (output): RobertaOutput(\n","              (dense): Linear(in_features=3072, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (5): RobertaLayer(\n","            (attention): RobertaAttention(\n","              (self): RobertaSelfAttention(\n","                (query): Linear(in_features=768, out_features=768, bias=True)\n","                (key): Linear(in_features=768, out_features=768, bias=True)\n","                (value): Linear(in_features=768, out_features=768, bias=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","              (output): RobertaSelfOutput(\n","                (dense): Linear(in_features=768, out_features=768, bias=True)\n","                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","            )\n","            (intermediate): RobertaIntermediate(\n","              (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            )\n","            (output): RobertaOutput(\n","              (dense): Linear(in_features=3072, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (6): RobertaLayer(\n","            (attention): RobertaAttention(\n","              (self): RobertaSelfAttention(\n","                (query): Linear(in_features=768, out_features=768, bias=True)\n","                (key): Linear(in_features=768, out_features=768, bias=True)\n","                (value): Linear(in_features=768, out_features=768, bias=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","              (output): RobertaSelfOutput(\n","                (dense): Linear(in_features=768, out_features=768, bias=True)\n","                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","            )\n","            (intermediate): RobertaIntermediate(\n","              (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            )\n","            (output): RobertaOutput(\n","              (dense): Linear(in_features=3072, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (7): RobertaLayer(\n","            (attention): RobertaAttention(\n","              (self): RobertaSelfAttention(\n","                (query): Linear(in_features=768, out_features=768, bias=True)\n","                (key): Linear(in_features=768, out_features=768, bias=True)\n","                (value): Linear(in_features=768, out_features=768, bias=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","              (output): RobertaSelfOutput(\n","                (dense): Linear(in_features=768, out_features=768, bias=True)\n","                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","            )\n","            (intermediate): RobertaIntermediate(\n","              (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            )\n","            (output): RobertaOutput(\n","              (dense): Linear(in_features=3072, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (8): RobertaLayer(\n","            (attention): RobertaAttention(\n","              (self): RobertaSelfAttention(\n","                (query): Linear(in_features=768, out_features=768, bias=True)\n","                (key): Linear(in_features=768, out_features=768, bias=True)\n","                (value): Linear(in_features=768, out_features=768, bias=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","              (output): RobertaSelfOutput(\n","                (dense): Linear(in_features=768, out_features=768, bias=True)\n","                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","            )\n","            (intermediate): RobertaIntermediate(\n","              (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            )\n","            (output): RobertaOutput(\n","              (dense): Linear(in_features=3072, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (9): RobertaLayer(\n","            (attention): RobertaAttention(\n","              (self): RobertaSelfAttention(\n","                (query): Linear(in_features=768, out_features=768, bias=True)\n","                (key): Linear(in_features=768, out_features=768, bias=True)\n","                (value): Linear(in_features=768, out_features=768, bias=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","              (output): RobertaSelfOutput(\n","                (dense): Linear(in_features=768, out_features=768, bias=True)\n","                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","            )\n","            (intermediate): RobertaIntermediate(\n","              (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            )\n","            (output): RobertaOutput(\n","              (dense): Linear(in_features=3072, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (10): RobertaLayer(\n","            (attention): RobertaAttention(\n","              (self): RobertaSelfAttention(\n","                (query): Linear(in_features=768, out_features=768, bias=True)\n","                (key): Linear(in_features=768, out_features=768, bias=True)\n","                (value): Linear(in_features=768, out_features=768, bias=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","              (output): RobertaSelfOutput(\n","                (dense): Linear(in_features=768, out_features=768, bias=True)\n","                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","            )\n","            (intermediate): RobertaIntermediate(\n","              (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            )\n","            (output): RobertaOutput(\n","              (dense): Linear(in_features=3072, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (11): RobertaLayer(\n","            (attention): RobertaAttention(\n","              (self): RobertaSelfAttention(\n","                (query): Linear(in_features=768, out_features=768, bias=True)\n","                (key): Linear(in_features=768, out_features=768, bias=True)\n","                (value): Linear(in_features=768, out_features=768, bias=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","              (output): RobertaSelfOutput(\n","                (dense): Linear(in_features=768, out_features=768, bias=True)\n","                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","            )\n","            (intermediate): RobertaIntermediate(\n","              (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            )\n","            (output): RobertaOutput(\n","              (dense): Linear(in_features=3072, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","        )\n","      )\n","      (pooler): RobertaPooler(\n","        (dense): Linear(in_features=768, out_features=768, bias=True)\n","        (activation): Tanh()\n","      )\n","    )\n","    (dense): Linear(in_features=196608, out_features=768, bias=True)\n","    (dropout): Dropout(p=0.1, inplace=False)\n","    (out_proj): Identity()\n","  )\n","  (flau_model_title): vec_output_FlaubertForSequenceClassification(\n","    (position_embeddings): Embedding(512, 768)\n","    (embeddings): Embedding(68729, 768, padding_idx=2)\n","    (layer_norm_emb): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","    (attentions): ModuleList(\n","      (0): MultiHeadAttention(\n","        (q_lin): Linear(in_features=768, out_features=768, bias=True)\n","        (k_lin): Linear(in_features=768, out_features=768, bias=True)\n","        (v_lin): Linear(in_features=768, out_features=768, bias=True)\n","        (out_lin): Linear(in_features=768, out_features=768, bias=True)\n","      )\n","      (1): MultiHeadAttention(\n","        (q_lin): Linear(in_features=768, out_features=768, bias=True)\n","        (k_lin): Linear(in_features=768, out_features=768, bias=True)\n","        (v_lin): Linear(in_features=768, out_features=768, bias=True)\n","        (out_lin): Linear(in_features=768, out_features=768, bias=True)\n","      )\n","      (2): MultiHeadAttention(\n","        (q_lin): Linear(in_features=768, out_features=768, bias=True)\n","        (k_lin): Linear(in_features=768, out_features=768, bias=True)\n","        (v_lin): Linear(in_features=768, out_features=768, bias=True)\n","        (out_lin): Linear(in_features=768, out_features=768, bias=True)\n","      )\n","      (3): MultiHeadAttention(\n","        (q_lin): Linear(in_features=768, out_features=768, bias=True)\n","        (k_lin): Linear(in_features=768, out_features=768, bias=True)\n","        (v_lin): Linear(in_features=768, out_features=768, bias=True)\n","        (out_lin): Linear(in_features=768, out_features=768, bias=True)\n","      )\n","      (4): MultiHeadAttention(\n","        (q_lin): Linear(in_features=768, out_features=768, bias=True)\n","        (k_lin): Linear(in_features=768, out_features=768, bias=True)\n","        (v_lin): Linear(in_features=768, out_features=768, bias=True)\n","        (out_lin): Linear(in_features=768, out_features=768, bias=True)\n","      )\n","      (5): MultiHeadAttention(\n","        (q_lin): Linear(in_features=768, out_features=768, bias=True)\n","        (k_lin): Linear(in_features=768, out_features=768, bias=True)\n","        (v_lin): Linear(in_features=768, out_features=768, bias=True)\n","        (out_lin): Linear(in_features=768, out_features=768, bias=True)\n","      )\n","      (6): MultiHeadAttention(\n","        (q_lin): Linear(in_features=768, out_features=768, bias=True)\n","        (k_lin): Linear(in_features=768, out_features=768, bias=True)\n","        (v_lin): Linear(in_features=768, out_features=768, bias=True)\n","        (out_lin): Linear(in_features=768, out_features=768, bias=True)\n","      )\n","      (7): MultiHeadAttention(\n","        (q_lin): Linear(in_features=768, out_features=768, bias=True)\n","        (k_lin): Linear(in_features=768, out_features=768, bias=True)\n","        (v_lin): Linear(in_features=768, out_features=768, bias=True)\n","        (out_lin): Linear(in_features=768, out_features=768, bias=True)\n","      )\n","      (8): MultiHeadAttention(\n","        (q_lin): Linear(in_features=768, out_features=768, bias=True)\n","        (k_lin): Linear(in_features=768, out_features=768, bias=True)\n","        (v_lin): Linear(in_features=768, out_features=768, bias=True)\n","        (out_lin): Linear(in_features=768, out_features=768, bias=True)\n","      )\n","      (9): MultiHeadAttention(\n","        (q_lin): Linear(in_features=768, out_features=768, bias=True)\n","        (k_lin): Linear(in_features=768, out_features=768, bias=True)\n","        (v_lin): Linear(in_features=768, out_features=768, bias=True)\n","        (out_lin): Linear(in_features=768, out_features=768, bias=True)\n","      )\n","      (10): MultiHeadAttention(\n","        (q_lin): Linear(in_features=768, out_features=768, bias=True)\n","        (k_lin): Linear(in_features=768, out_features=768, bias=True)\n","        (v_lin): Linear(in_features=768, out_features=768, bias=True)\n","        (out_lin): Linear(in_features=768, out_features=768, bias=True)\n","      )\n","      (11): MultiHeadAttention(\n","        (q_lin): Linear(in_features=768, out_features=768, bias=True)\n","        (k_lin): Linear(in_features=768, out_features=768, bias=True)\n","        (v_lin): Linear(in_features=768, out_features=768, bias=True)\n","        (out_lin): Linear(in_features=768, out_features=768, bias=True)\n","      )\n","    )\n","    (layer_norm1): ModuleList(\n","      (0): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","      (1): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","      (2): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","      (3): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","      (4): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","      (5): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","      (6): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","      (7): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","      (8): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","      (9): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","      (10): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","      (11): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","    )\n","    (ffns): ModuleList(\n","      (0): TransformerFFN(\n","        (lin1): Linear(in_features=768, out_features=3072, bias=True)\n","        (lin2): Linear(in_features=3072, out_features=768, bias=True)\n","      )\n","      (1): TransformerFFN(\n","        (lin1): Linear(in_features=768, out_features=3072, bias=True)\n","        (lin2): Linear(in_features=3072, out_features=768, bias=True)\n","      )\n","      (2): TransformerFFN(\n","        (lin1): Linear(in_features=768, out_features=3072, bias=True)\n","        (lin2): Linear(in_features=3072, out_features=768, bias=True)\n","      )\n","      (3): TransformerFFN(\n","        (lin1): Linear(in_features=768, out_features=3072, bias=True)\n","        (lin2): Linear(in_features=3072, out_features=768, bias=True)\n","      )\n","      (4): TransformerFFN(\n","        (lin1): Linear(in_features=768, out_features=3072, bias=True)\n","        (lin2): Linear(in_features=3072, out_features=768, bias=True)\n","      )\n","      (5): TransformerFFN(\n","        (lin1): Linear(in_features=768, out_features=3072, bias=True)\n","        (lin2): Linear(in_features=3072, out_features=768, bias=True)\n","      )\n","      (6): TransformerFFN(\n","        (lin1): Linear(in_features=768, out_features=3072, bias=True)\n","        (lin2): Linear(in_features=3072, out_features=768, bias=True)\n","      )\n","      (7): TransformerFFN(\n","        (lin1): Linear(in_features=768, out_features=3072, bias=True)\n","        (lin2): Linear(in_features=3072, out_features=768, bias=True)\n","      )\n","      (8): TransformerFFN(\n","        (lin1): Linear(in_features=768, out_features=3072, bias=True)\n","        (lin2): Linear(in_features=3072, out_features=768, bias=True)\n","      )\n","      (9): TransformerFFN(\n","        (lin1): Linear(in_features=768, out_features=3072, bias=True)\n","        (lin2): Linear(in_features=3072, out_features=768, bias=True)\n","      )\n","      (10): TransformerFFN(\n","        (lin1): Linear(in_features=768, out_features=3072, bias=True)\n","        (lin2): Linear(in_features=3072, out_features=768, bias=True)\n","      )\n","      (11): TransformerFFN(\n","        (lin1): Linear(in_features=768, out_features=3072, bias=True)\n","        (lin2): Linear(in_features=3072, out_features=768, bias=True)\n","      )\n","    )\n","    (layer_norm2): ModuleList(\n","      (0): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","      (1): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","      (2): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","      (3): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","      (4): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","      (5): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","      (6): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","      (7): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","      (8): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","      (9): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","      (10): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","      (11): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","    )\n","    (transformer): FlaubertModel(\n","      (position_embeddings): Embedding(512, 768)\n","      (embeddings): Embedding(68729, 768, padding_idx=2)\n","      (layer_norm_emb): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","      (attentions): ModuleList(\n","        (0): MultiHeadAttention(\n","          (q_lin): Linear(in_features=768, out_features=768, bias=True)\n","          (k_lin): Linear(in_features=768, out_features=768, bias=True)\n","          (v_lin): Linear(in_features=768, out_features=768, bias=True)\n","          (out_lin): Linear(in_features=768, out_features=768, bias=True)\n","        )\n","        (1): MultiHeadAttention(\n","          (q_lin): Linear(in_features=768, out_features=768, bias=True)\n","          (k_lin): Linear(in_features=768, out_features=768, bias=True)\n","          (v_lin): Linear(in_features=768, out_features=768, bias=True)\n","          (out_lin): Linear(in_features=768, out_features=768, bias=True)\n","        )\n","        (2): MultiHeadAttention(\n","          (q_lin): Linear(in_features=768, out_features=768, bias=True)\n","          (k_lin): Linear(in_features=768, out_features=768, bias=True)\n","          (v_lin): Linear(in_features=768, out_features=768, bias=True)\n","          (out_lin): Linear(in_features=768, out_features=768, bias=True)\n","        )\n","        (3): MultiHeadAttention(\n","          (q_lin): Linear(in_features=768, out_features=768, bias=True)\n","          (k_lin): Linear(in_features=768, out_features=768, bias=True)\n","          (v_lin): Linear(in_features=768, out_features=768, bias=True)\n","          (out_lin): Linear(in_features=768, out_features=768, bias=True)\n","        )\n","        (4): MultiHeadAttention(\n","          (q_lin): Linear(in_features=768, out_features=768, bias=True)\n","          (k_lin): Linear(in_features=768, out_features=768, bias=True)\n","          (v_lin): Linear(in_features=768, out_features=768, bias=True)\n","          (out_lin): Linear(in_features=768, out_features=768, bias=True)\n","        )\n","        (5): MultiHeadAttention(\n","          (q_lin): Linear(in_features=768, out_features=768, bias=True)\n","          (k_lin): Linear(in_features=768, out_features=768, bias=True)\n","          (v_lin): Linear(in_features=768, out_features=768, bias=True)\n","          (out_lin): Linear(in_features=768, out_features=768, bias=True)\n","        )\n","        (6): MultiHeadAttention(\n","          (q_lin): Linear(in_features=768, out_features=768, bias=True)\n","          (k_lin): Linear(in_features=768, out_features=768, bias=True)\n","          (v_lin): Linear(in_features=768, out_features=768, bias=True)\n","          (out_lin): Linear(in_features=768, out_features=768, bias=True)\n","        )\n","        (7): MultiHeadAttention(\n","          (q_lin): Linear(in_features=768, out_features=768, bias=True)\n","          (k_lin): Linear(in_features=768, out_features=768, bias=True)\n","          (v_lin): Linear(in_features=768, out_features=768, bias=True)\n","          (out_lin): Linear(in_features=768, out_features=768, bias=True)\n","        )\n","        (8): MultiHeadAttention(\n","          (q_lin): Linear(in_features=768, out_features=768, bias=True)\n","          (k_lin): Linear(in_features=768, out_features=768, bias=True)\n","          (v_lin): Linear(in_features=768, out_features=768, bias=True)\n","          (out_lin): Linear(in_features=768, out_features=768, bias=True)\n","        )\n","        (9): MultiHeadAttention(\n","          (q_lin): Linear(in_features=768, out_features=768, bias=True)\n","          (k_lin): Linear(in_features=768, out_features=768, bias=True)\n","          (v_lin): Linear(in_features=768, out_features=768, bias=True)\n","          (out_lin): Linear(in_features=768, out_features=768, bias=True)\n","        )\n","        (10): MultiHeadAttention(\n","          (q_lin): Linear(in_features=768, out_features=768, bias=True)\n","          (k_lin): Linear(in_features=768, out_features=768, bias=True)\n","          (v_lin): Linear(in_features=768, out_features=768, bias=True)\n","          (out_lin): Linear(in_features=768, out_features=768, bias=True)\n","        )\n","        (11): MultiHeadAttention(\n","          (q_lin): Linear(in_features=768, out_features=768, bias=True)\n","          (k_lin): Linear(in_features=768, out_features=768, bias=True)\n","          (v_lin): Linear(in_features=768, out_features=768, bias=True)\n","          (out_lin): Linear(in_features=768, out_features=768, bias=True)\n","        )\n","      )\n","      (layer_norm1): ModuleList(\n","        (0): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","        (1): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","        (2): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","        (3): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","        (4): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","        (5): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","        (6): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","        (7): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","        (8): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","        (9): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","        (10): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","        (11): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","      )\n","      (ffns): ModuleList(\n","        (0): TransformerFFN(\n","          (lin1): Linear(in_features=768, out_features=3072, bias=True)\n","          (lin2): Linear(in_features=3072, out_features=768, bias=True)\n","        )\n","        (1): TransformerFFN(\n","          (lin1): Linear(in_features=768, out_features=3072, bias=True)\n","          (lin2): Linear(in_features=3072, out_features=768, bias=True)\n","        )\n","        (2): TransformerFFN(\n","          (lin1): Linear(in_features=768, out_features=3072, bias=True)\n","          (lin2): Linear(in_features=3072, out_features=768, bias=True)\n","        )\n","        (3): TransformerFFN(\n","          (lin1): Linear(in_features=768, out_features=3072, bias=True)\n","          (lin2): Linear(in_features=3072, out_features=768, bias=True)\n","        )\n","        (4): TransformerFFN(\n","          (lin1): Linear(in_features=768, out_features=3072, bias=True)\n","          (lin2): Linear(in_features=3072, out_features=768, bias=True)\n","        )\n","        (5): TransformerFFN(\n","          (lin1): Linear(in_features=768, out_features=3072, bias=True)\n","          (lin2): Linear(in_features=3072, out_features=768, bias=True)\n","        )\n","        (6): TransformerFFN(\n","          (lin1): Linear(in_features=768, out_features=3072, bias=True)\n","          (lin2): Linear(in_features=3072, out_features=768, bias=True)\n","        )\n","        (7): TransformerFFN(\n","          (lin1): Linear(in_features=768, out_features=3072, bias=True)\n","          (lin2): Linear(in_features=3072, out_features=768, bias=True)\n","        )\n","        (8): TransformerFFN(\n","          (lin1): Linear(in_features=768, out_features=3072, bias=True)\n","          (lin2): Linear(in_features=3072, out_features=768, bias=True)\n","        )\n","        (9): TransformerFFN(\n","          (lin1): Linear(in_features=768, out_features=3072, bias=True)\n","          (lin2): Linear(in_features=3072, out_features=768, bias=True)\n","        )\n","        (10): TransformerFFN(\n","          (lin1): Linear(in_features=768, out_features=3072, bias=True)\n","          (lin2): Linear(in_features=3072, out_features=768, bias=True)\n","        )\n","        (11): TransformerFFN(\n","          (lin1): Linear(in_features=768, out_features=3072, bias=True)\n","          (lin2): Linear(in_features=3072, out_features=768, bias=True)\n","        )\n","      )\n","      (layer_norm2): ModuleList(\n","        (0): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","        (1): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","        (2): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","        (3): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","        (4): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","        (5): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","        (6): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","        (7): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","        (8): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","        (9): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","        (10): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","        (11): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","      )\n","    )\n","    (sequence_summary): SequenceSummary(\n","      (summary): Linear(in_features=768, out_features=27, bias=True)\n","      (activation): Identity()\n","      (first_dropout): Dropout(p=0.1, inplace=False)\n","      (last_dropout): Identity()\n","    )\n","    (dropout): Dropout(p=0.1, inplace=False)\n","    (classifier): Identity()\n","  )\n","  (flau_model_desc): vec_output_FlaubertForSequenceClassification(\n","    (position_embeddings): Embedding(512, 768)\n","    (embeddings): Embedding(68729, 768, padding_idx=2)\n","    (layer_norm_emb): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","    (attentions): ModuleList(\n","      (0): MultiHeadAttention(\n","        (q_lin): Linear(in_features=768, out_features=768, bias=True)\n","        (k_lin): Linear(in_features=768, out_features=768, bias=True)\n","        (v_lin): Linear(in_features=768, out_features=768, bias=True)\n","        (out_lin): Linear(in_features=768, out_features=768, bias=True)\n","      )\n","      (1): MultiHeadAttention(\n","        (q_lin): Linear(in_features=768, out_features=768, bias=True)\n","        (k_lin): Linear(in_features=768, out_features=768, bias=True)\n","        (v_lin): Linear(in_features=768, out_features=768, bias=True)\n","        (out_lin): Linear(in_features=768, out_features=768, bias=True)\n","      )\n","      (2): MultiHeadAttention(\n","        (q_lin): Linear(in_features=768, out_features=768, bias=True)\n","        (k_lin): Linear(in_features=768, out_features=768, bias=True)\n","        (v_lin): Linear(in_features=768, out_features=768, bias=True)\n","        (out_lin): Linear(in_features=768, out_features=768, bias=True)\n","      )\n","      (3): MultiHeadAttention(\n","        (q_lin): Linear(in_features=768, out_features=768, bias=True)\n","        (k_lin): Linear(in_features=768, out_features=768, bias=True)\n","        (v_lin): Linear(in_features=768, out_features=768, bias=True)\n","        (out_lin): Linear(in_features=768, out_features=768, bias=True)\n","      )\n","      (4): MultiHeadAttention(\n","        (q_lin): Linear(in_features=768, out_features=768, bias=True)\n","        (k_lin): Linear(in_features=768, out_features=768, bias=True)\n","        (v_lin): Linear(in_features=768, out_features=768, bias=True)\n","        (out_lin): Linear(in_features=768, out_features=768, bias=True)\n","      )\n","      (5): MultiHeadAttention(\n","        (q_lin): Linear(in_features=768, out_features=768, bias=True)\n","        (k_lin): Linear(in_features=768, out_features=768, bias=True)\n","        (v_lin): Linear(in_features=768, out_features=768, bias=True)\n","        (out_lin): Linear(in_features=768, out_features=768, bias=True)\n","      )\n","      (6): MultiHeadAttention(\n","        (q_lin): Linear(in_features=768, out_features=768, bias=True)\n","        (k_lin): Linear(in_features=768, out_features=768, bias=True)\n","        (v_lin): Linear(in_features=768, out_features=768, bias=True)\n","        (out_lin): Linear(in_features=768, out_features=768, bias=True)\n","      )\n","      (7): MultiHeadAttention(\n","        (q_lin): Linear(in_features=768, out_features=768, bias=True)\n","        (k_lin): Linear(in_features=768, out_features=768, bias=True)\n","        (v_lin): Linear(in_features=768, out_features=768, bias=True)\n","        (out_lin): Linear(in_features=768, out_features=768, bias=True)\n","      )\n","      (8): MultiHeadAttention(\n","        (q_lin): Linear(in_features=768, out_features=768, bias=True)\n","        (k_lin): Linear(in_features=768, out_features=768, bias=True)\n","        (v_lin): Linear(in_features=768, out_features=768, bias=True)\n","        (out_lin): Linear(in_features=768, out_features=768, bias=True)\n","      )\n","      (9): MultiHeadAttention(\n","        (q_lin): Linear(in_features=768, out_features=768, bias=True)\n","        (k_lin): Linear(in_features=768, out_features=768, bias=True)\n","        (v_lin): Linear(in_features=768, out_features=768, bias=True)\n","        (out_lin): Linear(in_features=768, out_features=768, bias=True)\n","      )\n","      (10): MultiHeadAttention(\n","        (q_lin): Linear(in_features=768, out_features=768, bias=True)\n","        (k_lin): Linear(in_features=768, out_features=768, bias=True)\n","        (v_lin): Linear(in_features=768, out_features=768, bias=True)\n","        (out_lin): Linear(in_features=768, out_features=768, bias=True)\n","      )\n","      (11): MultiHeadAttention(\n","        (q_lin): Linear(in_features=768, out_features=768, bias=True)\n","        (k_lin): Linear(in_features=768, out_features=768, bias=True)\n","        (v_lin): Linear(in_features=768, out_features=768, bias=True)\n","        (out_lin): Linear(in_features=768, out_features=768, bias=True)\n","      )\n","    )\n","    (layer_norm1): ModuleList(\n","      (0): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","      (1): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","      (2): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","      (3): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","      (4): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","      (5): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","      (6): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","      (7): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","      (8): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","      (9): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","      (10): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","      (11): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","    )\n","    (ffns): ModuleList(\n","      (0): TransformerFFN(\n","        (lin1): Linear(in_features=768, out_features=3072, bias=True)\n","        (lin2): Linear(in_features=3072, out_features=768, bias=True)\n","      )\n","      (1): TransformerFFN(\n","        (lin1): Linear(in_features=768, out_features=3072, bias=True)\n","        (lin2): Linear(in_features=3072, out_features=768, bias=True)\n","      )\n","      (2): TransformerFFN(\n","        (lin1): Linear(in_features=768, out_features=3072, bias=True)\n","        (lin2): Linear(in_features=3072, out_features=768, bias=True)\n","      )\n","      (3): TransformerFFN(\n","        (lin1): Linear(in_features=768, out_features=3072, bias=True)\n","        (lin2): Linear(in_features=3072, out_features=768, bias=True)\n","      )\n","      (4): TransformerFFN(\n","        (lin1): Linear(in_features=768, out_features=3072, bias=True)\n","        (lin2): Linear(in_features=3072, out_features=768, bias=True)\n","      )\n","      (5): TransformerFFN(\n","        (lin1): Linear(in_features=768, out_features=3072, bias=True)\n","        (lin2): Linear(in_features=3072, out_features=768, bias=True)\n","      )\n","      (6): TransformerFFN(\n","        (lin1): Linear(in_features=768, out_features=3072, bias=True)\n","        (lin2): Linear(in_features=3072, out_features=768, bias=True)\n","      )\n","      (7): TransformerFFN(\n","        (lin1): Linear(in_features=768, out_features=3072, bias=True)\n","        (lin2): Linear(in_features=3072, out_features=768, bias=True)\n","      )\n","      (8): TransformerFFN(\n","        (lin1): Linear(in_features=768, out_features=3072, bias=True)\n","        (lin2): Linear(in_features=3072, out_features=768, bias=True)\n","      )\n","      (9): TransformerFFN(\n","        (lin1): Linear(in_features=768, out_features=3072, bias=True)\n","        (lin2): Linear(in_features=3072, out_features=768, bias=True)\n","      )\n","      (10): TransformerFFN(\n","        (lin1): Linear(in_features=768, out_features=3072, bias=True)\n","        (lin2): Linear(in_features=3072, out_features=768, bias=True)\n","      )\n","      (11): TransformerFFN(\n","        (lin1): Linear(in_features=768, out_features=3072, bias=True)\n","        (lin2): Linear(in_features=3072, out_features=768, bias=True)\n","      )\n","    )\n","    (layer_norm2): ModuleList(\n","      (0): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","      (1): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","      (2): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","      (3): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","      (4): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","      (5): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","      (6): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","      (7): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","      (8): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","      (9): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","      (10): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","      (11): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","    )\n","    (transformer): FlaubertModel(\n","      (position_embeddings): Embedding(512, 768)\n","      (embeddings): Embedding(68729, 768, padding_idx=2)\n","      (layer_norm_emb): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","      (attentions): ModuleList(\n","        (0): MultiHeadAttention(\n","          (q_lin): Linear(in_features=768, out_features=768, bias=True)\n","          (k_lin): Linear(in_features=768, out_features=768, bias=True)\n","          (v_lin): Linear(in_features=768, out_features=768, bias=True)\n","          (out_lin): Linear(in_features=768, out_features=768, bias=True)\n","        )\n","        (1): MultiHeadAttention(\n","          (q_lin): Linear(in_features=768, out_features=768, bias=True)\n","          (k_lin): Linear(in_features=768, out_features=768, bias=True)\n","          (v_lin): Linear(in_features=768, out_features=768, bias=True)\n","          (out_lin): Linear(in_features=768, out_features=768, bias=True)\n","        )\n","        (2): MultiHeadAttention(\n","          (q_lin): Linear(in_features=768, out_features=768, bias=True)\n","          (k_lin): Linear(in_features=768, out_features=768, bias=True)\n","          (v_lin): Linear(in_features=768, out_features=768, bias=True)\n","          (out_lin): Linear(in_features=768, out_features=768, bias=True)\n","        )\n","        (3): MultiHeadAttention(\n","          (q_lin): Linear(in_features=768, out_features=768, bias=True)\n","          (k_lin): Linear(in_features=768, out_features=768, bias=True)\n","          (v_lin): Linear(in_features=768, out_features=768, bias=True)\n","          (out_lin): Linear(in_features=768, out_features=768, bias=True)\n","        )\n","        (4): MultiHeadAttention(\n","          (q_lin): Linear(in_features=768, out_features=768, bias=True)\n","          (k_lin): Linear(in_features=768, out_features=768, bias=True)\n","          (v_lin): Linear(in_features=768, out_features=768, bias=True)\n","          (out_lin): Linear(in_features=768, out_features=768, bias=True)\n","        )\n","        (5): MultiHeadAttention(\n","          (q_lin): Linear(in_features=768, out_features=768, bias=True)\n","          (k_lin): Linear(in_features=768, out_features=768, bias=True)\n","          (v_lin): Linear(in_features=768, out_features=768, bias=True)\n","          (out_lin): Linear(in_features=768, out_features=768, bias=True)\n","        )\n","        (6): MultiHeadAttention(\n","          (q_lin): Linear(in_features=768, out_features=768, bias=True)\n","          (k_lin): Linear(in_features=768, out_features=768, bias=True)\n","          (v_lin): Linear(in_features=768, out_features=768, bias=True)\n","          (out_lin): Linear(in_features=768, out_features=768, bias=True)\n","        )\n","        (7): MultiHeadAttention(\n","          (q_lin): Linear(in_features=768, out_features=768, bias=True)\n","          (k_lin): Linear(in_features=768, out_features=768, bias=True)\n","          (v_lin): Linear(in_features=768, out_features=768, bias=True)\n","          (out_lin): Linear(in_features=768, out_features=768, bias=True)\n","        )\n","        (8): MultiHeadAttention(\n","          (q_lin): Linear(in_features=768, out_features=768, bias=True)\n","          (k_lin): Linear(in_features=768, out_features=768, bias=True)\n","          (v_lin): Linear(in_features=768, out_features=768, bias=True)\n","          (out_lin): Linear(in_features=768, out_features=768, bias=True)\n","        )\n","        (9): MultiHeadAttention(\n","          (q_lin): Linear(in_features=768, out_features=768, bias=True)\n","          (k_lin): Linear(in_features=768, out_features=768, bias=True)\n","          (v_lin): Linear(in_features=768, out_features=768, bias=True)\n","          (out_lin): Linear(in_features=768, out_features=768, bias=True)\n","        )\n","        (10): MultiHeadAttention(\n","          (q_lin): Linear(in_features=768, out_features=768, bias=True)\n","          (k_lin): Linear(in_features=768, out_features=768, bias=True)\n","          (v_lin): Linear(in_features=768, out_features=768, bias=True)\n","          (out_lin): Linear(in_features=768, out_features=768, bias=True)\n","        )\n","        (11): MultiHeadAttention(\n","          (q_lin): Linear(in_features=768, out_features=768, bias=True)\n","          (k_lin): Linear(in_features=768, out_features=768, bias=True)\n","          (v_lin): Linear(in_features=768, out_features=768, bias=True)\n","          (out_lin): Linear(in_features=768, out_features=768, bias=True)\n","        )\n","      )\n","      (layer_norm1): ModuleList(\n","        (0): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","        (1): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","        (2): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","        (3): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","        (4): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","        (5): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","        (6): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","        (7): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","        (8): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","        (9): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","        (10): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","        (11): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","      )\n","      (ffns): ModuleList(\n","        (0): TransformerFFN(\n","          (lin1): Linear(in_features=768, out_features=3072, bias=True)\n","          (lin2): Linear(in_features=3072, out_features=768, bias=True)\n","        )\n","        (1): TransformerFFN(\n","          (lin1): Linear(in_features=768, out_features=3072, bias=True)\n","          (lin2): Linear(in_features=3072, out_features=768, bias=True)\n","        )\n","        (2): TransformerFFN(\n","          (lin1): Linear(in_features=768, out_features=3072, bias=True)\n","          (lin2): Linear(in_features=3072, out_features=768, bias=True)\n","        )\n","        (3): TransformerFFN(\n","          (lin1): Linear(in_features=768, out_features=3072, bias=True)\n","          (lin2): Linear(in_features=3072, out_features=768, bias=True)\n","        )\n","        (4): TransformerFFN(\n","          (lin1): Linear(in_features=768, out_features=3072, bias=True)\n","          (lin2): Linear(in_features=3072, out_features=768, bias=True)\n","        )\n","        (5): TransformerFFN(\n","          (lin1): Linear(in_features=768, out_features=3072, bias=True)\n","          (lin2): Linear(in_features=3072, out_features=768, bias=True)\n","        )\n","        (6): TransformerFFN(\n","          (lin1): Linear(in_features=768, out_features=3072, bias=True)\n","          (lin2): Linear(in_features=3072, out_features=768, bias=True)\n","        )\n","        (7): TransformerFFN(\n","          (lin1): Linear(in_features=768, out_features=3072, bias=True)\n","          (lin2): Linear(in_features=3072, out_features=768, bias=True)\n","        )\n","        (8): TransformerFFN(\n","          (lin1): Linear(in_features=768, out_features=3072, bias=True)\n","          (lin2): Linear(in_features=3072, out_features=768, bias=True)\n","        )\n","        (9): TransformerFFN(\n","          (lin1): Linear(in_features=768, out_features=3072, bias=True)\n","          (lin2): Linear(in_features=3072, out_features=768, bias=True)\n","        )\n","        (10): TransformerFFN(\n","          (lin1): Linear(in_features=768, out_features=3072, bias=True)\n","          (lin2): Linear(in_features=3072, out_features=768, bias=True)\n","        )\n","        (11): TransformerFFN(\n","          (lin1): Linear(in_features=768, out_features=3072, bias=True)\n","          (lin2): Linear(in_features=3072, out_features=768, bias=True)\n","        )\n","      )\n","      (layer_norm2): ModuleList(\n","        (0): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","        (1): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","        (2): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","        (3): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","        (4): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","        (5): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","        (6): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","        (7): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","        (8): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","        (9): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","        (10): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","        (11): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","      )\n","    )\n","    (sequence_summary): SequenceSummary(\n","      (summary): Linear(in_features=768, out_features=27, bias=True)\n","      (activation): Identity()\n","      (first_dropout): Dropout(p=0.1, inplace=False)\n","      (last_dropout): Identity()\n","    )\n","    (dropout): Dropout(p=0.1, inplace=False)\n","    (classifier): Identity()\n","  )\n","  (reduce_dim): Conv1d(2048, 768, kernel_size=(1,), stride=(1,))\n","  (reduce_dim2): Conv1d(768, 1, kernel_size=(1,), stride=(1,))\n","  (out): Linear(in_features=768, out_features=27, bias=True)\n",")"]},"metadata":{"tags":[]},"execution_count":82}]},{"cell_type":"markdown","metadata":{"id":"wbah-djKPyRB"},"source":["# Fuse Input Data"]},{"cell_type":"code","metadata":{"id":"ZZBQFQnFSn6L","executionInfo":{"status":"ok","timestamp":1620603490346,"user_tz":-120,"elapsed":391103,"user":{"displayName":"Sara Fattouh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjGrdvZo2x_8K8abskRubIgMK3SdtS4cG-aQE01nA=s64","userId":"09360594903764573322"}}},"source":["train_dataset = FusionDataset (train_df, tr_inputs_cam, tr_masks_cam, tr_inputs_flau, tr_masks_flau,\n","                            transform = image_transforms['train'])\n","\n","val_dataset = FusionDataset (val_df, val_inputs_cam, val_masks_cam, val_inputs_flau, val_masks_flau,\n","                          transform = image_transforms['valid'])\n","\n","\n","\n","test_dataset = FusionDataset (test_df, input_ids_test_cam, attention_masks_test_cam, input_ids_test_flau, attention_masks_test_flau\n","                           , transform = image_transforms['test'])"],"execution_count":83,"outputs":[]},{"cell_type":"code","metadata":{"id":"yPY9mTkrs2Jl","executionInfo":{"status":"ok","timestamp":1620603490347,"user_tz":-120,"elapsed":391096,"user":{"displayName":"Sara Fattouh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjGrdvZo2x_8K8abskRubIgMK3SdtS4cG-aQE01nA=s64","userId":"09360594903764573322"}}},"source":["# print(len(train_df), len(tr_inputs_cam), len(tr_inputs_flau))"],"execution_count":84,"outputs":[]},{"cell_type":"code","metadata":{"id":"VXWSFsjKVnXn","executionInfo":{"status":"ok","timestamp":1620603490348,"user_tz":-120,"elapsed":391088,"user":{"displayName":"Sara Fattouh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjGrdvZo2x_8K8abskRubIgMK3SdtS4cG-aQE01nA=s64","userId":"09360594903764573322"}}},"source":["# print(len(train_dataset))"],"execution_count":85,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Uh2PccgCXJeI"},"source":["# Data Loaders\n","\n","We need to use the DataLoaders to create iterable objects for us to work with. We tell it which datasets we want to use, give it a batch size, and shuffle the data"]},{"cell_type":"code","metadata":{"id":"nfFhYJYpSr05","executionInfo":{"status":"ok","timestamp":1620603490348,"user_tz":-120,"elapsed":391080,"user":{"displayName":"Sara Fattouh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjGrdvZo2x_8K8abskRubIgMK3SdtS4cG-aQE01nA=s64","userId":"09360594903764573322"}}},"source":["batch_size = 128  #increase batch size to reduce the noise \n","\n","train_dataloader = DataLoader(train_dataset,batch_size=batch_size,shuffle=True)\n","\n","validation_dataloader = DataLoader(val_dataset,batch_size=batch_size,shuffle=False)\n"," \n","test_dataloader = DataLoader(test_dataset,batch_size=batch_size,shuffle=False)"],"execution_count":86,"outputs":[]},{"cell_type":"code","metadata":{"id":"Y-gP7nVnSwAt","executionInfo":{"status":"ok","timestamp":1620603490349,"user_tz":-120,"elapsed":391073,"user":{"displayName":"Sara Fattouh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjGrdvZo2x_8K8abskRubIgMK3SdtS4cG-aQE01nA=s64","userId":"09360594903764573322"}}},"source":["optimizer = AdamW( model.parameters(),\n","                  lr = 2e-4, # args.learning_rate - default is 5e-5, our notebook had 2e-5\n","                  eps = 1e-8, # args.adam_epsilon  - default is 1e-8.\n","                  weight_decay= 0.001\n","                )"],"execution_count":87,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"PgoWU2g6S1CZ","executionInfo":{"status":"ok","timestamp":1620603490349,"user_tz":-120,"elapsed":391060,"user":{"displayName":"Sara Fattouh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjGrdvZo2x_8K8abskRubIgMK3SdtS4cG-aQE01nA=s64","userId":"09360594903764573322"}},"outputId":"a9b881d6-122c-42a2-f835-8e12791ba382"},"source":["def count_parameters(model):\n","    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n","count_parameters(model)"],"execution_count":88,"outputs":[{"output_type":"execute_result","data":{"text/plain":["1595164"]},"metadata":{"tags":[]},"execution_count":88}]},{"cell_type":"code","metadata":{"id":"0CRNzbwCS6vU","executionInfo":{"status":"ok","timestamp":1620603490351,"user_tz":-120,"elapsed":391050,"user":{"displayName":"Sara Fattouh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjGrdvZo2x_8K8abskRubIgMK3SdtS4cG-aQE01nA=s64","userId":"09360594903764573322"}}},"source":["from transformers import get_linear_schedule_with_warmup\n","\n","# Number of training epochs. The BERT authors recommend between 2 and 4. \n","# We chose to run for 4, but we'll see later that this may be over-fitting the\n","# training data.\n","epochs = 10\n","\n","# Total number of training steps is [number of batches] x [number of epochs]. \n","# (Note that this is not the same as the number of training samples).\n","total_steps = len(train_dataloader) * epochs\n","\n","# Create the learning rate scheduler.\n","scheduler = get_linear_schedule_with_warmup(optimizer, \n","                                            num_warmup_steps = 0, # Default value in run_glue.py\n","                                            num_training_steps = total_steps)"],"execution_count":89,"outputs":[]},{"cell_type":"code","metadata":{"id":"en6CE-_8S-Vw","executionInfo":{"status":"ok","timestamp":1620603490351,"user_tz":-120,"elapsed":391042,"user":{"displayName":"Sara Fattouh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjGrdvZo2x_8K8abskRubIgMK3SdtS4cG-aQE01nA=s64","userId":"09360594903764573322"}}},"source":["import torch.nn as nn\n","loss_criterion = nn.CrossEntropyLoss()"],"execution_count":90,"outputs":[]},{"cell_type":"code","metadata":{"id":"MLg8d2UmTBLp","executionInfo":{"status":"ok","timestamp":1620603490352,"user_tz":-120,"elapsed":391035,"user":{"displayName":"Sara Fattouh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjGrdvZo2x_8K8abskRubIgMK3SdtS4cG-aQE01nA=s64","userId":"09360594903764573322"}}},"source":["def flat_accuracy(preds, labels):\n","    pred_flat = np.argmax(preds, axis=1).flatten()\n","    labels_flat = labels.flatten()\n","    return np.sum(pred_flat == labels_flat) / len(labels_flat)"],"execution_count":91,"outputs":[]},{"cell_type":"code","metadata":{"id":"kY7xbQXqTHUF","executionInfo":{"status":"ok","timestamp":1620603490352,"user_tz":-120,"elapsed":391027,"user":{"displayName":"Sara Fattouh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjGrdvZo2x_8K8abskRubIgMK3SdtS4cG-aQE01nA=s64","userId":"09360594903764573322"}}},"source":["# from sklearn.metrics import f1_score\n","\n","# #seed_val = 42\n","# seed_val = 42\n","\n","# random.seed(seed_val)\n","# np.random.seed(seed_val)\n","# torch.manual_seed(seed_val)\n","# torch.cuda.manual_seed_all(seed_val)\n","\n","# # We'll store a number of quantities such as training and validation loss, \n","# # validation accuracy, and timings.\n","# training_stats = []\n","# train_loss_values = []\n","\n","# val_loss_values = []\n","# logits_values =[]\n","\n","# ############\n","\n","# total_train_accuracy = 0\n","# avg_train_accuracy = 0\n","\n","# train_accuracy_values = []\n","# val_accuracy_values = []\n","\n","# ##########\n","\n","# # Measure the total training time for the whole run.\n","# total_t0 = time.time()\n","\n","\n","\n","# # For each epoch...\n","# for epoch_i in range(0, epochs):\n","    \n","#     # ========================================\n","#     #               Training\n","#     # ========================================\n","    \n","#     # Perform one full pass over the training set.\n","\n","#     print(\"\")\n","#     print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n","#     print('Training...')\n","    \n","#     #tr and val\n","# #     vec_output_tr = []\n","# #     vec_output_val =[]\n","\n","#     # Measure how long the training epoch takes.\n","#     t0 = time.time()\n","\n","#     # Reset the total loss for this epoch.\n","#     total_train_loss = 0\n","#     total_train_accuracy = 0\n","#     predictions=[]\n","#     true_labels=[]\n","\n","#     # Put the model into training mode. Don't be mislead--the call to \n","#     # `train` just changes the *mode*, it doesn't *perform* the training.\n","#     # `dropout` and `batchnorm` layers behave differently during training\n","#     # vs. test (source: https://stackoverflow.com/questions/51433378/what-does-model-train-do-in-pytorch)\n","\n","#     model.to(device)\n","#     best_f1 = 0\n","#     model.train()\n","\n","#     # For each batch of training data...\n","#     for step, batch in (enumerate(train_dataloader)):\n","        \n","#         # Unpack this training batch from our dataloader. \n","#         #   \n","#         # As we unpack the batch, we'll also copy each tensor to the GPU using the \n","#         # `to` method.\n","#         #\n","#         # `batch` contains three pytorch tensors:\n","#         #   [0]: input ids \n","#         #   [1]: attention masks\n","#         #   [2]: labels \n","# #         return img,input_id_cam,input_mask_cam,input_id_flau,input_mask_flau\n","\n","#         b_img               = batch[0].to(device)\n","#         b_input_id_cam      = batch[1].to(device)\n","#         b_input_mask_cam    = batch[2].to(device)\n","#         b_input_id_flau     = batch[3].to(device)\n","#         b_input_mask_flau   = batch[4].to(device)\n","#         b_labels            = batch[5].to(device)\n","        \n","        \n","#         model.zero_grad()    #set the gradients to zero before starting to do backpropragation because PyTorch accumulates \n","#                             # the gradients on subsequent backward passes\n","\n","        \n","#         logits = model(b_img, b_input_id_cam , b_input_mask_cam, b_input_id_flau, b_input_mask_flau)  # 27\n","                            \n","#         #Defining the loss\n","#         loss = loss_criterion(logits, b_labels)\n","        \n","#         #saving the features_tr\n","# #         vec = vec.detach().cpu().numpy()\n","# #         vec_output_tr.extend(vec)\n","        \n","#         # Accumulate the training loss over all of the batches so that we can\n","#         # calculate the average loss at the end. `loss` is a Tensor containing a\n","#         # single value; the `.item()` function just returns the Python value \n","#         # from the tensor.\n","\n","#         total_train_loss += loss.item()\n","# #-------------------------------------------------------\n","\n","#         # Move logits and labels to CPU\n","#         logits = logits.detach().cpu().numpy()\n","\n","#         # Move logits and labels to CPU\n","#         predicted_labels=np.argmax(logits,axis=1)\n","#         predictions.extend(predicted_labels)\n","#         label_ids = b_labels.to('cpu').numpy()\n","#         true_labels.extend(label_ids)\n","\n","#         total_train_accuracy += flat_accuracy(logits, label_ids)\n","\n","\n","# #-------------------------------------------------------\n","\n","#         # Perform a backward pass to calculate the gradients.\n","#         loss.backward()\n","\n","#         # Clip the norm of the gradients to 1.0.\n","#         # This is to help prevent the \"exploding gradients\" problem.\n","#         torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n","\n","#         # Update parameters and take a step using the computed gradient.\n","#         # The optimizer dictates the \"update rule\"--how the parameters are\n","#         # modified based on their gradients, the learning rate, etc.\n","#         optimizer.step()\n","\n","#         # Update the learning rate.\n","#         scheduler.step()\n","\n","\n","#   # ------------------------------------------------------\n","        \n","#     avg_train_accuracy = total_train_accuracy / len(train_dataloader)\n","\n","#     print(\"\")\n","#     print(\"Training Accuracy: {}\".format(avg_train_accuracy))\n","#     train_accuracy_values.append(avg_train_accuracy)\n","\n","#     ######################################################################################\n","\n","#     # Calculate the average loss over all of the batches.\n","#     avg_train_loss = total_train_loss / len(train_dataloader)  \n","#     train_loss_values.append(avg_train_loss)  #-- move 2 lines up (newly added code block)\n","             \n","    \n","#     # Measure how long this epoch took.\n","#     training_time = format_time(time.time() - t0)\n","\n","    \n","#     print(\"  Average training loss: {0:.2f} \".format(avg_train_loss))\n","#     print(\"  Training epcoh took: {:} \".format(training_time))\n","        \n","#     # ========================================\n","#     #               Validation\n","#     # ========================================\n","#     # After the completion of each training epoch, measure our performance on\n","#     # our validation set.\n","\n","#     print(\"\")\n","#     print(\"Running Validation...\")\n","\n","#     t0 = time.time()\n","\n","#     # Put the model in evaluation mode--the dropout layers behave differently\n","#     # during evaluation.\n","#     model.eval()\n","\n","#     # Tracking variables \n","#     total_eval_accuracy = 0\n","#     total_eval_loss = 0\n","#     nb_eval_steps = 0\n","#     predictions=[]\n","#     true_labels=[]\n","    \n","\n","#     # Evaluate data for one epoch\n","#     for batch in (validation_dataloader):\n","        \n","#         # Unpack this training batch from our dataloader. \n","#         #\n","#         # As we unpack the batch, we'll also copy each tensor to the GPU using \n","#         # the `to` method.\n","#         #\n","#         # `batch` contains three pytorch tensors:\n","#         #   [0]: input ids \n","#         #   [1]: attention masks\n","#         #   [2]: labels \n","        \n","#         b_img = batch[0].to(device)\n","\n","#         b_input_id_cam = batch[1].to(device)\n","#         b_input_mask_cam = batch[2].to(device)\n","#         b_input_id_flau = batch[3].to(device)\n","#         b_input_mask_flau = batch[4].to(device)\n","\n","#         b_labels = batch[5].to(device)\n","        \n","        \n","#         # Tell pytorch not to bother with constructing the compute graph during\n","#         # the forward pass, since this is only needed for backprop (training).\n","#         with torch.no_grad():       \n","        \n","\n","#             # Forward pass, calculate logit predictions.\n","#             # token_type_ids is the same as the \"segment ids\", which \n","#             # differentiates sentence 1 and 2 in 2-sentence tasks.\n","#             # The documentation for this `model` function is here: \n","#             # https://huggingface.co/transformers/v2.2.0/model_doc/bert.html#transformers.BertForSequenceClassification\n","#             # Get the \"logits\" output by the model. The \"logits\" are the output\n","#             # values prior to applying an activation function like the softmax.\n","#             logits = model(b_img,b_input_id_cam ,b_input_mask_cam,b_input_id_flau,b_input_mask_flau)\n","            \n","#         #new\n","        \n","#         #defining the val loss\n","#         loss = loss_criterion(logits, b_labels)\n","        \n","#         # Accumulate the validation loss.\n","#         total_eval_loss += loss.item()\n","\n","#         # Move logits and labels to CPU\n","#         logits = logits.detach().cpu().numpy()\n","\n","#         # Move logits and labels to CPU\n","#         predicted_labels=np.argmax(logits,axis=1)\n","#         predictions.extend(predicted_labels)\n","#         label_ids = b_labels.to('cpu').numpy()\n","#         true_labels.extend(label_ids)\n","\n","#         ##########################################################################\n","\n","#         logits_values.append(predicted_labels)\n","\n","#         ##########################################################################\n","\n","#         #saving the features_tr\n","# #         vec = vec.detach().cpu().numpy()\n","# #         vec_output_val.extend(vec)\n","        \n","\n","#         # Calculate the accuracy for this batch of test sentences, and\n","#         # accumulate it over all batches.\n","#         total_eval_accuracy += flat_accuracy(logits, label_ids)\n","        \n","\n","#     # Report the final accuracy for this validation run.\n","#     avg_val_accuracy = total_eval_accuracy / len(validation_dataloader)\n","\n","# #--------------------------------\n","#     val_accuracy_values.append(avg_val_accuracy)\n","# #--------------------------------\n","#     print(\"  Accuracy: {}\".format(avg_val_accuracy))\n","\n","#     # Calculate the average loss over all of the batches.\n","#     avg_val_loss = total_eval_loss / len(validation_dataloader)\n","# #-----------------------------\n","#     val_loss_values.append(avg_val_loss)\n","\n","    \n","    \n","#     # Measure how long the validation run took.\n","#     validation_time = format_time(time.time() - t0)\n","    \n","#     print(\"  Validation Loss: {}\".format(avg_val_loss))\n","#     print(\"  Validation took: {:}\".format(validation_time))\n","#     print(\"  Validation F1-Score: {}\".format(f1_score(true_labels,predictions,average='macro')))\n","#     curr_f1=f1_score(true_labels,predictions,average='macro')\n","#     if curr_f1 > best_f1:\n","#         best_f1=curr_f1\n","#         torch.save(model.state_dict(), '/content/drive/My Drive/Rakuten/models/80_20_Mean_Hirarical_model.pt')\n","# #         np.save('best_vec_train_model_train.npy',vec_output_tr)\n","# #         np.save('best_vec_val.npy',vec_output_val)\n","        \n","#     # Record all statistics from this epoch.\n","# #     training_stats.append(\n","# #         {\n","# #             'epoch': epoch_i + 1,\n","# #             'Training Loss': avg_train_loss,\n","# #             'Valid. Loss': avg_val_loss,\n","# #             'Valid. Accur.': avg_val_accuracy,\n","# #             'Training Time': training_time,\n","# #             'Validation Time': validation_time\n","# #         }\n","# #     )\n","\n","# print(\"\")\n","# print(\"Training complete!\")\n","\n","# print(\"Total training took {:} (h:mm:ss)\".format(format_time(time.time()-total_t0)))\n","\n","# print()\n","\n","# plt.plot(np.array(train_loss_values), 'r', label='train loss')\n","# plt.plot(np.array(val_loss_values), 'b', label='val loss'  )\n","# plt.legend()\n","# plt.title('Model Loss')\n","# plt.show()\n","\n","# print()\n","\n","# plt.plot(np.array(train_accuracy_values), 'r', label='train accuracy')\n","# plt.plot(np.array(val_accuracy_values), 'b', label='val accuracy'  )\n","# plt.legend()\n","# plt.title('Model Accuracy')\n","# plt.show()\n","\n","# #print(logits_values)\n"],"execution_count":92,"outputs":[]},{"cell_type":"code","metadata":{"id":"A_lgPHpW0kv3","executionInfo":{"status":"ok","timestamp":1620603490352,"user_tz":-120,"elapsed":391019,"user":{"displayName":"Sara Fattouh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjGrdvZo2x_8K8abskRubIgMK3SdtS4cG-aQE01nA=s64","userId":"09360594903764573322"}}},"source":["# pwd"],"execution_count":93,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Te03NzKu28Fs","executionInfo":{"status":"ok","timestamp":1620603490794,"user_tz":-120,"elapsed":391453,"user":{"displayName":"Sara Fattouh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjGrdvZo2x_8K8abskRubIgMK3SdtS4cG-aQE01nA=s64","userId":"09360594903764573322"}},"outputId":"702a8f7e-fdae-481f-ad3f-cc6ab3feb8fb"},"source":["ls -la"],"execution_count":94,"outputs":[{"output_type":"stream","text":["total 493112\n","drwxr-xr-x 1 root root     4096 May  9 23:36 \u001b[0m\u001b[01;34m.\u001b[0m/\n","drwxr-xr-x 1 root root     4096 May  9 22:46 \u001b[01;34m..\u001b[0m/\n","drwxr-xr-x 4 root root     4096 May  6 13:43 \u001b[01;34m.config\u001b[0m/\n","-rw------- 1 root root 54129126 May  9 23:31 data\n","drwx------ 5 root root     4096 May  9 23:12 \u001b[01;34mdrive\u001b[0m/\n","drwxr-xr-x 5 root root     4096 May  9 23:20 \u001b[01;34mRakuten\u001b[0m/\n","drwxr-xr-x 1 root root     4096 May  6 13:44 \u001b[01;34msample_data\u001b[0m/\n","-rw-r--r-- 1 root root 13523704 May  9 23:36 test_inputs_cam.pt\n","-rw-r--r-- 1 root root 13523704 May  9 23:36 test_inputs_flau.pt\n","-rw-r--r-- 1 root root 13523704 May  9 23:36 test_masks_cam.pt\n","-rw-r--r-- 1 root root 13523704 May  9 23:36 test_masks_flau.pt\n","-rw-r--r-- 1 root root 76630776 May  9 23:36 tr_inputs_cam.pt\n","-rw-r--r-- 1 root root 76630776 May  9 23:36 tr_inputs_flau.pt\n","-rw-r--r-- 1 root root 76630776 May  9 23:36 tr_masks_cam.pt\n","-rw-r--r-- 1 root root 76630776 May  9 23:36 tr_masks_flau.pt\n","-rw-r--r-- 1 root root 22539000 May  9 23:36 val_inputs_cam.pt\n","-rw-r--r-- 1 root root 22539000 May  9 23:36 val_inputs_flau.pt\n","-rw-r--r-- 1 root root 22539000 May  9 23:36 val_masks_cam.pt\n","-rw-r--r-- 1 root root 22539000 May  9 23:36 val_masks_flau.pt\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"GMNZStqu0ZzH","executionInfo":{"status":"ok","timestamp":1620603490796,"user_tz":-120,"elapsed":391443,"user":{"displayName":"Sara Fattouh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjGrdvZo2x_8K8abskRubIgMK3SdtS4cG-aQE01nA=s64","userId":"09360594903764573322"}}},"source":["# cp './Hirarical_add_concat_best_model.pt' '../drive/My Drive/Rakuten/models/'"],"execution_count":95,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"RkB48fg1HidV"},"source":["# Model Testing"]},{"cell_type":"code","metadata":{"id":"XjYObOyZ-GAd","executionInfo":{"status":"ok","timestamp":1620603490796,"user_tz":-120,"elapsed":391435,"user":{"displayName":"Sara Fattouh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjGrdvZo2x_8K8abskRubIgMK3SdtS4cG-aQE01nA=s64","userId":"09360594903764573322"}}},"source":["model_path =  '/content/drive/My Drive/Rakuten/models/80_20_Mean_Hirarical_model.pt'"],"execution_count":96,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":340},"id":"jKTbBLHzHoYL","executionInfo":{"status":"error","timestamp":1620603492101,"user_tz":-120,"elapsed":392738,"user":{"displayName":"Sara Fattouh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjGrdvZo2x_8K8abskRubIgMK3SdtS4cG-aQE01nA=s64","userId":"09360594903764573322"}},"outputId":"13a1872e-bd80-44dc-dcea-1eda79cba46d"},"source":["checkpoint = torch.load(model_path)\n","model.load_state_dict(checkpoint) # A state_dict is simply a Python dictionary object \n","                                  # that maps each layer to its parameter tensor"],"execution_count":97,"outputs":[{"output_type":"error","ename":"EOFError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mEOFError\u001b[0m                                  Traceback (most recent call last)","\u001b[0;32m<ipython-input-97-c10a2c5dd794>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mcheckpoint\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcheckpoint\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# A state_dict is simply a Python dictionary object\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m                                   \u001b[0;31m# that maps each layer to its parameter tensor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, **pickle_load_args)\u001b[0m\n\u001b[1;32m    591\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopened_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    592\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0m_load\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopened_zipfile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap_location\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpickle_module\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mpickle_load_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 593\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_legacy_load\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopened_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap_location\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpickle_module\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mpickle_load_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    594\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    595\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36m_legacy_load\u001b[0;34m(f, map_location, pickle_module, **pickle_load_args)\u001b[0m\n\u001b[1;32m    760\u001b[0m             \"functionality.\")\n\u001b[1;32m    761\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 762\u001b[0;31m     \u001b[0mmagic_number\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpickle_module\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mpickle_load_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    763\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mmagic_number\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mMAGIC_NUMBER\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    764\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Invalid magic number; corrupt file?\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mEOFError\u001b[0m: Ran out of input"]}]},{"cell_type":"code","metadata":{"id":"46UaiCmcHwR4","executionInfo":{"status":"aborted","timestamp":1620603491703,"user_tz":-120,"elapsed":392332,"user":{"displayName":"Sara Fattouh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjGrdvZo2x_8K8abskRubIgMK3SdtS4cG-aQE01nA=s64","userId":"09360594903764573322"}}},"source":["import matplotlib.pyplot as plt\n","from sklearn.metrics import f1_score\n","\n","def predict_pyt(model, prediction_dataloader):\n","    \"\"\"\n","    model: pytorch model\n","    prediction_dataloader: DataLoader object for which the predictions has to be made.\n","    return:\n","        predictions:    - Direct predicted labels\n","        softmax_logits: - logits which are normalized with softmax on output\"\"\"\n","    \n","    \n","    print(\"\")\n","    print(\"Running Testing...\")\n","\n","    t0 = time.time()\n","\n","    # Put the model in evaluation mode--the dropout layers behave differently\n","    # during evaluation.\n","    model.eval()\n","\n","    # Tracking variables \n","    total_eval_accuracy = 0\n","    total_eval_loss = 0\n","    nb_eval_steps = 0\n","    predictions=[]\n","    true_labels=[]\n","    logits_values =[]\n","    val_accuracy_values = []\n","    val_loss_values = []\n","    \n","    \n","    total_t0 = time.time()\n","    # Evaluate data for one epoch\n","    for batch in (prediction_dataloader):\n","        \n","        # Unpack this training batch from our dataloader. \n","        #\n","        # As we unpack the batch, we'll also copy each tensor to the GPU using \n","        # the `to` method.\n","        #\n","        # `batch` contains three pytorch tensors:\n","        #   [0]: input ids \n","        #   [1]: attention masks\n","        #   [2]: labels \n","        \n","        b_img = batch[0].to(device)\n","\n","        b_input_id_cam = batch[1].to(device)\n","        b_input_mask_cam = batch[2].to(device)\n","        b_input_id_flau = batch[3].to(device)\n","        b_input_mask_flau = batch[4].to(device)\n","\n","        b_labels = batch[5].to(device)\n","        \n","        \n","        # Tell pytorch not to bother with constructing the compute graph during\n","        # the forward pass, since this is only needed for backprop (training).\n","        with torch.no_grad():       \n","        \n","\n","            # Forward pass, calculate logit predictions.\n","            # token_type_ids is the same as the \"segment ids\", which \n","            # differentiates sentence 1 and 2 in 2-sentence tasks.\n","            # The documentation for this `model` function is here: \n","            # https://huggingface.co/transformers/v2.2.0/model_doc/bert.html#transformers.BertForSequenceClassification\n","            # Get the \"logits\" output by the model. The \"logits\" are the output\n","            # values prior to applying an activation function like the softmax.\n","\n","\n","            logits = model(b_img,b_input_id_cam ,b_input_mask_cam,b_input_id_flau,b_input_mask_flau)\n","            \n","        #new\n","        \n","        #defining the val loss\n","        loss = loss_criterion(logits, b_labels)\n","        \n","        # Accumulate the validation loss.\n","        total_eval_loss += loss.item()\n","\n","        # Move logits and labels to CPU\n","        logits = logits.detach().cpu().numpy()\n","\n","        # Move logits and labels to CPU\n","        predicted_labels=np.argmax(logits,axis=1)\n","        predictions.extend(predicted_labels)\n","        label_ids = b_labels.to('cpu').numpy()\n","        true_labels.extend(label_ids)\n","\n","        ##########################################################################\n","\n","        logits_values.append(predicted_labels)\n","\n","        ##########################################################################\n","\n","        #saving the features_tr\n","#         vec = vec.detach().cpu().numpy()\n","#         vec_output_val.extend(vec)\n","        \n","\n","        # Calculate the accuracy for this batch of test sentences, and\n","        # accumulate it over all batches.\n","        total_eval_accuracy += flat_accuracy(logits, label_ids)\n","        \n","\n","    # Report the final accuracy for this validation run.\n","    avg_val_accuracy = total_eval_accuracy / len(validation_dataloader)\n","\n","#--------------------------------\n","    val_accuracy_values.append(avg_val_accuracy)\n","#--------------------------------\n","    print(\"  Accuracy: {}\".format(avg_val_accuracy))\n","\n","    # Calculate the average loss over all of the batches.\n","    avg_val_loss = total_eval_loss / len(validation_dataloader)\n","#-----------------------------\n","    val_loss_values.append(avg_val_loss)\n","\n","    \n","    \n","    # Measure how long the validation run took.\n","    validation_time = format_time(time.time() - t0)\n","    \n","    print(\"  Test Loss: {0}\".format(avg_val_loss))\n","    print(\"  Test took: {:}\".format(validation_time))\n","    print(\"Test F1-Score: {}\".format(f1_score(true_labels,predictions,average='macro')))\n","    curr_f1=f1_score(true_labels,predictions,average='macro')\n","\n","\n","    print(\"\")\n","    print(\"Testing complete!\")\n","\n","    #print(\"Total testing took {:} (h:mm:ss)\".format(format_time(time.time()-total_t0)))\n","    print()\n","    plt.plot(np.array(val_accuracy_values), 'r', label='Test accuracy')\n","    plt.legend()\n","    plt.title('Test Curve')\n","    plt.show()\n","\n","    print()\n","    print('DONE')\n","    return predictions, true_labels\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"TwK3RdRpLy30","executionInfo":{"status":"aborted","timestamp":1620603491704,"user_tz":-120,"elapsed":392325,"user":{"displayName":"Sara Fattouh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjGrdvZo2x_8K8abskRubIgMK3SdtS4cG-aQE01nA=s64","userId":"09360594903764573322"}}},"source":["predictions_val, true_label = predict_pyt(model, test_dataloader)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"39V6jDdaNsXe","executionInfo":{"status":"aborted","timestamp":1620603491705,"user_tz":-120,"elapsed":392318,"user":{"displayName":"Sara Fattouh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjGrdvZo2x_8K8abskRubIgMK3SdtS4cG-aQE01nA=s64","userId":"09360594903764573322"}}},"source":[" Results = pd.DataFrame(\n","    {'Pred':   predictions_val,\n","     'Ground': true_label\n","    })\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ldN8sldJNt6z","executionInfo":{"status":"aborted","timestamp":1620603491705,"user_tz":-120,"elapsed":392310,"user":{"displayName":"Sara Fattouh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjGrdvZo2x_8K8abskRubIgMK3SdtS4cG-aQE01nA=s64","userId":"09360594903764573322"}}},"source":["Results.to_csv( \"80_20_mean_hir\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"4cBJLSjqN0QE","executionInfo":{"status":"aborted","timestamp":1620603491706,"user_tz":-120,"elapsed":392304,"user":{"displayName":"Sara Fattouh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjGrdvZo2x_8K8abskRubIgMK3SdtS4cG-aQE01nA=s64","userId":"09360594903764573322"}}},"source":["from sklearn.metrics import accuracy_score\n","accuracy_score(Results['Ground'], Results['Pred'])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"pJ_jqBUhN2DS","executionInfo":{"status":"aborted","timestamp":1620603491706,"user_tz":-120,"elapsed":392296,"user":{"displayName":"Sara Fattouh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjGrdvZo2x_8K8abskRubIgMK3SdtS4cG-aQE01nA=s64","userId":"09360594903764573322"}}},"source":[" from sklearn.metrics import balanced_accuracy_score\n"," balanced_accuracy_score(Results['Ground'], Results['Pred'])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"zgVKrD4AN3rL","executionInfo":{"status":"aborted","timestamp":1620603491706,"user_tz":-120,"elapsed":392288,"user":{"displayName":"Sara Fattouh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjGrdvZo2x_8K8abskRubIgMK3SdtS4cG-aQE01nA=s64","userId":"09360594903764573322"}}},"source":["from sklearn.metrics import f1_score\n","print(f1_score(Results['Ground'],Results['Pred'],average='macro'))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"tua-ntUoN5RA","executionInfo":{"status":"aborted","timestamp":1620603491707,"user_tz":-120,"elapsed":392280,"user":{"displayName":"Sara Fattouh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjGrdvZo2x_8K8abskRubIgMK3SdtS4cG-aQE01nA=s64","userId":"09360594903764573322"}}},"source":["from sklearn.metrics import confusion_matrix\n","confusion_mat = confusion_matrix(Results['Ground'], Results['Pred'])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"3DDKlW9zN7Dz","executionInfo":{"status":"aborted","timestamp":1620603491707,"user_tz":-120,"elapsed":392272,"user":{"displayName":"Sara Fattouh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjGrdvZo2x_8K8abskRubIgMK3SdtS4cG-aQE01nA=s64","userId":"09360594903764573322"}}},"source":["from sklearn.metrics import classification_report\n","\n","target_names = ['class 1', 'class 2', 'class 3', 'class 4', 'class 5', 'class 6',\n","                'class 7', 'class 8', 'class 9', 'class 10', 'class 11', 'class 12', \n","                'class 13', 'class 14', 'class 15', 'class 16', 'class 17', 'class 18',\n","                'class 19','class 20', 'class 21', 'class 22', 'class 23', 'class 24', 'class 25', 'class 26', 'class 27']\n","\n","print(classification_report(Results['Ground'], Results['Pred'], target_names=target_names))\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"LgR1NPo-N8uo","executionInfo":{"status":"aborted","timestamp":1620603491707,"user_tz":-120,"elapsed":392264,"user":{"displayName":"Sara Fattouh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjGrdvZo2x_8K8abskRubIgMK3SdtS4cG-aQE01nA=s64","userId":"09360594903764573322"}}},"source":["import seaborn as sn\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","\n","df_cm = pd.DataFrame(confusion_mat, index = [i for i in range(27)],\n","                                    columns = [i for i in range(27)])\n","plt.figure(figsize = (30,30))\n","sn.heatmap(df_cm, annot=True)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ePlJz0k0N-cH","executionInfo":{"status":"aborted","timestamp":1620603491708,"user_tz":-120,"elapsed":392257,"user":{"displayName":"Sara Fattouh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjGrdvZo2x_8K8abskRubIgMK3SdtS4cG-aQE01nA=s64","userId":"09360594903764573322"}}},"source":["\n","from numpy import array\n","from numpy import argmax\n","from sklearn.preprocessing import LabelEncoder\n","from sklearn.preprocessing import OneHotEncoder\n","\n","values = np.array(true_label)\n","print(values)\n","# integer encode\n","label_encoder = LabelEncoder()\n","integer_encoded = label_encoder.fit_transform(values)\n","print(integer_encoded)\n","\n","onehot_encoder = OneHotEncoder(sparse=False)\n","integer_encoded = integer_encoded.reshape(len(integer_encoded), 1)\n","true_label_onehot_encoded = onehot_encoder.fit_transform(integer_encoded)\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"_KHygQblOAAe","executionInfo":{"status":"aborted","timestamp":1620603491708,"user_tz":-120,"elapsed":392249,"user":{"displayName":"Sara Fattouh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjGrdvZo2x_8K8abskRubIgMK3SdtS4cG-aQE01nA=s64","userId":"09360594903764573322"}}},"source":["values = np.array(predictions_val)\n","print(values)\n","# integer encode\n","label_encoder = LabelEncoder()\n","integer_encoded = label_encoder.fit_transform(values)\n","print(integer_encoded)\n","\n","onehot_encoder = OneHotEncoder(sparse=False)\n","integer_encoded = integer_encoded.reshape(len(integer_encoded), 1)\n","predictions_val_onehot_encoded = onehot_encoder.fit_transform(integer_encoded)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"kzDCpwKdOCb8","executionInfo":{"status":"aborted","timestamp":1620603491708,"user_tz":-120,"elapsed":392239,"user":{"displayName":"Sara Fattouh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjGrdvZo2x_8K8abskRubIgMK3SdtS4cG-aQE01nA=s64","userId":"09360594903764573322"}}},"source":["print(__doc__)\n","\n","import numpy as np\n","import matplotlib.pyplot as plt\n","from sklearn import svm, datasets\n","from sklearn.metrics import roc_curve, auc\n","from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import label_binarize\n","from sklearn.multiclass import OneVsRestClassifier\n","\n","\n","\n","n_classes = 27\n","\n","# Compute ROC curve and ROC area for each class\n","fpr = dict()\n","tpr = dict()\n","roc_auc = dict()\n","for i in range(n_classes):\n","    fpr[i], tpr[i], _ = roc_curve(true_label_onehot_encoded[:, i], predictions_val_onehot_encoded[:, i])\n","    roc_auc[i] = auc(fpr[i], tpr[i])\n","\n","# Compute micro-average ROC curve and ROC area\n","fpr[\"micro\"], tpr[\"micro\"], _ = roc_curve(y_test.ravel(), y_score.ravel())\n","roc_auc[\"micro\"] = auc(fpr[\"micro\"], tpr[\"micro\"])\n","\n","# # Plot of a ROC curve for a specific class\n","# plt.figure()\n","# plt.plot(fpr[2], tpr[2], label='ROC curve (area = %0.2f)' % roc_auc[2])\n","# plt.plot([0, 1], [0, 1], 'k--')\n","# plt.xlim([0.0, 1.0])\n","# plt.ylim([0.0, 1.05])\n","# plt.xlabel('False Positive Rate')\n","# plt.ylabel('True Positive Rate')\n","# plt.title('Receiver operating characteristic example')\n","# plt.legend(loc=\"lower right\")\n","# plt.show()\n","\n","# Plot ROC curve\n","plt.figure()\n","plt.plot(fpr[\"micro\"], tpr[\"micro\"],\n","         label='micro-average ROC curve (area = {0:0.2f})'\n","               ''.format(roc_auc[\"micro\"]))\n","for i in range(n_classes):\n","    plt.plot(fpr[i], tpr[i], label='ROC curve of class {0} (area = {1:0.2f})'\n","                                   ''.format(i, roc_auc[i]))\n","\n","plt.plot([0, 1], [0, 1], 'k--')\n","plt.xlim([0.0, 1.0])\n","plt.ylim([0.0, 1.05])\n","plt.xlabel('False Positive Rate')\n","plt.ylabel('True Positive Rate')\n","plt.title('Some extension of Receiver operating characteristic to multi-class')\n","plt.legend(loc=\"lower right\")\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"EXzij-qNOD4v","executionInfo":{"status":"aborted","timestamp":1620603491709,"user_tz":-120,"elapsed":392232,"user":{"displayName":"Sara Fattouh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjGrdvZo2x_8K8abskRubIgMK3SdtS4cG-aQE01nA=s64","userId":"09360594903764573322"}}},"source":["print(__doc__)\n","\n","import numpy as np\n","import matplotlib.pyplot as plt\n","from sklearn import svm, datasets\n","from sklearn.metrics import roc_curve, auc\n","from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import label_binarize\n","from sklearn.multiclass import OneVsRestClassifier\n","\n","\n","\n","n_classes = 27\n","\n","# Compute ROC curve and ROC area for each class\n","fpr = dict()\n","tpr = dict()\n","roc_auc = dict()\n","for i in range(n_classes):\n","    fpr[i], tpr[i], _ = roc_curve(true_label_onehot_encoded[:, i], predictions_val_onehot_encoded[:, i])\n","    roc_auc[i] = auc(fpr[i], tpr[i])\n","\n","# Compute micro-average ROC curve and ROC area\n","fpr[\"micro\"], tpr[\"micro\"], _ = roc_curve(y_test.ravel(), y_score.ravel())\n","roc_auc[\"micro\"] = auc(fpr[\"micro\"], tpr[\"micro\"])\n","\n","# # Plot of a ROC curve for a specific class\n","# plt.figure()\n","# plt.plot(fpr[2], tpr[2], label='ROC curve (area = %0.2f)' % roc_auc[2])\n","# plt.plot([0, 1], [0, 1], 'k--')\n","# plt.xlim([0.0, 1.0])\n","# plt.ylim([0.0, 1.05])\n","# plt.xlabel('False Positive Rate')\n","# plt.ylabel('True Positive Rate')\n","# plt.title('Receiver operating characteristic example')\n","# plt.legend(loc=\"lower right\")\n","# plt.show()\n","\n","# Plot ROC curve\n","plt.figure()\n","plt.plot(fpr[\"micro\"], tpr[\"micro\"],\n","         label='micro-average ROC curve (area = {0:0.2f})'\n","               ''.format(roc_auc[\"micro\"]))\n","# for i in range(n_classes):\n","#     plt.plot(fpr[i], tpr[i], label='ROC curve of class {0} (area = {1:0.2f})'\n","#                                    ''.format(i, roc_auc[i]))\n","\n","for i in range(n_classes):\n","    plt.plot(fpr[i], tpr[i])\n","\n","plt.plot([0, 1], [0, 1], 'k--')\n","plt.xlim([0.0, 1.0])\n","plt.ylim([0.0, 1.05])\n","plt.xlabel('False Positive Rate')\n","plt.ylabel('True Positive Rate')\n","plt.title('Some extension of Receiver operating characteristic to multi-class')\n","plt.legend(loc=\"lower right\")\n","plt.show()"],"execution_count":null,"outputs":[]}]}